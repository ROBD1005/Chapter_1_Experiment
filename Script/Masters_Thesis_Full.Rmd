---
title: "Masters Thesis Full"
author: "R.J. Dellinger"
date: "`r Sys.Date()`"
output: pdf_document
---

# Load Libraries

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
# Setting up the R environment & loading essential packages
knitr::opts_chunk$set(echo = TRUE, message = FALSE)

# Data manipulation and general utilities
#install_github(fmmattioni/downloadthis)
library(broom) # Tidy statistical summaries
library(devtools) # Package development tools
library(downloadthis) # Data download helper
library(forcats) # Factor manipulation
library(dplyr) # Data manipulation
library(glue) # Interpolated strings
library(here) # Path construction
library(htmltools) # HTML content tools
library(knitr) # Dynamic report generation
library(latex2exp) # LaTeX to plotmath
library(lubridate) # Date/time handling
library(magick) # Image processing
library(measurements) # Unit conversions
library(png) # PNG file handling
library(purrr) # Functional programming
library(readr) # Reading rectangular data
library(remotes) # Remote package installation
library(rmarkdown) # Markdown rendering
library(stringr) # String operations
library(tibble) # Modern data frames
library(tidyr) # Data tidying
library(tidyverse) # Data science packages
library(usethis) # Package setup tools

# Plotting and visualization
#install_github("hrbrmstr/ggchicklet")
library(cowplot) # Enhanced ggplot2 plots
library(extrafont) # Font handling for plots
library(ggchicklet) # Rounded rectangle plots
library(ggforce) # Enhanced ggplot2 features
library(ggfx) # Effects for ggplot2
library(ggimage) # Images in ggplot2
library(gginnards) # ggplot2 internals
library(ggplot2) # Create elegant data visualisations
library(ggpmisc) # ggplot2 extensions
library(ggpp) # ggplot2 plus-plus
library(ggpubr) # Publication ready plots
library(ggrepel) # Repel overlapping text
library(ggsci) # Scientific journal palettes
library(ggstatsplot) # Stats in ggplot2
library(ggsignif) # Significance marks on plots
library(ggthemes) # Additional ggplot2 themes
library(grid) # Grid graphics
library(gridExtra) # Extra grid functions
library(gt) # Create display tables
library(gtExtras) # Extra features for gt
library(gtsummary) # Summary tables
library(hrbrthemes) # Extra ggplot2 themes
library(latex2exp) # LaTeX to plotmath
library(magick) # Image processing
library(markdown) # Render markdown
library(patchwork) # Combine ggplots
library(png) # PNG file handling
library(RColorBrewer) # Color palettes
library(scales) # Visualization scales
library(tibble) # Modern data frames

# Geospatial data handling
#install_github("ropensci/rnaturalearthhires")

#install_github(rstudio/gt)
#install_github(ddsjoberg/gtsummary)
#install_github(jthomasmock/gtExtras)
#install_github(hrbrmstr/ggchicklet)
library(ggmap) # Spatial visualization tools
library(ggmapinset) # Map insets for ggplot2
library(ggspatial) # Spatial data with ggplot2
library(lwgeom) # Geospatial operations
library(raster) # Geographic data analysis
library(rnaturalearth) # World map data
library(rnaturalearthdata) # Natural Earth data
library(rnaturalearthhires) # High-res Earth data
library(sf) # Simple Features for R
library(sp) # Spatial data classes

# Statistical analysis and modeling
#install_github('colin-olito/LoLinR')
#install_github(padpadpadpad/nls.multstart)
#install_github(padpadpadpad/rTPC)
library(AICcmodavg) # Model selection information
library(boot) # Bootstrap functions
library(car) # Regression and ANOVA tests
library(emmeans) # Estimated marginal means
library(lme4) # Linear mixed-effects models
library(lmerTest) # Tests in linear mixed effects
library(LoLinR) # Local linear models
library(MASS) # Statistical functions from Venables and Ripley
library(Matrix) # Sparse and dense matrix classes
library(methods) # Formal methods and classes
library(minpack.lm) # Non-linear least squares
library(multcomp) # Simultaneous inference
library(MuMIn) # Multi-model inference
library(nlstools) # Tools for nonlinear regression
library(nls.multstart) # Robust non-linear regression
library(NISTunits) # Units and constants from NIST
library(performance) # Model performance metrics
library(rstatix) # Pipe-friendly framework for basic statistical tests
library(rTPC) # Thermal performance curves
library(respR) # Respirometry analysis
library(respirometry) # Tools for respirometry data
library(seacarb) # Seawater carbonate chemistry
library(stats) # Statistical functions
library(zoo) # Time series data handling




# Generate citations for the packages
seacarb_citation <- citation("seacarb")
respR_citation <- citation("respR")
nls_multstart_citation <- citation("nls.multstart")
MuMIn_citation <- citation("MuMIn")
rTPC_citation <- citation("rTPC")
LoLinR_citation <- citation("LoLinR")
performance_citation <- citation("performance")
tidyverse_citation <- citation("tidyverse")
boot_citation <- citation("boot")

# Convert the citations to BibTeX format
seacarb_bib <- toBibtex(seacarb_citation)
respR_bib <- toBibtex(respR_citation)
nls_multstart_bib <- toBibtex(nls_multstart_citation)
MuMIn_bib <- toBibtex(MuMIn_citation)
rTPC_bib <- toBibtex(rTPC_citation)
LoLinR_bib <- toBibtex(LoLinR_citation)
performance_bib <- toBibtex(performance_citation)
tidyverse_bib <- toBibtex(tidyverse_citation)
boot_bib <- toBibtex(boot_citation)

# Print the BibTeX entries
cat(seacarb_bib)
cat(respR_bib)
cat(nls_multstart_bib)
cat(MuMIn_bib)
cat(rTPC_bib)
cat(LoLinR_bib)
cat(performance_bib)
cat(tidyverse_bib)
cat(boot_bib)

```


```{r setup, include=FALSE, message=FALSE, warning=FALSE}
citation(Schoolfield, R. M., Sharpe, P. J. & Magnuson, C. E. Non-linear regression of biological temperature-dependent rate models based on absolute reaction-rate theory. J. Theor. Biol. 88, 719-731 (1981))

fn1 <- function(df_in){ in_nam <- deparse(substitute(df_in) )
         col_names <-paste(in_nam, names(df_in), sep="$")
         cat(col_names) }


```


```{r setup, include=FALSE, message=FALSE, warning=FALSE}
print(utils::citation("MuMIn"), bibtex=TRUE)
print(utils::citation("rTPC"), bibtex=TRUE)
print(utils::citation("respR"), bibtex=TRUE)
print(utils::citation("nls.multstart"), bibtex=TRUE)
print(citation("performance"), bibtex=TRUE) #well 
print(citation("LoLinR"), bibtex=TRUE)
print(citation("ggmapinset"), bibtex=TRUE)
print(citation("LoLinR"), bibtex=TRUE)
print(citation("gtsummary"), bibtex=TRUE)
print(citation("ggmapinset"), bibtex=TRUE)
print(citation("raster"), bibtex=TRUE)
print(citation("rnaturalearth"), bibtex=TRUE)
print(citation("ggstatsplot"), bibtex=TRUE)
print(citation("ggmap"), bibtex=TRUE)
print(toBibtex(citation("ggmap"), bibtex=TRUE))
print(toBibtex(citation("ggmapinset"), bibtex=TRUE))


```


```{r setup, include=FALSE, message=FALSE, warning=FALSE}
# Custom Functions for Data Analysis Visualization

# Define Functions & Formatting 
format_p_values <- function(data) {
  data %>%
    mutate(
      p.value = case_when(
        p.value < 0.001 ~ "<0.001 ***",
        p.value < 0.01  ~ "<0.01 **",
        p.value < 0.05  ~ "<0.05 *",
        is.na(p.value)  ~ "",
        TRUE            ~ sprintf("%.2f", p.value)  # Use sprintf for precise formatting
      )
    )
}

# Define a custom theme for ggplot
common_theme <- theme_minimal(base_family="serif") +
  theme(
    text = element_text(size = 14, family = "serif", color = "grey25"),
    line = element_line(color = "grey25"),
  
    
    # Set the background to white
    plot.background = element_rect(fill = "white", colour = "white"),
    # Set font family and color for text
    # Remove legend
    legend.position = 'none',
    # Adjust strip text for facetted plots
    strip.text = element_text(hjust = 0.5, size = 9, family = "serif"), 
    strip.text.x = element_text(vjust = 0.5, size = 11, family = "serif"), 
    strip.text.y = element_text(vjust = 0.5, size = 11, family = "serif"), 
    # Adjust axis titles
    axis.title.x = element_text(margin = margin(t = 7.5), size = 14,
                                hjust = 0.5, family = "serif"),
    axis.title.y = element_text(margin = margin(r = 7.5), size = 14,
                                vjust = 0.5, family = "serif", ),
    # Adjust axis text
    axis.text = element_text(size = 9, family = "serif"),
    # Adjust plot title
    plot.title = element_text(hjust = 0.5, size = 14, family = "serif"),
    # Adjust plot margins
    plot.margin = margin(t = 0.5, r = 0.5, b = 0.5, l = 0.5, unit="cm"),
    # Increase line thickness
    panel.border = element_rect(color = NA, fill = NA, linewidth = 0.5),
    # Set panel grid lines
    panel.grid.major = element_line(color = "gray75", linewidth = 0.15),
    panel.grid.minor = element_blank()
  )



# Function to apply a custom theme to a gt table
gt_theme <- function(gt_table) {
  gt_table %>%
    # Apply text and cell styles
    tab_style(
      style = cell_text(align = "center", v_align = "middle", 
                        font = "serif", size = 12,
                        color = "grey25"),
      locations = cells_body(columns = everything())
    ) %>%
    tab_style(
      style = cell_text(align = "center", v_align = "middle", font = "serif",
                        weight = "bold", size = 12.5, 
                        color = "grey25"),
      locations = cells_column_labels(columns = everything())
    ) %>% 
    tab_style(
      style = cell_text(align = "center", v_align = "middle", font = "serif",
                        weight = "bold", size = 16, 
                        color = "grey25"),
      locations = cells_title(groups = "title")
    ) %>%
    tab_style(
      style = cell_text(align = "center", v_align = "middle", font = "serif",
      size = 14, color = "grey25",  weight = "bold"),
      locations = cells_title(groups = "subtitle")
    ) %>% 
    # Apply general font options and table alignment
    tab_options(
      table.font.names = "serif",
      table.font.color = "grey25",
      table.font.size = 12,
      table.width = pct(70),
      table.align = "center",
      #cell pading
      container.padding.y = px(6),
      
      # Data padding
      data_row.padding = px(8),
      data_row.padding.horizontal = px(6),
      
      # Top and bottom borders of the table body
      table.border.top.width = px(2),
      table.border.top.color = "grey25",
      table.border.bottom.width = px(2),
      table.border.bottom.color = "grey25",
      
      # Ensure consistent border width for the top and bottom of the table body
      table_body.border.top.width = px(1.5),
      table_body.border.top.color = "grey25",
      table_body.border.bottom.width = px(1.5),
      table_body.border.bottom.color = "grey25",
      
      # Horizontal line settings (hlines) inside the body
      table_body.hlines.width = px(0),
      table_body.hlines.color = "transparent",
      table_body.vlines.color = "grey25",
      table_body.vlines.width = px(0.5),
      
      # Column label settings
      column_labels.background.color = "white",
      column_labels.border.top.width = px(1.5),
      column_labels.border.top.color = "grey25",
      column_labels.border.bottom.width = px(1.5),
      column_labels.border.bottom.color = "grey25",
      column_labels.padding = px(6),
      column_labels.padding.horizontal = px(6.5),
      
      #heading and subtitle settings 
      heading.padding = px(6.5),
      heading.align = "center",
      heading.title.font.size = 15,
      heading.subtitle.font.size = 13,
      heading.title.font.weight = "bolder",
      heading.subtitle.font.weight = "bold",
      heading.background.color = "white",  # Sets the header background to white
      heading.border.bottom.color ="grey25",
      
      # Group row settings
      row_group.padding = px(6),
      row_group.border.top.width = px(1.5),
      row_group.border.top.color = "grey25",
      row_group.border.bottom.width = px(1.5),
      row_group.border.bottom.color = "grey25",
      
      # Other styling options
      source_notes.font.size = 11,
      footnotes.font.size = 11,
      footnotes.padding = px(6),
      footnotes.multiline = TRUE
    ) 
}

```


# Schematics for Thermal Performance Curves
Created using rTPC package and ggplot to visualize thermal performance curve hypothesis (using generated data from quadratic equations)

```{r Thermal-Performance-Curve-Schematic, fig.height=6, fig.width=8, message=FALSE, warning=FALSE, fig.align="center"}
# Extend the temperature values from 0 to 32
temperatures <- seq(0, 32, by = 1)

peak_rate_low=150
metabolic_rate_low <- ifelse(temperatures <= 16, 
                             (peak_rate_low / 80) * (4 * temperatures - (temperatures^2 / 18)),  
                             ifelse(temperatures <= 19, 
                                    peak_rate_low - 5 * (peak_rate_low / 60) * (temperatures - 16)^2.5, 
                                    24)) 

# Convert to tibble for low pH
metabolic_rate_low <- tibble(temp = temperatures, rate = metabolic_rate_low, curve_id = "low")

peak_rate_high=80
# High pH curve with adjustments at the ends
metabolic_rate_ambient <- ifelse(temperatures <= 22, 
                                 (peak_rate_high / 100) * (5 * temperatures - (temperatures^2 / 22)), 
                                 ifelse(temperatures <= 27, 
                                        peak_rate_high - ((temperatures - 22) / (27 - 22)) * (peak_rate_high - 40), 
                                        10))  

# Convert to tibble for high pH
metabolic_rate_ambient <- tibble(temp = temperatures, rate = metabolic_rate_ambient, curve_id = "high")

# Combine both data frames
df_tpc <- bind_rows(metabolic_rate_ambient, metabolic_rate_low)

high_data <- subset(df_tpc, curve_id == "high")

# get start vals
high_start_vals <- get_start_vals(high_data$temp, high_data$rate, model_name = 'sharpeschoolhigh_1981')
# get limits
high_low_lims <- get_lower_lims(high_data$temp, high_data$rate, model_name = 'sharpeschoolhigh_1981')
high_upper_lims <- get_upper_lims(high_data$temp, high_data$rate, model_name = 'sharpeschoolhigh_1981')

# fit model
high_fit <- nls_multstart(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 15),
                                                     data = high_data,
                                                     iter = 1,
                                                     start_lower = high_start_vals,
                                                     start_upper = high_start_vals,
                                                     lower = high_low_lims,
                                                     upper = high_upper_lims,
                                                     supp_errors = 'Y')
# predict new data
high_new_data <- data.frame(temp = seq(min(high_data$temp), max(high_data$temp), 0.5))
high_preds <- augment(high_fit, newdata = high_new_data)

low_data <- subset(df_tpc, curve_id == "low")
# get start vals
low_start_vals <- get_start_vals(low_data$temp, low_data$rate, model_name = 'sharpeschoolhigh_1981')
# get limits
low_low_lims <- get_lower_lims(low_data$temp, low_data$rate, model_name = 'sharpeschoolhigh_1981')
low_upper_lims <- get_upper_lims(low_data$temp, low_data$rate, model_name = 'sharpeschoolhigh_1981')

# fit model
low_fit <- nls_multstart(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 15),
                                                     data = low_data,
                                                     iter = 1,
                                                     start_lower = low_start_vals,
                                                     start_upper = low_start_vals,
                                                     lower = low_low_lims,
                                                     upper = low_upper_lims,
                                                     supp_errors = 'Y')

# predict new data
low_new_data <- data.frame(temp = seq(min(low_data$temp), max(low_data$temp), 0.5))
low_preds <- augment(low_fit, newdata = low_new_data)

# calculate topt
low_topt <- get_topt(low_fit)
high_topt <- get_topt(high_fit)
# Calculate the highest point of each curve
low_rmax <- get_rmax(low_fit)
high_rmax <- get_rmax(high_fit)
# calculate ct max and min 
low_ctmax <- get_ctmax(low_fit)
high_ctmax <- get_ctmax(high_fit)
low_ctmin <- get_ctmin(low_fit)
high_ctmin <- get_ctmin(high_fit)
# calculate thermal breadth range
low_tbr <- get_breadth(low_fit)
high_tbr <- get_breadth(high_fit)

# plotting tbr calculation 
# Calculate 80% of the peak rate
rmax_80 <- 0.8 * high_rmax
# Filter the temperatures at or above 80% of the maximum rate
temp_at_80pct <- high_preds$temp[high_preds$.fitted >= rmax_80]
# Determine the minimum and maximum temperatures of this range
tbr_temp_range <- range(temp_at_80pct)
middle_point_high <- mean(tbr_temp_range) # for plotting label 

# thermal breadt hfor low ph
low_rmax_80 <- 0.8 * low_rmax
temp_at_80pct_low <- low_preds$temp[low_preds$.fitted >= low_rmax_80]
tbr_temp_range_low <- range(temp_at_80pct_low)

# Create a tibble with a single fake point for temp color bar
temp_data <- tibble(x = 32, y = 1, diff = 0)

# Define the coordinates for the key square
square_data <- data.frame(
  xmin = 26.5,
  xmax = 28,
  ymin = 97,
  ymax = 100
)


low_preds <- low_preds %>%
  filter(temp <= 24)

high_preds <- high_preds %>%
  filter(.fitted > 5)
TPC_schematic <- # Extend the temperature values from 0 to 32
temperatures <- seq(0, 32, by = 1)

peak_rate_low=150
metabolic_rate_low <- ifelse(temperatures <= 16, 
                             (peak_rate_low / 80) * (4 * temperatures - (temperatures^2 / 18)),  
                             ifelse(temperatures <= 19,  # Starting the decline earlier and ending it sooner at 19°C
                                    peak_rate_low - 5 * (peak_rate_low / 60) * (temperatures - 16)^2.5,  # Steeper and quicker decline
                                    24))  # Set to 24 after 19°C

# Convert to tibble for low pH
metabolic_rate_low <- tibble(temp = temperatures, rate = metabolic_rate_low, curve_id = "low")

peak_rate_high=80
# High pH curve with adjustments at the ends
metabolic_rate_ambient <- ifelse(temperatures <= 22, 
                                 (peak_rate_high / 100) * (5 * temperatures - (temperatures^2 / 22)), 
                                 ifelse(temperatures <= 27, 
                                        peak_rate_high - ((temperatures - 22) / (27 - 22)) * (peak_rate_high - 40), 
                                        10))  # Set to 0 after 27°C

# Convert to tibble for high pH
metabolic_rate_ambient <- tibble(temp = temperatures, rate = metabolic_rate_ambient, curve_id = "high")

# Combine both data frames
df_tpc <- bind_rows(metabolic_rate_ambient, metabolic_rate_low)

high_data <- subset(df_tpc, curve_id == "high")

# get start vals
high_start_vals <- get_start_vals(high_data$temp, high_data$rate, model_name = 'sharpeschoolhigh_1981')
# get limits
high_low_lims <- get_lower_lims(high_data$temp, high_data$rate, model_name = 'sharpeschoolhigh_1981')
high_upper_lims <- get_upper_lims(high_data$temp, high_data$rate, model_name = 'sharpeschoolhigh_1981')

# fit model
high_fit <- nls_multstart(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 15),
                                                     data = high_data,
                                                     iter = 1,
                                                     start_lower = high_start_vals,
                                                     start_upper = high_start_vals,
                                                     lower = high_low_lims,
                                                     upper = high_upper_lims,
                                                     supp_errors = 'Y')
# predict new data
high_new_data <- data.frame(temp = seq(min(high_data$temp), max(high_data$temp), 0.5))
high_preds <- augment(high_fit, newdata = high_new_data)

low_data <- subset(df_tpc, curve_id == "low")
# get start vals
low_start_vals <- get_start_vals(low_data$temp, low_data$rate, model_name = 'sharpeschoolhigh_1981')
# get limits
low_low_lims <- get_lower_lims(low_data$temp, low_data$rate, model_name = 'sharpeschoolhigh_1981')
low_upper_lims <- get_upper_lims(low_data$temp, low_data$rate, model_name = 'sharpeschoolhigh_1981')

# fit model
low_fit <- nls_multstart(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 15),
                                                     data = low_data,
                                                     iter = 1,
                                                     start_lower = low_start_vals,
                                                     start_upper = low_start_vals,
                                                     lower = low_low_lims,
                                                     upper = low_upper_lims,
                                                     supp_errors = 'Y')

# predict new data
low_new_data <- data.frame(temp = seq(min(low_data$temp), max(low_data$temp), 0.5))
low_preds <- augment(low_fit, newdata = low_new_data)

# calculate topt
low_topt <- get_topt(low_fit)
high_topt <- get_topt(high_fit)
# Calculate the highest point of each curve
low_rmax <- get_rmax(low_fit)
high_rmax <- get_rmax(high_fit)
# calculate ct max and min 
low_ctmax <- get_ctmax(low_fit)
high_ctmax <- get_ctmax(high_fit)
low_ctmin <- get_ctmin(low_fit)
high_ctmin <- get_ctmin(high_fit)
# calculate thermal breadth range
low_tbr <- get_breadth(low_fit)
high_tbr <- get_breadth(high_fit)

# plotting tbr calculation 
# Calculate 80% of the peak rate
rmax_80 <- 0.8 * high_rmax
# Filter the temperatures at or above 80% of the maximum rate
temp_at_80pct <- high_preds$temp[high_preds$.fitted >= rmax_80]
# Determine the minimum and maximum temperatures of this range
tbr_temp_range <- range(temp_at_80pct)
middle_point_high <- mean(tbr_temp_range) # for plotting label 

# thermal breadt hfor low ph
low_rmax_80 <- 0.8 * low_rmax
temp_at_80pct_low <- low_preds$temp[low_preds$.fitted >= low_rmax_80]
tbr_temp_range_low <- range(temp_at_80pct_low)

# Create a tibble with a single fake point for temp color bar
temp_data <- tibble(x = 32, y = 1, diff = 0)

# Define the coordinates for the key square
square_data <- data.frame(
  xmin = 26.5,
  xmax = 28,
  ymin = 97,
  ymax = 100
)

heat_colors <- c(
    "#ffffcc", 
    "#ffeda0",
    "#fed976",  
    "#ffd480",
    "#feb24c",  
    "#ff9a17",
    "#fd8d3c",  
    "#fc4e2a",
    "#fc4e2a",  
    "#e31a1c", 
    "red3"
)

low_preds <- low_preds %>%
  filter(temp <= 24)

high_preds <- high_preds %>%
  filter(.fitted > 5)

TPC_schematic <- ggplot() +
  geom_point(data = temp_data, aes(x = x, y = y, fill = diff), color = "grey25", size=1, alpha=.01) +
  #plotting ct max and min for the ambient curve above the low curve 
  geom_segment(aes(x = high_ctmax-3, y = 5, xend = high_ctmax-3, yend = high_rmax[1]* 0.175), linetype = "dotted", col = 'grey25') +
  geom_segment(aes(x = high_ctmax-3, y = high_rmax[1]* 0.35, xend = high_ctmax-3, yend = high_rmax[1]* 0.225), 
               arrow = arrow(type = "closed", length = unit(0.1, "inches")), color = "grey25", size = 0.5) + 
  geom_text(aes(x =  high_ctmax-3, y = high_rmax[1]*0.4), label = TeX("$CT_{max}$"), size=7, color = "grey25", family = "serif") +  # Label for CTmax 
  geom_segment(aes(x=1, y=5, xend = 1, yend = high_rmax[1]*0.175), linetype = "dotted", col = 'grey25') +
  geom_segment(aes(x = 1, y = high_rmax[1]*0.35, xend = 1, yend = high_rmax[1]*0.225), 
               arrow = arrow(type = "closed", length = unit(0.1, "inches")), color = "grey25", size = 0.5) +
  geom_text(aes(x = 1, y = high_rmax[1]*0.4), label = TeX("$CT_{min}$"), size=7, color = "grey25", family = "serif") +  # Label for CTmin
  #plotting the ambient curve 
  geom_line(data = high_preds, aes(temp, .fitted, color = "high"), size = 1, col = 'grey25') + # High metabolic rate curve
  geom_segment(aes(x = high_topt[1], y = 5, xend = high_topt[1], yend = high_rmax[1]), linetype = "dashed", col = 'grey25', size=0.5)+  # Thermal Opt Line
  geom_segment(aes(x=0, y=high_rmax[1], xend = high_topt[1], yend = high_rmax[1]), linetype = "dashed", col = 'grey25', size=0.5) +
  geom_text(aes(x = 1, y = high_rmax[1]), label = TeX("$R_{max}$"), size=7, vjust = -0.5, color = "grey25", family = "serif") +
  geom_text(aes(x = high_topt[1], y = high_rmax[1]), label = TeX("$T_{opt}$"), size=7, vjust = -0.5, color = "grey25", family = "serif") +  # Label for Thermal Optimum
  geom_segment(aes(x = tbr_temp_range[1]+0.25, y = high_rmax[1]*0.8, xend = tbr_temp_range[2]-0.15, yend = high_rmax[1]*0.8), 
               linetype = "dotted", color = "grey25", size = 0.5, arrow = arrow(type = "closed", length = unit(0.15, "inches"), ends = "both"))+
  geom_text(aes(x = middle_point_high[1], y = high_rmax[1]* 0.8), label = TeX("$T_{br}$"), size=7, vjust = 1.3, hjust=0.8, color = "grey25", family = "serif") +
  geom_segment(aes(x = high_topt-6, xend = high_topt-3, y = high_rmax[1]*0.76, yend = high_rmax[1]*0.95), 
                 arrow = arrow(length = unit(0.3, "cm")), color = "grey25") +  # Representing Ea
  geom_text(aes(x = high_topt-5, y = high_rmax[1] * 0.925), label = TeX("$E_{a}$"), size=7, color = "grey25", family = "serif") +
  geom_segment(aes(x = high_topt+3, xend = high_topt+5, y = high_rmax[1]*0.88, yend = high_rmax[1]*0.6), 
                 arrow = arrow(length = unit(0.3, "cm")), color = "grey25") +
  geom_text(aes(x = high_topt+4.5, y = high_rmax[1] * 0.825), label = TeX("$E_{h}$"), size=7, color = "grey25", family = "serif") +
geom_segment(aes(y= 5, yend=5, x=0, xend=36), linetype = "dashed", color = "grey25", linewidth=1) +  # Dotted line at y = 0
#geom_hline(yintercept = 5, linetype = "dashed", color = "grey25", linewidth=1) +  # Dotted line at y = 0
scale_fill_gradientn(colors = heat_colors, limits = c(0, 26), guide = guide_colorbar(title = "Temperature", title.position = "bottom", ticks.colour = NA))+
theme_minimal() +  # Start with a minimal theme
  labs(x=NULL, y = "Metabolic Rate") +  # Add y-axis label
  theme(plot.background = element_blank(),  # Remove background elements
        panel.background = element_blank(),  # Remove panel background elements
        axis.line = element_line(arrow = arrow(type = "closed", length = unit(0.2, "inches")), size = 1, color="grey25"),
        panel.border = element_blank(),
        axis.title.x = element_blank(),  # Remove x-axis title
        axis.title.y = element_text(size = rel(1.5), family="serif"),
        axis.text = element_blank(),
        axis.text.x = element_blank(),  # Optional: Remove x-axis text if desired
        axis.ticks = element_blank(),
        panel.grid = element_blank(),
        text=element_text(size=12, family="serif"),
        legend.position = "bottom",
        legend.direction = "horizontal",
        legend.key = element_blank(),
        legend.key.width = unit(0.17, "npc"),
        legend.key.height = unit(5, "mm"),
        legend.margin = margin(t = 0, r = 0, b = 0, l = 0),
        legend.box.margin = margin(t = -5, r = 0, b = 0, l = 0),
        legend.title = element_text(hjust = 0.5, size = rel(1.4), family="serif"),  # Apply serif font to legend title
        legend.text = element_blank(),  # Apply serif font to legend text
        plot.margin = margin(0.3, 0.3, 0.3, 0.3, "cm"),
        plot.title = element_text(family="serif")) + # Apply serif font to the plot title if you have one
  xlim(0, 36) +
  ylim(0, 80) 

print(TPC_schematic)

TPC_schematic_hypothesis <- ggplot() +
  geom_point(data = temp_data, aes(x = x, y = y, fill = diff), color = "grey25", size=1, alpha=.01) +
  #plotting the low curve first 
  geom_line(data = low_preds, aes(temp, .fitted, color = "low"), size = 1, col = 'cyan3') +     # Low metabolic rate curve
  geom_segment(aes(x = low_topt[1], y = 5, xend = low_topt[1], yend = low_rmax[1]), linetype = "dashed", col = 'cyan3', size=0.5) + 
  geom_segment(aes(x=0, y=low_rmax[1], xend = low_topt[1], yend = low_rmax[1]), linetype = "dashed", col = 'cyan3', size=0.5) +
  geom_segment(aes(x=tbr_temp_range_low[1]+0.25, y = low_rmax[1]*0.8, xend = tbr_temp_range_low[2], yend = low_rmax[1]*0.8), 
               linetype = "dotted", color = "cyan3", size = 0.5)+
  #plotting ct max and min for the ambient curve above the low curve 
  geom_segment(aes(x = high_ctmax-3, y = 5, xend = high_ctmax-3, yend = high_rmax[1]* 0.175), linetype = "dotted", col = 'grey25') +
  geom_segment(aes(x = high_ctmax-3, y = high_rmax[1]* 0.35, xend = high_ctmax-3, yend = high_rmax[1]* 0.225), 
               arrow = arrow(type = "closed", length = unit(0.1, "inches")), color = "grey25", size = 0.5) + 
  geom_text(aes(x =  high_ctmax-3, y = high_rmax[1]*0.4), label = TeX("$CT_{max}$"), size=7, color = "grey25", family = "serif") +  # Label for CTmax 
  geom_segment(aes(x=1, y=5, xend = 1, yend = high_rmax[1]*0.175), linetype = "dotted", col = 'grey25') +
  geom_segment(aes(x = 1, y = high_rmax[1]*0.35, xend = 1, yend = high_rmax[1]*0.225), 
               arrow = arrow(type = "closed", length = unit(0.1, "inches")), color = "grey25", size = 0.5) +
  geom_text(aes(x = 1, y = high_rmax[1]*0.4), label = TeX("$CT_{min}$"), size=7, color = "grey25", family = "serif") +  # Label for CTmin
  #plotting the ambient curve 
  geom_line(data = high_preds, aes(temp, .fitted, color = "high"), size = 1, col = 'orange') + # High metabolic rate curve
  geom_segment(aes(x = high_topt[1], y = 5, xend = high_topt[1], yend = high_rmax[1]), linetype = "dashed", col = 'orange1', size=0.5)+  # Thermal Opt Line
  geom_segment(aes(x=0, y=high_rmax[1], xend = high_topt[1], yend = high_rmax[1]), linetype = "dashed", col = 'orange1', size=0.5) +
  geom_text(aes(x = 1, y = high_rmax[1]), label = TeX("$R_{max}$"), size=7, vjust = -0.5, color = "grey25", family = "serif") +
  geom_text(aes(x = high_topt[1], y = high_rmax[1]), label = TeX("$T_{opt}$"), size=7, vjust = -0.5, color = "grey25", family = "serif") +  # Label for Thermal Optimum
  geom_segment(aes(x = tbr_temp_range[1]+0.25, y = high_rmax[1]*0.8, xend = tbr_temp_range[2]-0.15, yend = high_rmax[1]*0.8), 
               linetype = "dotted", color = "grey25", size = 0.5, arrow = arrow(type = "closed", length = unit(0.15, "inches"), ends = "both"))+
  geom_text(aes(x = middle_point_high[1], y = high_rmax[1]* 0.8), label = TeX("$T_{br}$"), size=7, vjust = 1.3, hjust=0.8, color = "grey25", family = "serif") +
  geom_segment(aes(x = high_topt-6, xend = high_topt-3, y = high_rmax[1]*0.76, yend = high_rmax[1]*0.95), 
                 arrow = arrow(length = unit(0.3, "cm")), color = "grey25") +  # Representing Ea
  geom_text(aes(x = high_topt-5, y = high_rmax[1] * 0.925), label = TeX("$E_{a}$"), size=7, color = "grey25", family = "serif") +
  geom_segment(aes(x = high_topt+3, xend = high_topt+5, y = high_rmax[1]*0.88, yend = high_rmax[1]*0.6), 
                 arrow = arrow(length = unit(0.3, "cm")), color = "grey25") +
  geom_text(aes(x = high_topt+4.5, y = high_rmax[1] * 0.825), label = TeX("$E_{h}$"), size=7, color = "grey25", family = "serif") +
geom_segment(aes(y= 5, yend=5, x=0, xend=36), linetype = "dashed", color = "grey25", linewidth=1) +  # Dotted line at y = 0
#geom_hline(yintercept = 5, linetype = "dashed", color = "grey25", linewidth=1) +  # Dotted line at y = 0
scale_fill_gradientn(colors = heat_colors, limits = c(0, 26), guide = guide_colorbar(title = "Temperature", title.position = "bottom", ticks.colour = NA))+
scale_color_manual(values = c("orange", "cyan3"), 
                     labels = c("Ambient pH", "Decreased pH"), guide = FALSE) +
theme_minimal() +  # Start with a minimal theme
  labs(x=NULL, y = "Respiration Rate") +  # Add y-axis label
  theme(plot.background = element_blank(),  # Remove background elements
        panel.background = element_blank(),  # Remove panel background elements
        axis.line = element_line(arrow = arrow(type = "closed", length = unit(0.2, "inches")), size = 1, color="grey25"),
        panel.border = element_blank(),
        axis.title.x = element_blank(),  # Remove x-axis title
        axis.title.y = element_text(size = rel(1.5), family="serif"),
        axis.text = element_blank(),
        axis.text.x = element_blank(),  # Optional: Remove x-axis text if desired
        axis.ticks = element_blank(),
        panel.grid = element_blank(),
        text=element_text(size=12, family="serif"),
        legend.position = "bottom",
        legend.direction = "horizontal",
        legend.key = element_blank(),
        legend.key.width = unit(0.17, "npc"),
        legend.key.height = unit(5, "mm"),
        legend.margin = margin(t = 0, r = 0, b = 0, l = 0),
        legend.box.margin = margin(t = -5, r = 0, b = 0, l = 0),
        legend.title = element_text(hjust = 0.5, size = rel(1.4), family="serif"),  # Apply serif font to legend title
        legend.text = element_blank(),  # Apply serif font to legend text
        plot.margin = margin(0.3, 0.3, 0.3, 0.3, "cm"),
        plot.title = element_text(family="serif")) + # Apply serif font to the plot title if you have one
  xlim(0, 36) +  # Set x-axis limits
    geom_rect(data = square_data, aes(xmin = xmin, xmax = xmax, ymin = ymin-2.5, ymax = ymax-2.5), 
            fill = "orange", color = "orange") +
  geom_rect(data = square_data, aes(xmin = xmin, xmax = xmax, ymin = ymin-10.5, ymax = ymax-10.5), 
            fill = "cyan3", color = "cyan3") + 
  #geom_rect(data = square_data, aes(xmin = xmin-1, xmax = xmax+7.5, ymin = ymin-14, ymax = ymax+1),
          #  fill = NA, color = "grey25", size = 0.5) + # This will draw the border around the square
  annotate("text", x = square_data$xmax+3.1, y = square_data$ymax-3.75, family = "serif", label = "Ambient pH", size=5, color = "grey25") +
  annotate("text", x = square_data$xmax+3.65, y = square_data$ymax-11.75, family = "serif", label = "Decreased pH", size=5, color = "grey25")

print(TPC_schematic_hypothesis)

ggsave(here::here("Figures", "TPC_schematic.png"), plot = TPC_schematic, width = 8, height = 6, dpi = 1300)
ggsave(here::here("Figures", "TPC_schematic_hypothesis.png"), plot = TPC_schematic_hypothesis, width = 8, height = 6, dpi = 1300)

```


```{r Clearing Environment, message=FALSE, include=FALSE} 
# Clear the environment
rm(list = ls())
```

# Study Site Map 
Mapping Point Fermin study site 

```{r Study-Site-Map, fig.height=6, fig.width=4.5, message=FALSE, warning=FALSE, fig.align="center"}

# Define study site coordinates and labels
center_lon <- -118.28599
center_lat <- 33.70679092
location <-  "(Point Fermin State Beach, CA)"

# Define regional area
regional_lonlim <- c(-126, -114)
regional_latlim <- c(28, 44)

# Expand the limits by on each side
studysite_lonlim <- c(floor(center_lon)-0.5, ceiling(center_lon)+0.5)
studysite_latlim <- c(floor(center_lat)-0.5, ceiling(center_lat)+0.5)

# Define bounding box coordinates for the west coast 
regional_bbox <- st_bbox(c(xmin = regional_lonlim[1], ymin = regional_latlim[1], 
                         xmax = regional_lonlim[2], ymax = regional_latlim[2]), crs = st_crs(4326)) 
# create a bounding box polygon
regional_polygon <- st_as_sfc(regional_bbox)

# Define the bounding box as an sf object for the study site
studysite_bbox <- st_bbox(c(xmin = studysite_lonlim[1], ymin = studysite_latlim[1], 
                         xmax = studysite_lonlim[2], ymax = studysite_latlim[2]), crs = st_crs(4326))
studysite_polygon <- st_as_sfc(studysite_bbox)

# Loading data  for regional and study site maps  
sf_use_s2(FALSE) # to fix sf error)

# loading land data, repairing invalid geometries and subsetting to map bounding box
land <- ne_countries(scale = 10, returnclass = "sf")
land_data <- land[c("geometry", "continent", "name")] %>%  mutate(type = "Land", type2= "Boundary", label=name, .keep="unused")
land_data_valid <- st_is_valid(land_data, NA_on_exception = TRUE) 
if (any(is.na(land_data_valid) | !land_data_valid)) {
  land_data <- st_make_valid(land_data)}
regional_land_data <- st_intersection(land_data, regional_polygon)

# loading coastline data, repairing invalid geometries and subsetting to map bounding box
coastlines <- ne_coastline(scale = 10, returnclass = "sf")
coastline_data <- coastlines["geometry"] %>% mutate(type = "Land", type2= "Boundary")
coastline_data_valid <- st_is_valid(coastline_data, NA_on_exception = TRUE)
if (any(is.na(coastline_data_valid) | !coastline_data_valid)) {
  coastline_data <- st_make_valid(coastline_data)}
regional_coastline_data <- st_intersection(coastline_data, regional_polygon)

# loading rivers data, repairing invalid geometries and subsetting to map bounding box
rivers <- ne_download(scale = 10, type = "rivers_lake_centerlines", category = "physical", returnclass = "sf")
rivers_data <- rivers[c("geometry", "rivernum", "label")] %>% mutate(type = "Water", type2="Boundary")
rivers_data_valid <- st_is_valid(rivers_data, NA_on_exception = TRUE)
if (any(is.na(rivers_data_valid) | !rivers_data_valid)) {
  rivers_data <- st_make_valid(rivers_data)}
regional_rivers_data <- st_intersection(rivers_data, regional_polygon)

# loading lakes data, repairing invalid geometries and subsetting to map bounding box
lakes <- ne_download(scale = 10, type = "lakes", category = "physical", returnclass = "sf")
lakes_data <- lakes[c("geometry", "label")] %>% mutate(type = "Water", type2="Boundary") 
lakes_data_valid <- st_is_valid(lakes_data, NA_on_exception = TRUE)
if (any(is.na(lakes_data_valid) | !lakes_data_valid)) {
  lakes_data <- st_make_valid(lakes_data)}
regional_lakes_data <- st_intersection(lakes_data, regional_polygon)

#getting high resolution study site data from GADM
#gadm_data <- raster::getData('GADM', country='USA', level=0)
rds_file <- here::here("Data", "GADM", "gadm_USA_map_level0_sp.rds")
gadm_data <- readRDS(rds_file)

studysite_land_data <- st_as_sf(gadm_data, crs=st_crs(4326)) %>%
  st_intersection(studysite_polygon, dimension="polygon") %>% mutate(type = "Land", type2= "Boundary")
studysite_coastline_data <- st_boundary(studysite_land_data)

# create the point and text for the study site (in the same coordinate system as the map)
study_site <- st_sfc(st_point(c(center_lon, center_lat)), crs = st_crs(4326))
study_site_text <- st_sfc(st_point(c(center_lon, center_lat*1.005)), crs = st_crs(4326)) %>% # nudge up
  st_sf(data.frame(location = location), crs = st_crs(4326))
study_site_text_2 <- st_sfc(st_point(c(center_lon, center_lat*1.0075)), crs = st_crs(4326)) %>% # nudge up
  st_sf(data.frame(location = "STUDY SITE"), crs = st_crs(4326))

# Download and process ocean labels
ocean_labels <- ne_download(type = "geography_marine_polys", category = "physical", scale = 10) %>% 
  st_transform(crs = 4326) %>% 
  st_intersection(regional_polygon) %>%
  st_centroid() %>%
  dplyr::select(label, name_en, geometry)

# Download and process geography labels
geography_labels <- ne_download(type = "geography_regions_polys", category = "physical", scale = 10) %>% 
  st_transform(crs = 4326) %>% 
  st_intersection(regional_polygon) %>%
  st_centroid() %>%
  dplyr::mutate(label = LABEL, name_en=NAME_EN) %>%
  dplyr::select(label, name_en, geometry)

# Combine and categorize labels
all_labels <- rbind(ocean_labels, geography_labels) %>%
  mutate(is_capitalized = toupper(label) == label) %>% 
  mutate(label = ifelse(is_capitalized == FALSE, name_en, label)) %>% 
  dplyr::select(-name_en)

pacific_ocean_index <- which(all_labels$label == "NORTH PACIFIC OCEAN")
all_labels$geometry[pacific_ocean_index] <- st_sfc(st_point(c(st_coordinates(all_labels[pacific_ocean_index, ])[1, 1] - 2, 
                                                              st_coordinates(all_labels[pacific_ocean_index, ])[1, 2] + 2)), 
                                                   crs = st_crs(all_labels))

continental_labels <- which(all_labels$label == "NORTH AMERICA" | all_labels$label == "NORTH PACIFIC OCEAN")
continental_labels <- all_labels[continental_labels, ] %>% 
  mutate(label = case_when(label == "NORTH AMERICA" ~ "NORTH\nAMERICA",
                          label == "NORTH PACIFIC OCEAN" ~ "NORTH\nPACIFIC OCEAN"))

inset_labels <- which(all_labels$label == "Channel Islands of California")
inset_labels <- all_labels[inset_labels, ] %>% 
  mutate(label = "Channel\nIslands")

# Filter out the "ROCKY MOUNTAINS" label
all_labels <- filter(all_labels, label != "ROCKY MOUNTAINS" & label != "NORTH AMERICA"
                     & label != "NORTH PACIFIC OCEAN" & label != "Channel Islands of California")

all_labels <- all_labels %>%
  mutate(label = str_replace_all(label, " ", "\n"))


# Create a regional map with an inset study site
study_site_map <- ggplot() +
  # mapping the regional background data to create boundaries for the map
  #geom_sf(regional_background, mapping=aes(fill=type, color=type), linewidth = 0.2) +
  
  # mapping the regional land and coastline data to create the land area and boundaries
  geom_sf(regional_land_data, mapping=aes(fill=type, color=type), linewidth = 0.2) +
  geom_sf(regional_coastline_data, mapping=aes(color=type2, fill=type2), linewidth = 0.1, alpha=0.8) +
  
    #shading the overlays to add a 3 dimensional look 
  geom_sf(regional_coastline_data, mapping=aes(color=type2),
          fill=NA, color = "darkolivegreen", linewidth = 1, alpha = 0.05) + 
  geom_sf(regional_coastline_data, mapping=aes(color=type2), fill=NA, color = "darkolivegreen", linewidth = 3, alpha = 0.05) +
  geom_sf(regional_lakes_data, mapping=aes(color=type2),
          fill = NA, color="darkolivegreen", size=1.2, linewidth=0.15, alpha=0.05) + 
  geom_sf(regional_rivers_data, mapping=aes(color=type2),
          fill = NA, color="darkolivegreen", size=1.2, linewidth=0.5, alpha=0.75) +
  geom_sf(regional_rivers_data, mapping=aes(color=type2),
          fill=NA, color = "darkolivegreen", size=1.2, linewidth=1, alpha=0.05) + 
  geom_sf(regional_rivers_data, mapping=aes(color=type2),
          fill=NA, color = "darkolivegreen", size=1.2, linewidth=2, alpha=0.05) +
  geom_sf(regional_rivers_data, mapping=aes(color=type2),
          fill=NA, color = "darkolivegreen3", size=1.2, linewidth=1.5, alpha=0.05) +
  geom_sf(regional_coastline_data, mapping=aes(color=type), fill=NA, color = "darkolivegreen3", linewidth = 1.5, alpha = 0.05) +
  geom_sf(regional_rivers_data, mapping=aes(color=type),
          fill=NA, color = "white", size=1.25, linewidth=0.5, alpha=0.05) +
  geom_sf(regional_coastline_data, mapping=aes(color=type),
          fill=NA, color = "white", linewidth = 1.5, alpha = 0.05) +
  geom_sf(regional_coastline_data, mapping=aes(color=type),
          fill=NA, color = "grey20", size=1.5, linewidth=0.075, alpha=0.05) +
  
  # mapping the regional rivers and lakes data with shading (shading first then overlaying river and and lakes)
  geom_sf(regional_rivers_data, mapping=aes(color=type, fill=type), size=0.8, linewidth=0.4) + #inserting rivers
  geom_sf(regional_lakes_data, mapping=aes(color=type, fill=type), size=0.8, linewidth=0.1) + #inserting lakes

  # Adding labels with conditional styling, size, and nudging
  geom_sf_text(data = all_labels, mapping = aes(geometry = geometry, label = label, 
                                              fontface = ifelse(is_capitalized, "bold", "italic")),
             family = "serif", 
             size = ifelse(all_labels$is_capitalized, 1.5,  1.8),  # Larger size for capitalized
             color = "grey30", 
             check_overlap = TRUE, 
             nudge_x = ifelse(all_labels$is_capitalized, 0.5, 0),  # More nudge for capitalized
             nudge_y = ifelse(all_labels$is_capitalized, 0.1, 0)) + # More nudge for capitalized
  geom_sf_text(data= continental_labels, mapping = aes(geometry = geometry, label = label), family = "serif", 
              fontface="bold", size = 2.5, color = "grey30", check_overlap = TRUE, nudge_x = 1) +
  geom_sf_text(data= inset_labels, mapping = aes(geometry = geometry, label = label), family = "serif",
              fontface = "italic", size = 1.5, color = "grey30", check_overlap = TRUE, nudge_x = 0.5, nudge_y=-0.75) +
  
  # adding geom sf insets to create the inset plot 
  geom_inset_frame(target.aes = list(fill = "cyan3")) +
  geom_sf_inset(studysite_land_data, mapping=aes(fill=type), color=NA, linewidth = 0.1, map_base = "none") +
  geom_sf_inset(studysite_coastline_data, mapping=aes(color=type2),
                fill=NA, linewidth = 0.15, alpha = 0.3, map_base = "none") +
  geom_sf_inset(studysite_coastline_data, mapping=aes(color=type),
                fill=NA, color = "darkolivegreen", size=0.8, linewidth = 1, alpha = 0.05, map_base = "none") +
  geom_sf_inset(studysite_coastline_data, mapping=aes(color=type),
                fill=NA, color = "darkolivegreen", size=0.8, linewidth = 3, alpha = 0.05, map_base = "none") +
  geom_sf_inset(studysite_coastline_data, mapping=aes(color=type),
                fill=NA, color = "darkolivegreen3", size=0.8, linewidth = 2, alpha = 0.05, map_base = "none") +
  geom_sf_inset(studysite_coastline_data, mapping=aes(color=type),
                fill=NA, color = "white", size=0.6, linewidth = 0.5, alpha = 0.05, map_base = "none") +
  geom_sf_inset(studysite_coastline_data, mapping=aes(color=type),
                fill=NA, color = "grey20", size=1,linewidth = 0.075, alpha = 0.05, map_base = "none") +
  #insert study site point 
  geom_sf_text_inset(study_site_text_2, label = "STUDY SITE", mapping = aes(), size = 3, 
                  color = 'grey30', family = "serif",
                  fontface="bold", map_base="none", y_nudge = -1) +
  geom_sf_text_inset(study_site_text, label = location, mapping = aes(), size = 2, 
                  color = 'grey30', family = "serif",
                  fontface="bold", map_base="none", y_nudge = 1) +
  geom_sf_inset(study_site, mapping=aes(), color="orange", size=0.01, map_base = "none") +
  geom_inset_frame()+
  geom_rect(data = data.frame(), aes(xmin = regional_lonlim[1], xmax = regional_lonlim[2],
  ymin = regional_latlim[1], ymax = regional_latlim[2]), fill = NA, colour = "grey30", size=1.2, linewidthe=0.2) +
  coord_sf_inset(inset = configure_inset(centre = study_site, scale = 5, units = "km",
                                         translation = c(-360, -280), radius = 60), expand=FALSE, clip="on")  +
  scale_fill_manual(values = c("Land" = "darkolivegreen3", "Boundary" = "#6B8E23", "Water" = "cyan3")) +
  scale_color_manual(values = c("Land" = "darkolivegreen3", "Boundary" = "#6B8E23", "Water" = "cyan3")) +
  scale_x_continuous(breaks = seq(st_bbox(regional_bbox)["xmin"], st_bbox(regional_bbox)["xmax"], by = 4),
                     limits = c(st_bbox(regional_bbox)["xmin"], st_bbox(regional_bbox)["xmax"]), expand=c(0,0)) +
  scale_y_continuous(breaks = seq(st_bbox(regional_bbox)["ymin"], st_bbox(regional_bbox)["ymax"], by = 4),
                     limits = c(st_bbox(regional_bbox)["ymin"], st_bbox(regional_bbox)["ymax"]), expand=c(0,0)) +
  xlab("Longitude") + ylab("Latitude") +
  theme_minimal(base_family="serif") +
  theme(plot.background = element_rect(fill = "white", colour = "white"),
    panel.background= element_rect(fill="cyan3"), panel.grid = element_blank(), panel.border = element_blank(),
    legend.position = "none", 
    axis.title.x = element_text(margin = margin(t = 7.5), size = 13, hjust = 0.5, family="serif"),
    axis.title.y = element_text(margin = margin(r = 7.5), size = 13, vjust = 0.5, family="serif"),  # Same size for both axes
    axis.text = element_text(size = 9, family="serif"),
    plot.margin = margin(t = 1, r = 1, b = 1, l = 1, unit = "cm")) +
  #spatial aware north arrow placement and specification
  annotation_north_arrow(location = "tr", pad_y = unit(0.35, "cm"), pad_x = unit(0.3, "cm"),
                         width=unit(0.6, "cm"), height = unit(0.8, "cm"), which_north = "grid",
                         style = north_arrow_fancy_orienteering(text_family = "serif", text_size=8,
                         text_col="grey30", fill=c("grey30", "white"))) +
   # spatial-aware automagic scale bar
  annotation_scale(location = "br", style="ticks", height = unit(0.15, "cm"), text_family = "serif", pad_x = unit(0.4, "cm")) 

print(study_site_map)

# Save the plot
ggsave(filename = here("Figures", "study_site_map.png"), plot = study_site_map, width = 4.5, height = 6, dpi = 1600)
```

# Analyzing Study Site Environmental Data


Temperature & pH Data from UCSD Shore Stations 1924-2023 (Newport Beach Pier)
 
Carter, Melissa L.; Flick, Reinhard E.; Terrill, Eric; Beckhaus, Elena C.; Martin, Kayla; Fey, Connie L.; Walker, Patricia W.; Largier, John L.; McGowan, John A. (2022). Shore Stations Program, Newport Beach - Balboa Pier. In Shore Stations Program Data Archive: Current and Historical Coastal Ocean Temperature and Salinity Measurements from California Stations. UC San Diego Library Digital Collections. https://doi.org/10.6075/J0GX4BCP

Temperature and pH data provided by the Shore Stations Program sponsored at Scripps Institution of Oceanography by California Department of Parks and Recreation, Natural Resources Division,  Award# C1670003, and pH data provided by the Southern California Coastal Ocean Observing System.

## Temperature Study Site Data 
```{r}
library(climate)
library(ggplot2)
library(ggthemes)


co2 = meteo_noaa_co2()
head(co2)
co2$date = ISOdate(co2$yy, co2$mm, 1)
ggplot(co2, aes(date, co2_avg)) + 
  geom_line()+ geom_smooth()+
  theme_bw()+
  labs(
    title = "Carbon Dioxide (CO2)",
    subtitle = paste0("Mauna Loa Observatory "),
    caption = "data source: NOAA",
    x = "",
    y = "ppm"
) Metabolism as a Function of Mass and Body Temperature
Description
The function estimates basal (or resting) metabolic rate (W) as a function of mass (g) and temperature (C). The function is based on empirical data and the metabolic theory of ecology (assumes a 3/4 scaling exponent) \insertCiteGillooly2001TrenchR.
Description
Creates a Pcrit plot (the threshold below which oxygen consumption rate can no longer be sustained) based on paired PO2 and MO2 values. Five Pcrit metrics are plotted: the traditional breakpoint metric (broken stick regression, black), the nonlinear regression metric (Marshall et al. 2013, green), the sub-prediction interval metric (Birk et al. 2019, red), the alpha-based Pcrit method (Seibel et al., 2021, blue), and the linear low O2 (LLO) method (Reemeyer & Rees 2019, purple). For details on how the Pcrit values are calculated, see calc_pcrit.
Calculate the oxygen supply capacity (alpha)
Description
The oxygen supply capacity (
𝛼
α) is a species- and temperature-specific value quantifying an animal's ability to extract oxygen from the ambient medium to support its metabolism (e.g. umol O2 / g / hr / kPa). This function calculates 
𝛼
α based on the single highest 
𝛼
0
α0 (MO2/PO2) value in the dataset. If there are outliers that make this prohibitive, consider setting a threshold MO2 value with mo2_threshold.

Usage
calc_alpha(po2, mo2, avg_top_n = 1, MR = NULL, mo2_threshold = Inf)
Usage
plot_pcrit(
  po2,
  mo2,
  avg_top_n = 1,
  level = 0.95,
  iqr = 1.5,
  NLR_m = 0.065,
  MR = NULL,
  mo2_threshold = Inf,
  showNLRs = FALSE,
  ...
Usage
Qmetabolism_from_mass_temp(m, T_b, taxon)
Arguments
m	
numeric mass (grams).

T_b	
numeric body temperature (C).

taxon	
character organism type. Options: "bird", "mammal", "reptile", "amphibian", "invertebrate".

Value
numeric basal metabolism (W).



 lmeresampler
```

```{r}
```






```{r Temperature Data, message=FALSE, warning=FALSE, fig.height=3, fig.width=4.5, fig.align="center"}

# Read the CSV file
newport_temp <- read_csv(here("Data", "Site_Data", "NewportBeach_TEMP_1924-2023.csv"))

# Data Cleaning and Transformation
# Filter data based on TEMP_FLAG and SURF_TEMP_C values, then create a Date column
newport_temp_data <- newport_temp %>%
  filter(is.na(TEMP_FLAG) | TEMP_FLAG %in% c(0, 5, 6)) %>%
  filter(between(SURF_TEMP_C, 5, 35)) %>%
  mutate(Date = make_date(YEAR, MONTH, DAY)) %>%
  dplyr::select(Date, Temperature = SURF_TEMP_C)

filtered_temp_data <- newport_temp_data %>% 
   dplyr::filter(between(Date, as.Date("2020-01-01"), as.Date("2024-01-01"))) 
  
# Calculate Daily Mean Temperature
daily_temp_stats <- filtered_temp_data %>%
  group_by(Date) %>%
  summarise(Mean_Temp = mean(Temperature, na.rm = TRUE)) %>% 
  na.omit()

#Calculate Monthly Mean Temperature
monthly_temp_stats <- daily_temp_stats %>%
  mutate(Month = lubridate::month(Date, label = TRUE, abbr = TRUE))

ym_temp_stats_summary <- newport_temp_data %>%
  mutate(Month = lubridate::month(Date, label = TRUE, abbr = TRUE)) %>% 
  mutate(YM= zoo::as.yearmon(Date)) %>%
  group_by(Month) %>%
  group_by(YM) %>%
  summarise(
    Mean = mean(Temperature, na.rm = TRUE),
    SD = sd(Temperature, na.rm = TRUE),
    Min = min(Temperature, na.rm = TRUE),
    Max = max(Temperature, na.rm = TRUE),
    N = n(),
    SE = SD / sqrt(N),
    Lower_95CI = Mean - qt(0.975, N - 1) * SE,
    Upper_95CI = Mean + qt(0.975, N - 1) * SE) %>%
  arrange(YM)

# Plotting
temperature_timeseries <- ggplot(ym_temp_stats_summary, aes(x = as.Date(YM), y = Mean)) +
  geom_line(aes(color = Mean), size = 0.25) +
  geom_smooth(method = "lm", se = FALSE, color = "red3", size=0.5) +
  scale_color_gradient(low = "#fed911", high = "red2", limits = c(10, 25), breaks = seq(10, 25, by = 5), name = "Temp") +
  scale_x_date(date_breaks = "10 years", date_labels = "%Y") +
  scale_y_continuous(limits = c(10, 24), breaks = seq(10, 24, by = 2), expand=c(0,0)) +
  common_theme +
  labs(y = "Temperature (°C)", x="Date", color = "Temp (°C)", 
       title = "", subtitle = "")

# Overall and Experimental Mean Temperature Calculation
overall_mean_temp <- mean(daily_temp_stats$Mean_Temp, na.rm = TRUE)
overall_se_temp <- sd(daily_temp_stats$Mean_Temp, na.rm = TRUE) / sqrt(nrow(daily_temp_stats))
experimental_mean_temp <- daily_temp_stats %>%
  filter(between(Date, as.Date("2022-08-01"), as.Date("2022-09-30"))) %>%
  summarise(Experimental_Mean_Temp = mean(Mean_Temp, na.rm = TRUE)) %>%
  pull(Experimental_Mean_Temp)

# Print mean temperatures
cat("Overall mean temperature:", round(overall_mean_temp, 2), "°C\n")
cat("Experimental (Duration) mean temperature:", round(experimental_mean_temp, 2), "°C\n")

hline_data <- data.frame(yintercept = seq(12, 26, by = 2))
hline_data$color_value <- as.factor(hline_data$yintercept)  # gradient mapping
hline_data$yintercept <- hline_data$yintercept


# Define the colors, taking the first eight colors from your list
heat_colors <- c(
   "#fed976", "#ffd480", "#feb24c", "#ff9a17", "#fd8d3c", "#fc4e2a", "#e31a1c", "red3"
)

# Modify the histogram plotting code to reflect these changes
histogram_temp <- gghistostats(
  data = daily_temp_stats,
  x = Mean_Temp,
  binwidth = 0.5,
  bin.args = list(color="#A40000", fill = "#FF4D4D",
                  linewidth = 0.225, alpha=0.85),
  xlab = "Sea Surface Temperature (°C)",
  ylab = "Frequency",
  type = "parametric",
  centrality.plotting = TRUE,
  centrality.type = "parametric",
  centrality.line.args = list(color = "#A40000", size = 0.6, linetype = "dashed"),
    centrality.label.args = list(
      size = 5,  # Smaller font size for centrality labels
      color = "grey25"  # Example color setting
    ),
  results.subtitle = FALSE,
  ggplot.component= list(geom_vline(data = hline_data, aes(xintercept = yintercept,
                                                           color = color_value),
                               linewidth = 0.4, linetype="dashed")))  +
  scale_color_manual(values = heat_colors) +
xlab("Temperature (°C)") +
ylab("Frequency") +
scale_y_continuous(
  name = "Frequency",
  limits=c(0,125),
  sec.axis = sec_axis(~., name = NULL, labels=NULL), expand=c(0,0)) +
theme(plot.title = element_blank(), plot.subtitle = element_blank(), legend.position = "none") +
common_theme

# Ensure the Month column is a factor and in the right order for plotting
monthly_temp_stats$Month <- factor(monthly_temp_stats$Month,
levels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"))

# Create the plot
temp_violin_plot <- ggplot(monthly_temp_stats, aes(x = Month, y = Mean_Temp, fill=Month)) +
  geom_hline(data = hline_data, aes(yintercept = yintercept,
                                    color = color_value), size = 0.3) +
 # geom_violin(trim = FALSE, scale="width", color="#8B0000", linewidth = 0.25) +
  geom_violin(trim = FALSE, scale="width", color="#A40000", fill="#FF4D4D",
              linewidth = 0.225,  alpha=0.85) +
  geom_boxplot(width = 0.2, outlier.shape = NA, color=NA, fill="white",
               alpha=0.55) +
  geom_boxplot(width = 0.2, outlier.shape = NA, color="#A40000", fill=NA, linewidth = 0.2) +
  scale_color_manual(values = heat_colors) +
  common_theme +
  scale_y_continuous(breaks = seq(10, 28, by = 2), limits = c(10, 28), expand=c(0,0)) +
  #geom_text(stat="count", aes(label=paste0("(N=",..count..,")")), y=.35*max(monthly_temp_stats$Mean_Temp), family = "serif", 
           # color="grey25", size =3) +
  labs(x = NULL, y = "Temperature (°C)") 

# Display the plots
print(histogram_temp)
print(temp_violin_plot)
print(temperature_timeseries)

# Assuming 'histogram_temp' is your ggplot object
ggsave(filename = here("Output", "temp_timeseries.png"), plot = temperature_timeseries, width = 4.5, height = 3, dpi = 1200)
ggsave(filename = here("Figures", "daily_temp_histogram.png"), plot = histogram_temp, width = 4.5, height = 3, dpi = 1200)
ggsave(filename = here("Figures", "monthly_temp_violin_heat_colors.png"), plot = temp_violin_plot, width = 4.5, height = 3, dpi = 1200)


```

## pH Study Site Data 

```{r pH Data, warning=FALSE, message=FALSE, fig.height=3, fig.width=4.5, fig.align="center"}

# Read the CSV file
newportbeach_pH <- read_csv(here("Data", "Site_Data", "Newport_pH_2020-2024.csv"))
# Read and preprocess data
newportbeach.pH.hourly <- read_csv(here("Data", "Site_Data", "Newport_pH_2005-2024.csv"))

# Data Cleaning and Transformation for pH data
filtered_pH_data <- newportbeach_pH %>%
    mutate(pH = sea_water_ph_reported_on_total_scale_salinity_corrected,
         time = as.POSIXct(time, format = "%Y-%m-%dT%H:%M:%SZ", tz = "UTC"),
         Date = as.Date(time)) %>%
  filter(sea_water_ph_reported_on_total_scale_salinity_corrected_qc_agg == 1) %>% 
  filter(Date >= as.Date("2020-01-01") & Date <= as.Date("2024-01-01")) %>% 
  dplyr::select(Date, time, pH) %>%  # Rename PH to pH for clarity
  na.omit()

filtered.newportbeach.pH.hourly <- newportbeach.pH.hourly %>%
   mutate(pH = sea_water_ph_reported_on_total_scale_salinity_corrected,
         time = as.POSIXct(time, format = "%Y-%m-%dT%H:%M:%SZ", tz = "UTC"),
         Date = as.Date(time)) %>%
  filter(sea_water_ph_reported_on_total_scale_salinity_corrected_qc_agg == 1) %>% 
  filter(Date >= as.Date("2020-01-01") & Date <= as.Date("2024-01-01")) %>% 
  filter(pH > 7.25 & pH <= 8.5) %>%
  dplyr::select(Date, time, pH) %>%
  arrange(time) %>% 
  mutate(gap = c(0, diff(time) > hours(6)),  # Create a logical vector where TRUE indicates a gap
         group = cumsum(gap)) %>% na.omit()

# Calculate 3-hour mean and confidence intervals
six_hour_ph_stats <- filtered.newportbeach.pH.hourly %>%
  mutate(SixHourInterval = floor_date(time, "6 hours")) %>%
  group_by(group, SixHourInterval) %>%
  summarise(
    Mean = mean_pH(pH, na.rm = TRUE),
    SD = sd(pH, na.rm = TRUE),
    N = n(),
    SE = ifelse(N > 1, SD / sqrt(N), NA),
    Lower_95CI = ifelse(N > 1, Mean - qt(0.975, N - 1) * SE, NA),
    Upper_95CI = ifelse(N > 1, Mean + qt(0.975, N - 1) * SE, NA)
  ) %>%
  ungroup() %>%  # Ungroup for plotting
  na.omit()

# Calculate Daily Mean pH
daily_pH_stats <- filtered_pH_data %>%
  group_by(Date) %>%
  summarise(Mean_pH = mean_pH(pH, na.rm = TRUE)) %>% 
  na.omit() %>% 
  filter(Mean_pH > 7.25 & Mean_pH < 8.5)

#Calculate Monthly Mean pH (Adjust column names as necessary)
monthly_pH_stats <- daily_pH_stats %>%
  mutate(Month = lubridate::month(Date, label = TRUE, abbr = TRUE)) %>% 
  na.omit()

# mean pH calculations 
overall_mean_pH <- mean(daily_pH_stats$Mean_pH, na.rm = TRUE)
# Print mean temperatures
cat("Overall mean pH:", round(overall_mean_pH, 2), "°C\n")

# pH plot timeseries with IQR lines and overall mean
pH_timeseries <- ggplot(six_hour_ph_stats, aes(x = SixHourInterval, y = Mean, group=group)) +
  geom_line(aes(color = Mean), size = 0.25) +
  geom_hline(yintercept = overall_mean_pH, linetype = "dashed",
             color = "#036C96FF", size = 0.5,
             alpha=0.5) +
  scale_color_gradient(low = "#027BABFF", high = "#009ACD",
                       limits = c(7.25, 8.5), breaks = seq(7.325, 8.5, by = 0.25), name = "pH") +
  scale_x_datetime(date_breaks = "1 year", date_labels = "%b %Y") +
  scale_y_continuous(limits = c(7.25, 8.5), breaks = seq(7.25, 8.5, by = 0.25), expand=c(0,0)) +
  common_theme +
  labs(x = "Date", y = TeX("$pH_{T}$"),
       title = "", subtitle = "")

# 'pH_values' with experimental treatment values
pH_values <-  data.frame(xintercept = c(7.7, 7.9))  # Experiential treatment values
pH_values$color_value <- as.factor(pH_values$xintercept)  # gradient mapping
max_value_pH <- max(daily_pH_stats$Mean_pH, na.rm = TRUE) 
total_observations_pH <- nrow(daily_pH_stats)

pH_colors_treatments = c("cyan3", "orange")

# Now, creating the histogram for pH
histogram_pH <- gghistostats(
  data = daily_pH_stats,
  x = Mean_pH,
  binwidth = 0.04,
  bin.args = list(color = "#027BABFF", fill = "#009ACD", linewidth=0.225, alpha=0.95),
  xlab = TeX("$pH_{T}$"),
  ylab = "Frequency",
  type = "parametric",
  centrality.plotting = TRUE,
  centrality.type = "parametric",
  centrality.line.args = list(color = "#036C96FF", size = 0.6, linetype = "dashed"),
  centrality.label.args = list(
      size = 5,  # Smaller font size for centrality labels
      color = "grey25"  # Example color setting
    ),
  results.subtitle = FALSE, 
  ggplot.component = list(
    geom_vline(data = pH_values, aes(xintercept = xintercept, color = color_value),
               linetype = "dashed", linewidth = 0.4))) +
  scale_color_manual(values = pH_colors_treatments) +
  xlab(TeX("$pH_{T}$")) +
  ylab("Frequency") +
  scale_y_continuous(name = "Frequency",
  limits = c(0, 350), breaks = seq(0, 350, by = 50),
  sec.axis = sec_axis(~., name = NULL, labels=NULL), expand=c(0,0)) +
theme(plot.title = element_blank(), plot.subtitle = element_blank(), legend.position = "none") +
common_theme

# Ensure the Month column is a factor and in the right order for plotting
monthly_pH_stats$Month <- factor(monthly_pH_stats$Month,
levels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"))

# Adapting the plotting steps for pH data
pH_violin_plot <- ggplot(monthly_pH_stats, aes(x = Month, y = Mean_pH, fill= Month)) +
  geom_hline(data = pH_values, aes(yintercept = xintercept, color = color_value),
             size = 0.3) +
 # geom_violin(trim = FALSE, scale="width", color="grey25", linewidth = 0.2) +
  geom_violin(trim = FALSE, scale="width", color="#027BABFF", fill="#0098D4FF",
              linewidth = 0.225) +
  geom_boxplot(width = 0.2, outlier.shape = NA, color=NA, fill="white",
               alpha=0.5) +
  geom_boxplot(width = 0.2, outlier.shape = NA, color="#036C96FF", fill=NA,
               alpha=0.6, linewidth = 0.2) +
  scale_color_manual(values = pH_colors_treatments) +
  common_theme +
  theme(plot.title = element_blank(),
        plot.subtitle = element_blank(),legend.position = "none"
       # plot.margin = margin(t = 0.5, r = 2, b = 0.5, l = 0.5, unit="cm")
        ) +
  xlab("Month") + 
  ylab(TeX("$pH_{T}$")) +
  #geom_text(stat="count", aes(label=paste0("(N=",..count..,")")),
            #y=.8612*max(monthly_pH_stats$Mean_pH), family = "serif",
            #color="grey25", size =3) +
  scale_y_continuous(breaks = seq(7.25, 8.5, by = 0.25),
                     limits=c(7.25,8.5), expand=c(0,0)) # +
  #expand_limits(y=.86125*max(monthly_pH_stats$Mean_pH))

# Print the plots
print(pH_timeseries)
print(pH_violin_plot)
print(histogram_pH)

# Save the plot if needed
ggsave(filename = here("Figures", "monthly_pH_violin_plot.png"), plot = pH_violin_plot, width = 4.5, height = 3, dpi = 1200)
ggsave(filename = here("Figures", "daily_pH_histogram.png"), plot = histogram_pH, width = 4.5, height = 3, dpi = 1200)
ggsave(filename = here("Figures", "pH_timeseries.png"), plot = pH_timeseries, width = 4.5, height = 3, dpi = 1200)

```

## Combining Plot 

```{r, warning=FALSE, message=FALSE}

histogram_temp <- histogram_temp + 
  theme(plot.margin =margin(l = 0.2, r = 0.2, t = 0.8, b = 0.2, unit = "cm"),
        axis.title.y = element_text(size = 13, margin = margin(r = 10)),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 13),
        axis.text.x = element_text(size = 9)
        ) 
histogram_pH <- histogram_pH +
  theme(plot.margin =margin(l = 0.2, r = 0.2, t = 0.8, b = 0.4, unit = "cm"),
        axis.title.y = element_text(size = 13, margin=margin(r = 15)),
        axis.text.y = element_text(size = 9),
        axis.title.x = element_text(size = 13),
        axis.text.x = element_text(size = 9)
        )


combined_histograms <- cowplot::plot_grid(histogram_temp, histogram_pH,
                                          labels = c("(A)", "(B)"),
  label_size = 16,  # Adjust label size
  hjust = -0.15,  # Adjust horizontal justification
  label_fontfamily = "serif",  # Set label font family to serif
  axis = 'tblr',  # Reduce the gap by aligning top and bottom
  rel_heights = c(3, 3),
  ncol = 1, rel_widths = c(4, 4))
print(combined_histograms)
ggsave(filename = here("Figures", "combined_histograms.png"), plot = combined_histograms, width = 6, height = 6, dpi = 1400)

pH_violin_plot <- pH_violin_plot + 
  theme(plot.margin =margin(l = 0.2, r = 0.4, t = 1.15, b = 0.6, unit = "cm"),
        axis.title.y = element_text(size = 14),
        axis.text.y = element_text(size = 10.5),
        axis.title.x = element_text(size = 14),
        axis.text.x = element_text(size = 10.5))
temp_violin_plot <- temp_violin_plot +
  theme(plot.margin =margin(l = 0.25, r = 0.4, t = 1.15, b = 0.2, unit = "cm"),
        axis.title.y = element_text(size = 14, margin = margin(r = 10)),
        axis.text.y = element_text(size = 10.5),
        axis.title.x = element_text(size = 14, margin = margin(r = 5)),
        axis.text.x = element_text(size = 10.5))

combined_violin_plots <- cowplot::plot_grid(temp_violin_plot, pH_violin_plot,
                                           labels = c("(B)", "(C)"),
  label_size = 16,  # Adjust label size 
  hjust = -0.15,  # Adjust horizontal justification
  label_fontfamily = "serif",  # Set label font family to serif
  axis = 'tblr',  # Reduce the gap by aligning top and bottom
  rel_heights = c(2, 2.25),
  ncol = 1, rel_widths = c(4.5, 4.25))

study_site_map_adjusted <- study_site_map +
  theme(plot.margin = margin(l = 0.15, r = 0.65, t = 1, b = 0.575, unit = "cm"),
        axis.title.y = element_text(size = 14),
        axis.text.y = element_text(size = 11),
        axis.title.x = element_text(size = 14),
        axis.text.x = element_text(size = 11))
  
combined_plot <- cowplot::plot_grid(study_site_map_adjusted, combined_violin_plots, 
                                    labels = c("(A)", ""),
                                    label_size = 16, hjust = -0.15,
                                    label_fontfamily = "serif",
                                    rel_widths = c(4.2, 4.6),
                                    ncol = 2, axis = 'tblr') +
  theme(plot.background = element_rect(fill = "white", colour = "white"))

ggsave(filename = here("Figures", "combined_study_plot.png"), plot = combined_plot, width = 8.4, height = 6, dpi = 1400)


```

# Mesocosm Carbonate System Calculations & Temperature Data Analysis 

## Carbonate Chemistry Calculations 

Reading in pH calibration data and calculating pH and reading in total alkalinity data to calcualte carbinate system parameters within the mesocosm system. 

```{r Mesocosm pH, Total Alkalinity, and Carbonate System Calculations, warning=FALSE, message=FALSE}


# Read in daily mesocosm data 
mesocosm_daily_data <- read_csv(here("Data", "Mesocosm_Data", "Mesocosm_Daily_Datasheet.csv"),
  show_col_types = FALSE) %>%
  mutate(Date = mdy(Date), 
         Tank_ID = as.factor(Tank_ID), 
         pH_Treatment = as.factor(pH_Treatment), 
         Temp_Treatment = as.factor(Temp_Treatment))

# Read pH calibration data
pHcalib <- read_csv(here("Data", "Mesocosm_Data", "TrispHcalibration.csv"), show_col_types = FALSE) %>%
  mutate(TrisCalDate = ymd(TrisCalDate))

# Read mesocosm daily datasheet and select relevant columns
pHData <- read_csv(here("Data", "Mesocosm_Data", "Mesocosm_Daily_Datasheet.csv"),
  show_col_types = FALSE) %>%
  mutate(TrisCalDate = ymd(TrisCalDate),
         Date = mdy(Date), 
         Temp_Treatment = as.factor(Temp_Treatment), 
         pH_Treatment = as.factor(pH_Treatment),
         Tank_ID = as.factor(Tank_ID)) %>% 
  dplyr::select(Temp_C, mV, Salinity, TrisCalDate, Date, Tank_ID, pH_Treatment, Temp_Treatment)

# Calculate pH values using calibration data
pHSlope <- pHcalib %>%
  group_by(TrisCalDate) %>%
  summarise(fitpH = list(lm(mVTris ~ TTris, data = cur_data()))) %>%
  mutate(tidy_fit = purrr::map(fitpH, broom::tidy)) %>%
  unnest(tidy_fit) %>%
  dplyr::select(TrisCalDate, term, estimate) %>%
  pivot_wider(names_from = term, values_from = estimate)
# SOP 6a potentiometric pH calculation using calibration data slopes
# total hydrogen scale (pHT) using the equation \(\text{pH}(X) = \text{pH}(S) + \frac{E_S - E_X}{RT \ln 10 / F}\)


# Join pH calibration data with daily mesocosm data
pHjoined <- left_join(pHData, pHSlope, by = "TrisCalDate") %>%
  mutate(mVTris_calc = Temp_C * TTris + `(Intercept)`) %>%
  drop_na(Temp_C, mV)

# Calculate pH
pHcalc <- pHjoined %>%
  mutate(pH = seacarb::pH(Ex = mV, Etris = mVTris_calc, S = Salinity, T = Temp_C)) %>%
   dplyr::select(Date, Tank_ID, pH_Treatment, Temp_Treatment, pH)

# Read and process Total Alkalinity (TA) data
read_process_TA_data <- function(file_path, date, start_date, end_date) {
  read_csv(file_path, show_col_types = FALSE) %>% 
    mutate(Date = as.Date(date),
           Start_Date = as.Date(start_date),
           End_Date = as.Date(end_date)) %>%
    rename(Tank_ID = SampleID) %>%
    mutate(Tank_ID = gsub("-", "_", Tank_ID),
           Tank_ID = gsub("TNK_SUMP", "TNK_SMP", Tank_ID)) %>%
    group_by(Tank_ID) %>%
    do({
      df <- .
      dates <- seq(.$Start_Date, .$End_Date, by = "day")
      df[rep(seq_len(nrow(df)), each = length(dates)), ] %>%
        mutate(Date = dates)
    }) %>%
    ungroup() %>%
    dplyr::select(Date, Tank_ID, TA, TA_evap)
}

# Read and process TA data for different dates
TAdata_combined <- bind_rows(
  read_process_TA_data(here("Data", "Mesocosm_Data", "Total_Alkalinity", "TA2022-09-01.csv"), "2022-09-01", "2022-08-27", "2022-09-01"),
  read_process_TA_data(here("Data", "Mesocosm_Data", "Total_Alkalinity", "TA2022-08-22.csv"), "2022-08-22", "2022-08-22", "2022-08-26"),
  read_process_TA_data(here("Data", "Mesocosm_Data", "Total_Alkalinity", "TA2022-08-16.csv"), "2022-08-16", "2022-08-16", "2022-08-21")
) %>%
  filter(Tank_ID != "CRM",
         Tank_ID != "Junk") %>% 
   dplyr::select(Date, Tank_ID, TA) %>%
  arrange(Date, Tank_ID)
  # Dickson SOP 3b : Alkalinity<sub>(total) = [HCO<sub>3<sub>^-]+2[CO<sub>3<sub>^-2]+[B(OH)<sub>4<sub>^- ] + [OH^-] + [HPO<sub>4<sub>-2] + 2[PO<sub>4<sub>-3]+[SiO(OH)<sub>3<sub>^-] +[NH<sub>3<sub>]+ [HS-]+...-[H^+]<sub>F<sub>-[HSO<sub>4<sub>^-]-[HF]-[H<sub>3<sub?PO<sub>4<sub4<sub>-.... (1)

# Merge TA and pH data
completed_meso_data <- left_join(mesocosm_daily_data, pHcalc, by = c("Date", "Tank_ID", "pH_Treatment", "Temp_Treatment")) %>%
  left_join(TAdata_combined, by = c("Date", "Tank_ID"))

# Convert TA from ??mol/kg to mol/kg
completed_meso_data <- completed_meso_data %>% 
  mutate(TA = as.numeric(TA * 1e-6))

# Initialize columns for carbonate system parameters
completed_meso_data$K1 <- NA
completed_meso_data$K2 <- NA
completed_meso_data$CO2 <- NA
completed_meso_data$pCO2 <- NA
completed_meso_data$pCO2insitu <- NA
completed_meso_data$HCO3 <- NA
completed_meso_data$CO3 <- NA
completed_meso_data$DIC <- NA
completed_meso_data$OmegaAragonite <- NA
completed_meso_data$OmegaCalcite <- NA

# Calculate carbonate system parameters
for (i in 1:nrow(completed_meso_data)) {
  pH <- completed_meso_data$pH[i] 
  TA <- completed_meso_data$TA[i]  
  Salinity <- completed_meso_data$Salinity[i] 
  Temp_C <- completed_meso_data$Temp_C[i] 
  
  
  
  carbonate_params <- seacarb::carb(flag = 8, var1 = pH, var2 = TA, S = Salinity,
  T = Temp_C, Patm = 1, k1k2 = "l", kf = "pf", ks = "d", eos = "teos10",
                            pHscale = "T", b = "u74", warn = "y")
  
  K1 <- AK_CARB_1_LUEK00(t_k=(Temp_C+273.15),s=Salinity, p_bar=0)
  K2 <- AK_CARB_2_LUEK00(t_k=(Temp_C+273.15),s=Salinity, p_bar=0)
  # var1 = pH 
  # pHscale = "T" for total scale
  # var2 = TA  in mol/kg
  # S = Salinity in ppt
  # T = Temp_C Temperature in degrees Celsius
  # Patm = 1 Surface atmospheric pressure in atm
  # Sit = 0 Concentration of total silicate
  # Pt = 0 Concentration of total phosphate 
  # k1k2 = "l" using Lueker et al. (2000) for K1 and K2
  # kf= "pf" using Perez and Fraga (1987) recommended by Guide to Best Practices for Ocean CO2 Measurements
  # "d" using Ks from Dickson (1990) 
  # "u74" for the Uppstrom (1974) formulation,
  # "teos10" to specify T and S according to Thermodynamic Equation Of Seawater 

  
  completed_meso_data$pCO2[i] <- carbonate_params$pCO2
  completed_meso_data$pCO2insitu[i] <- carbonate_params$pCO2insitu
  completed_meso_data$HCO3[i] <- carbonate_params$HCO3
  completed_meso_data$CO3[i] <- carbonate_params$CO3
  completed_meso_data$DIC[i] <- carbonate_params$DIC
  completed_meso_data$OmegaAragonite[i] <- carbonate_params$OmegaAragonite
  completed_meso_data$OmegaCalcite[i] <- carbonate_params$OmegaCalcite
  completed_meso_data$OmegaCalcite[i] <- carbonate_params$OmegaCalcite
}

# Convert parameters to ??mol/kg
completed_meso_data <- completed_meso_data %>% 
  mutate(TA = TA * 1e6,
         HCO3 = HCO3 * 1e6,
         CO3 = CO3 * 1e6,
         DIC = DIC * 1e6)

# pCO2, CO2 partial pressure
# HCO3	 HCO3 concentration (mol/kg)
# CO3	 CO3 concentration (mol/kg) 
# DIC	 DIC concentration (mol/kg)  
# ALK	 ALK, total alkalinity (mol/kg)
# Omega aragonite, aragonite saturation state
# Omega calcite, calcite saturation state

completed_meso_data <- completed_meso_data %>% 
dplyr::select(Date, Tank_ID, pH_Treatment, Temp_Treatment, Temp_C, pH, mV, DO_mgL, DO_Percent, Conductivity, Salinity, TA, K1, K2, pCO2, pCO2insitu, HCO3, CO3, DIC, OmegaAragonite, OmegaCalcite)
write_csv(completed_meso_data, here("Data", "Mesocosm_Data", "Completed_Mesocosm_Dataset_Carbonate.csv"))





# Since pH is on a logarithmic scale, averaging pH values directly does not provide the true arithmetic mean of what is likely truly important to the organism, [H+]  concentration. Calculation of the mean pH by averaging the [H+] concentration. 

# Function to calculate mean pH
mean_pH <- function(pH, na.rm = TRUE) {
  # Convert pH values to [H+]
  H_plus <- 10^(-pH)
  # Calculate mean of [H+]
  mean_H_plus <- mean(H_plus, na.rm = na.rm)
  # Convert mean of [H+] back to mean pH
  mean_pH <- -log10(mean_H_plus)
  return(mean_pH)
}


low_mean_pH <- completed_meso_data %>% 
  filter(pH_Treatment == "Low") %>%
  dplyr::summarize(Mean_pH = mean(pH, na.rm = TRUE),
                   SD_pH = sd(pH, na.rm = TRUE),
                   N = n(),
                   SE = SD_pH/sqrt(N),
                   Upper_CI = Mean_pH + 1.96 * SE,
                   Lower_CI = Mean_pH - 1.96 * SE)

ambient_mean_pH <- completed_meso_data %>%
  filter(pH_Treatment == "Ambient") %>%
  dplyr::summarize(Mean_pH = mean(pH, na.rm = TRUE),
                   SD_pH = sd(pH, na.rm = TRUE),
                   N = n(),
                   SE = SD_pH/sqrt(N),
                   Upper_CI = Mean_pH + 1.96 * SE,
                   Lower_CI = Mean_pH - 1.96 * SE)

overall_low_pH <- paste("Overall low pH:",
    paste(round(low_mean_pH$Mean_pH, 2), "±", round(low_mean_pH$SE, 2)))

overall_ambient_pH <- paste("Overall ambient pH:",
    paste(round(ambient_mean_pH$Mean_pH, 2), "±", round(ambient_mean_pH$SE, 2)))


overall_low_pH
overall_ambient_pH  

#list of abbrev used in the script 

# pCO2, CO2 partial pressure
# HCO3	 HCO3 concentration (mol/kg)
# CO3	 CO3 concentration (mol/kg)
# DIC	 DIC concentration (mol/kg)
# ALK	 ALK, total alkalinity (mol/kg)
# Omega aragonite, aragonite saturation state
# Omega calcite, calcite saturation state
# TPC Thermal Performance Curve
# DO Dissolved Oxygen
# DO% Dissolved Oxygen Percentage
# Temp Temperature
# Salinity Salinity
# TA Total Alkalinity
# pH pH
# mV millivolts
# Conductivity Conductivity
# Temp_Treatment Temperature Treatment
# pH_Treatment pH Treatment
# Tank_ID Tank ID
# Date Date
# K1 K1 constant
# K2 K2 constant
# SE Standard Error
# N Number of Observations
# CI Confidence Interval
# SD Standard Deviation
# Low Low pH Treatment
# Ambient Ambient pH Treatment
# Mean_pH Mean pH
# SD_pH Standard Deviation of pH
# Upper_CI Upper Confidence Interval
# Lower_CI Lower Confidence Interval
# list of symbols used 
# 

```


```{r}
# Fit the linear model
mesocosm_temp_lm_model <- lm(Temp_C ~ Temp_Treatment * pH_Treatment, data = mesocosm.data)

# Perform ANOVA on the model
anova_results <- anova(mesocosm_temp_lm_model)
print(anova_results)

# Extract LaTeX equations using equatiomatic
equation_latex <- extract_eq(mesocosm_temp_lm_model, 
                             swap_var_names = c(
                               "Temp_C" = "Temperature (ºC)",
                               "Temp_Treatment" = "Temp Treatment",
                               "pH_Treatment" = "pH Treatment"
                             ),
                             wrap = TRUE,
                             terms_per_line = 3)

# Print the LaTeX equation
print(equation_latex)

# Define colors for pH treatments
pH_colors <- c("Low" = "cyan3", "Ambient" = "orange")

# Plot the data and fitted model
ggplot(mesocosm.data, aes(x = Temp_C, y = Temp_Treatment, color = pH_Treatment)) +
  geom_point(size = 3, alpha = 0.6) +
  geom_smooth(method = "lm", aes(fill = pH_Treatment), alpha = 0.2) +
  scale_color_manual(values = pH_colors) +
  scale_fill_manual(values = pH_colors) +
  labs(title = "Temperature vs Temp Treatment by pH Treatment",
       x = "Temperature (ºC)",
       y = "Temp Treatment") +
  theme_minimal(base_size = 15)
```

## Stats for Mesocosm Data

```{r Mesocosm Carbonate System and Temperature Data Analysis, warning=FALSE, message=FALSE}

mesocosm.data <- read_csv(here("Data", "Mesocosm_Data", "Completed_Mesocosm_Dataset_Carbonate.csv"), show_col_types = FALSE) %>% 
  filter(pH_Treatment != "Sump") %>%  # Filter out rows where pH_Treatment is "Sump"
  filter(!Date == "2022-08-16") %>%  # Filter out specific date
  mutate(Temp_Treatment = as.factor(Temp_Treatment),
         pH_Treatment = as.factor(pH_Treatment))

# Calculate mean and standard deviation of pH by treatment
ph.summary <- mesocosm.data %>%
  filter(pH_Treatment != "Sump") %>% 
  dplyr::select(pH, pH_Treatment) %>% 
  group_by(pH_Treatment) %>%
  summarize(Mean_pH = mean(pH, na.rm = TRUE),
            Sd_pH = sd(pH, na.rm = TRUE))

print(ph.summary)

#pH Anova Test
mesocosm_pH_lm_model <- lm(pH ~ Temp_Treatment * pH_Treatment, data = mesocosm.data)
anova_mescososm_pH_results <- anova(mesocosm_pH_lm_model)
print(anova_mescososm_pH_results)
check_model(mesocosm_pH_lm_model)

# Perform t-test to compare means
t_test_result <- t.test(pH ~ pH_Treatment, data = mesocosm.data)

# Print the t-test results
#print(t_test_result)

# Convert t-test result to a tidy tibble and apply conditional formatting
t_test_table <- tidy(t_test_result) %>%
  format_p_values() 

# Remove periods from the column names and clean up data frame
colnames(t_test_table) <- gsub("\\.", "", colnames(t_test_table))
colnames(t_test_table) <- tools::toTitleCase(colnames(t_test_table))

t_test_table <- t_test_table %>% 
  dplyr::mutate(
          Estimate1 = round(Estimate1, digits = 2),
          Estimate2 = round(Estimate2, digits = 2),
          Estimate = round(Estimate, digits = 2),
          Pvalue = Pvalue,
          Tvalue= round(Statistic, digits = 2),
          Confhigh = round(Confhigh, digits = 2), 
          Conflow = round(Conflow, digits = 2),
          CI = paste(Conflow,"-",Confhigh),
          df=round(Parameter, digits = 2)) %>% 
  dplyr::select(-Conflow, -Confhigh, -Statistic,
                -Parameter) %>% 
  dplyr::select(Method, Estimate1, Estimate2, Estimate,
                CI, Tvalue, Pvalue, df)
  
         
# Create the gt table and format
t_test_gt_table <- t_test_table %>% 
  gt() %>%
  tab_header(
    title = md("**Summary of Welch Two Sample T-Test for pH Treatments**"),
    subtitle = md("Comparison of pH Conditions: Ambient vs. Low")
  ) %>%
  cols_label(
    Method = "Method",
    Estimate1 = "Mean pH (Ambient)",
    Estimate2 = "Mean pH (Low)",
    Estimate = "Difference",
    CI = "95% CI",
    Tvalue = "t-Statistic",
    Pvalue = "p-Value",
    df = "Degrees of Freedom"
  ) %>% gt_theme


print(t_test_gt_table)
# save the gt tables 
gtsave(t_test_gt_table, here("Figures", "gt_table_ttest_pH.png"))


# Temperature Anova Test
mesocosm_temp_lm_model <- lm(Temp_C ~ Temp_Treatment * pH_Treatment, data = mesocosm.data)

extract_eq(mesocosm_temp_lm_model, wrap = TRUE)

extract_eq(
  model = mesocosm_temp_lm_model,
  swap_var_names = c(
    "Temp_C" = "Temperature ( C)",
    "Temp_Treatment" = "Temp Treatment",
    "flipper_length_mm" = "pH Treatment"
  ),
  var_colors = c(
    flipper_length_mm = "firebrick",
    body_mass_g = "cornflowerblue"
  ),
  wrap = TRUE,
  terms_per_line = 3
)

# Perform ANOVA on the model
anova_mescososm_temp_results <- anova(mesocosm_temp_lm_model)
print(anova_mescososm_temp_results)
check_model(mesocosm_temp_lm_model)

anova_df <- tidy(anova_mescososm_temp_results) %>% 
  format_p_values() 
  
anova_df <- anova_df %>%   
  mutate(
    statistic = round(statistic, digits = 2),
        term = gsub("_", " ", term),     term = gsub(":", " ?? ", term),# Replace underscores with spaces 
    term = tools::toTitleCase(term),  # Capitalize terms
    # Round statistical metrics to two significant figures
    sumsq = round(sumsq, digits = 2),
    meansq = round(meansq, digits = 2)
  ) 

anova_df <- anova_df %>%
  mutate(across(everything(), ~ ifelse(is.na(.), "", .))) %>%
  dplyr::select(term, p.value, statistic, sumsq, meansq, df) 

#print(anova_df)

# Perform a Tukey's HSD test
tukey_result <- TukeyHSD(aov(Temp_C ~ Temp_Treatment * pH_Treatment, data = mesocosm.data))

tukey_df <- broom::tidy(tukey_result) 

# Filter Tukey results for "Low" vs "Ambient"
tukey_filtered <- tukey_df %>%
    mutate(
        estimate = round(estimate, digits=2),
        conf.low = round(conf.low, digits=2),
        conf.high = round(conf.high, digits=2),
        p.value = as.numeric(adj.p.value)
    ) %>%
  #sepearting terms into filterable conditions toselect for only relevant comparisons 
  filter(term == "Temp_Treatment:pH_Treatment") %>% 
  format_p_values() %>%
    separate(contrast, into = c("pH_Treatment_Left", "pH_Treatment_Right"), sep = "-") %>%
    separate(pH_Treatment_Left, into = c("Temp_Left", "pH_Left"), sep = ":") %>%
    separate(pH_Treatment_Right, into = c("Temp_Right", "pH_Right"), sep = ":") %>% 
  # filtering for comparisons between temp treatments 
  filter(Temp_Left == Temp_Right) %>% 
  mutate(
    contrast = paste0(Temp_Left, "°C ", pH_Left, " × ",
                      Temp_Right, "°C ", pH_Right),
    term = gsub("_", " ", term),
    term = gsub(":", " × ", term),
    confidence.interval = paste0("(",conf.low, ", ", conf.high, ")")) %>%
  dplyr::select(contrast, estimate, confidence.interval, adj.p.value=p.value) 


#print(tukey_filtered)

# Create the GT table for ANOVA results
energy_anova_gt_table <- gt(anova_df) %>%
  tab_header(
    title = md("**Two-Way ANOVA Results for Temperature by Temperature & pH Treatment**"),
    subtitle = md("Interactions Between Temperature & pH Treatments")
  ) %>%
  cols_label(
    term = "Term",
    p.value = "P-Value",
    statistic = "F-Statistic",
    sumsq = "Sum of Squares",
    meansq = "Mean Squares",
    df = "Degrees of Freedom"
  ) %>%
  gt_theme
  gt_theme


# Create the GT table for Tukey results
energy_tukey_gt_table <- gt(tukey_filtered) %>%
  tab_header(
    title = md("**Tukey HSD Post Hoc Test Results for Temperature<br/> by Temperature & pH Treatment**"),
    subtitle = md("Specific Comparisons Between pH Treatments Across Temperature")
  ) %>%
  cols_label(
    contrast = "Comparison",
    estimate = "Difference",
    confidence.interval = "Confidence Interval",
    adj.p.value = "Adjusted P-Value"
  ) %>% gt_theme


invisible(print(energy_anova_gt_table))
invisible(print(energy_tukey_gt_table))


# Saving tables as PNG files
gtsave(energy_anova_gt_table, filename = here::here("Figures", "anova_temperature_results.png"))
gtsave(energy_tukey_gt_table, filename = here::here("Figures", "tukey_hsd_temperature_results_simplified.png"))

```

##  Summary Statistics for Mesocosm Data

gtsummary package and CSS embedded edits & labels 


```{r}
# Load necessary libraries
library(tidyverse)
library(gtsummary)
library(gt)
library(here)
library(bstfun)

# Load the mesocosm dataset
mesocosm.data <- read_csv(here("Data", "Mesocosm_Data", "Completed_Mesocosm_Dataset_Carbonate.csv"),
                          show_col_types = FALSE) %>%
  filter(pH_Treatment != "Sump", Date != "2022-08-16") %>%
  mutate(Temp_Treatment = factor(Temp_Treatment),
         pH_Treatment = factor(pH_Treatment))
```

```{r Mesocosm Data Summary Statistics, warning=FALSE, message=FALSE}
style_tbl_compact()

# Load the data
mesocosm.data <- read_csv(here("Data", "Mesocosm_Data", "Completed_Mesocosm_Dataset_Carbonate.csv"),
                          show_col_types = FALSE)


# Prepare the data by filtering and selecting relevant columns
gt_meso_summary <- mesocosm.data %>%
  filter(pH_Treatment != "Sump", Date != "2022-08-16") %>%
  select(Temp_Treatment, pH_Treatment, Temp_C, pH, DO_mgL,
  Salinity, TA, pCO2insitu, HCO3, CO3, DIC) %>%
  droplevels() %>%
  mutate(
    TA = as.numeric(as.character(TA)),
    Temp_Treatment = factor(paste0(Temp_Treatment, " °C"),
    levels = c("12 °C", "14 °C", "16 °C", "18 °C", "20 °C", "22 °C", "24 °C", "26 °C")),
    pH_Treatment = factor(paste0(pH_Treatment, " pH"),
    levels = c("Ambient pH", "Low pH"))
  ) %>%
  arrange(Temp_Treatment, pH_Treatment) %>%
  ungroup()
# % DO 

# Function to generate and process HTML in summary tables
generate_summary_tables <- function(gt_meso_summary, variable_labels) {
  all_tables <- list()
  
  temp_treatments <- c("12", "14", "16", "18", "20", "22", "24", "26")
  
  for (variable in names(variable_labels)) {
    temp_tables <- list()
    
    for (temp in temp_treatments) {
      filtered_data <- gt_meso_summary %>%
        filter(Temp_Treatment == paste0(temp, " °C")) %>%
        dplyr::select(all_of(variable), pH_Treatment)
      
# Create a table for each temperature
      temp_table <- filtered_data %>%
        tbl_continuous(
          variable = all_of(variable),
          statistic = pH_Treatment ~ "{mean}&nbsp;±&nbsp;{SE}",
          digits = ~2
        ) %>%
        modify_header(
          label ~ html(
"<div style='font-size: 120%; line-height: 1.25; font-family: serif;'><b>Treatment</b></div>"),
          all_stat_cols() ~ html(sprintf(
"<div style='font-size: 120%%; white-space: nowrap; vertical-align: middle; text-align: center; font-family: serif;'>%s</div>",
          variable_labels[[variable]])), text_interpret = "html"
        ) %>%
        modify_table_body(
          ~ .x %>%
            mutate(
              label = ifelse(
                row_type == "label",
                gt::html(sprintf("<b>Temperature&nbsp;%s°C<b/>", temp)),
                label
              )
            )
        ) %>%
        modify_footnote(all_stat_cols() ~ NA) 
        
      temp_tables[[paste0("tbl_continuous_variable_", temp)]] <- temp_table

    }
    
    all_tables[[variable]] <- tbl_stack(temp_tables, quiet = TRUE)
  }
  
merged_tables <- tbl_merge(all_tables) %>%
  modify_table_styling(
    align = "center",
      columns = all_stat_cols(),
      rows = str_detect(row_type, "level"),
    fmt_fun = function(x) {
      # Apply HTML formatting to each cell individually
      sapply(x, function(cell) {
        gt::html(sprintf(
"<span style='white-space: nowrap; vertical-align: middle; padding: 1px 1px; font-size: 100%%; font-family: serif;'>%s</span>", cell))
      })
    },
    text_interpret = "html"
  ) 

merged_tables <- merged_tables %>%
  modify_table_styling(
    columns = label,
    rows = str_detect(label, "Temperature"), # This ensures we only target temperature labels
    fmt_fun = function(x) {
      # Apply HTML formatting to each cell individually
      sapply(x, function(cell) {
        gt::html(sprintf(
"<span style='white-space: nowrap; vertical-align: bottom; text-align: left; padding: 2px 1px; font-size: 115%%; font-family: serif;'>%s</span>", cell))
      })
    },
    text_interpret = "html"
  ) %>%
  modify_table_styling(
    columns = label,
    rows = str_detect(label, "pH"), # This ensures we only target temperature labels
    fmt_fun = function(x) {
      # Apply HTML formatting to each cell individually
      sapply(x, function(cell) {
        gt::html(sprintf(
"<span style='white-space: nowrap; vertical-align: middle; text-align: left; padding: 2px 1px; font-size: 115%%; font-family: serif;'>%s</span>", cell))
      })
    },
    text_interpret = "html"
  )

merged_final_tables <- merged_tables %>%
  modify_spanning_header(list(all_stat_cols() ~
    gt::html(
"<span style='font-family: serif;font-size: 175%; line-height: 1.85;'><b>Variable (<span>&#x03BC;</span><span style='margin-right: -0.5em;'></span>&#x0302;&nbsp;&nbsp;&nbsp;±&nbsp;&sigma;<sub>M</sub>)</b></span>"
    )),
    text_interpret = "html") 
 
#extract col and row names if necessary 
#column_names_vector <- show_header_names(mesocosm_summary_table, quiet=TRUE)[2]$column
#rownames <- mesocosm_summary_table$table_body$label

 trimmed_tbl <- tbl_butcher(merged_final_tables)
 
 trimmed_tbl$table_body <- trimmed_tbl$table_body %>% 
  mutate(
    variable = case_when(
    str_detect(label, "Temperature") ~ "Temp_Treatment", 
    str_detect(label, "Ambient") ~ "Ambient_pH_Treatment",
    str_detect(label, "Low") ~ "Low_pH_Treatment"
    )) 

gt_summary_tbl <- trimmed_tbl %>% 
  as_gt(rowname_col = vars("label")) %>% 
tab_options(
    table.font.color = "black", # Setting the font color to black
    table.background.color = "white",
    table.border.bottom.color = "grey30",
    table.border.top.color = "grey30",
    heading.border.bottom.color = "grey30",
    column_labels.border.top.color = "grey30",
    column_labels.border.bottom.color = "grey30",
    table_body.border.top.color = "grey30",
    table_body.border.bottom.color = "grey30",
    footnotes.border.bottom.color = "grey30",
    table_body.hlines.width = px(0),
    table_body.vlines.width = px(0)
  ) %>% 
  gt::tab_footnote(
    footnote = overall_ambient_pH,
    locations = cells_body(column = label, row = variable == "Ambient_pH_Treatment")) %>%
  gt::tab_footnote(
    footnote = overall_low_pH,
    locations = cells_body(column = label, row = variable == "Low_pH_Treatment")) %>%
  tab_style(
    style = list(css = "font-family: serif; vertical-align: top; text-align: left;
    white-space: none; font-size: 120%; line-height: 1"),
    locations = cells_footnotes())  %>%
  opt_table_font(font = "serif") %>% 
tab_style(style = list(
    cell_borders(sides = "bottom", color = "grey30", weight=px(1.75))),
    locations = cells_body(
     rows = str_detect(label, "Temperature") 
    )) %>% 
tab_style(style = list(
    cell_borders(sides = "bottom", color = "grey30", weight=px(1.75)),
    cell_fill(color = "cyan3", alpha = 0.15)),
    locations = cells_body(
     rows = str_detect(label, "Low pH"))) %>% 
tab_style(style = list(
    cell_borders(sides = "top", color = "grey30", weight=px(1.75)),
    cell_fill(color = "orange", alpha = 0.15)),
    locations = cells_body(
     rows = str_detect(label, "Ambient pH") 
    )) %>% 
tab_header(
    title = md("**Appendix 3. Mesocosm Biogeochemistry Summary Statistics Stratified by Temperature & pH Treatment**"),
    subtitle = md("Summary Statistics Presented as Mean ?? Standard Error (SE)")) %>%
  tab_style(
    style = list(css = "white-space: nowrap; vertical-align: middle; text-align: center; line-height: 1.25; padding: 10px 10px; font-size: 165%; font-family: serif"),
    locations = cells_title()
  ) 

  

  return(gt_summary_tbl)
}


variable_labels <- list(
  "Temp_C" = 
    "<span style='font-weight: bold;'>Temperature</span> <br/>
     <span class='unit' style='font-size: 90%;font-weight: bold;'>(°C)</span>",
  "pH" = 
    "<span style='font-weight: bold;'>pH<sub>T</sub></span>",
  "DO_mgL" = 
    "<span style='font-weight: bold;'>DO</span> <br/>
    <span class='unit' style='font-size: 90%;font-weight: bold;'>(mg&nbsp;L<sup>-1</sup>)</span>",
  "Salinity" = 
    "<span style='font-weight: bold;'>Salinity</span> <br/>
     <span class='unit' style='font-size: 90%;font-weight: bold;'>(ppt)</span>",
  "TA" = 
    "<span style='font-weight: bold;'>TA</span> <br/>
     <span class='unit' style='font-size: 90%;font-weight: bold;'>(??mol&nbsp;kg<sup>???1</sup>)</span>",
  "pCO2insitu" = 
    "<span style='font-weight: bold;'>pCO<sub>2</sub></span> <br/>
     <span class='unit' style='font-size: 90%;font-weight: bold;'>(??atm)</span>",
  "HCO3" =
    "<span style='font-weight: bold;'>HCO<sub>3</sub><sup>-</sup></span> <br/>
     <span class='unit' style='font-size: 90%;font-weight: bold;'>(??mol&nbsp;kg<sup>???1</sup>)</span>",
  "CO3" = 
    "<span style='font-weight: bold;'>CO<sub>3</sub><sup>2-</sup></span> <br/>
     <span class='unit' style='font-size: 90%;font-weight: bold;'>(??mol&nbsp;kg<sup>???1</sup>)</span>",
  "DIC" = 
    "<span style='font-weight: bold;'>DIC</span> <br/>
     <span class='unit' style='font-size: 90%;font-weight: bold;'>(??mol&nbsp;kg<sup>???1</sup>)</span>"
)


mesocosm_summary_table <- generate_summary_tables(gt_meso_summary, variable_labels) 

invisible(print(mesocosm_summary_table)) 

gtsave(mesocosm_summary_table.final , file = here("Figures", "gt_summary_table.png"),
       expand=20) 
#gt_summary_table$table_body
#zoom
#zoom
```

## Plots for Mesocosm Data 

```{r Mesocosm Data Summary Statistics, warning=FALSE, message=FALSE}


library(ggpubr)
ggpubr::ggboxplot(temp.data, x = "Temp_Treatment", y = "Temp_C", color = "pH_Treatment",
                  fill = "pH_Treatment", bxp.errorbar=TRUE,
                  outlier.shape = NA, error.plot = "errorbar",
          palette = c("cyan3", "orange")) +
  scale_y_continuous(breaks = seq(10, 34, by = 2)) +
  labs( x = "Temperature Treatment",
       y = "Temperature (°C)") + 
  scale_color_manual(values = c("cyan4", "darkorange2")) +
  common_theme +
  theme(panel.grid.major.x = element_blank(),
       panel.grid.major.y = element_line(colour = "grey75", linetype = "longdash")) 


# Remove rows with missing values
pH.data <- mesocosm.data %>%
  dplyr::select(Temp_Treatment, pH_Treatment, pH) %>%
  na.omit()

# Create the plot with adjusted centrality label arguments
pH_treatment_plot <- pH.data %>% 
  ggbetweenstats(
    data = pH.data,
    x = pH_Treatment,
    y = pH,
    type = "parametric",
    pairwise.display = "significant",
    var.equal = FALSE,  # Welch's t-test
    title = NULL,
    subtitle = NULL,
    xlab = "pH Treatment",
    ylab = TeX("$pH_{T}$"),
    results.subtitle = FALSE,  # Don't display statistical results as subtitle
    centrality.point.args = list(
      size = 4,
      stroke = 0,
      color = c("orange", "cyan3")  # Set colors for both points and centrality points
    ),
    centrality.label.args = list(
      size = 2,  # Smaller font size for centrality labels
      nudge_x = -0.5, segment.linetype = 4,
      color = "grey25"  # Example color setting
    )
  ) +
  scale_y_continuous(limits = c(7.3, 8.3), breaks = seq(7.3, 8.3, by = 0.2), expand = c(0, 0)) +
  geom_round(comparisons = list(c("Ambient", "Low")), test.args = list(exact = TRUE),
              map_signif_level = TRUE, test = "t.test") + # Add significance annotations
  scale_color_manual(values=c("orange", "cyan3")) + # Set colors for the significance annotations
  common_theme 

## remove layer corresponding to sample size
pH_treatment_plot <- delete_layers(pH_treatment_plot, "GeomText")

print(pH_treatment_plot)
ggsave(here("Figures", "pH_Treatment_Plot.png"), plot = pH_treatment_plot, width = 4.5, height = 4, dpi = 1300)

# Ensure Temp_Treatment is a factor right from the data preparation
mesocosm.data$Temp_Treatment <- factor(mesocosm.data$Temp_Treatment)

# Prepare the data
temp.data <- mesocosm.data %>%
  select(Temp_Treatment, pH_Treatment, Temp_C) %>%
  mutate(Interaction = interaction(Temp_Treatment, pH_Treatment, sep = "°C \n")) %>%
  na.omit()

# Create a data frame for horizontal lines
hline_data <- data.frame(Temp_Treatment = unique(temp.data$Temp_Treatment))

# Define the start and end colors for the gradient
start_color <- "#ffeda0"  # Light color (low)
end_color <- "red3"       # Dark color (high)

# Create a color function with the desired gradient
color_pal <- colorRampPalette(c(start_color, end_color))

# Generate the colors for the range 12 to 26, incrementing by 2
temp_treatments <- seq(12, 26, by = 2)
colors <- color_pal(length(temp_treatments))

# Create a named vector of colors corresponding to each Temp_Treatment value
color_mapping <- setNames(colors, as.character(temp_treatments))

mesocosm.data <- mesocosm.data %>%
  mutate(Temp_Treatment = as.character(Temp_Treatment),  # Ensure Temp_Treatment is character for matching
         Color = color_mapping[Temp_Treatment])  

temp_treatment_plot <- ggplot(temp.data, aes(x = as.factor(Temp_Treatment), y = Temp_C, fill = pH_Treatment)) +
 #geom_vline(data = hline_data, aes(xintercept = y_intercept), color = "gray", linetype = "dashed") +
  stat_boxplot(aes(color = pH_Treatment, fill= pH_Treatment),
               geom = "errorbar", linewidth = 0.4, width = 0.85) +
  geom_boxplot(aes(color = pH_Treatment, fill= pH_Treatment), outlier.shape = NA, linewidth = 0.25, width = 0.85) +# Maintain semi-transparency to visualize colors effectively
  labs(title = NULL,
       x = "Temperature Treatment",
       y = "Seawater Temperature (°C)") +
scale_y_continuous(limits = c(10, 30), breaks = seq(10, 30, by = 2)) +
  scale_fill_manual(values = c("Ambient" = "orange", "Low" = "cyan3")) +
  scale_color_manual(values = c("Ambient" = "darkorange3", "Low" = "cyan4")) +  # Ensure these are your Temp_Treatment categories
  theme(legend.position = "right") +#for horizontal lines
 # scale_color_gradient(low = "#ffeda0", high = "red3", name = "Temperature Treatment") +  # Apply gradient color for temperature treatment lines
  common_theme +
  theme(legend.position = "none")  # Adjust legend position for better visualization

print(temp_treatment_plot)
ggsave(here("Figures", "Temperature_Treatment_Plot.png"), plot = temp_treatment_plot, width = 4.5, height = 3, dpi = 1300)

temp_treatment_plot <- temp_treatment_plot 
pH_treatment_plot <- pH_treatment_plot 


combined_temp_pH <- cowplot::plot_grid(temp_treatment_plot, pH_treatment_plot, 
                                       labels = c("(A)", "(B)"),
                                       label_size = 14, label_fontfamily = "serif",
        

                                       nrow = 1, 
                                       rel_widths = c(2.3, 2)) +
  theme(plot.background = element_rect(fill = "white", colour = "white"))


ggsave(here("Figures", "Combined_Temperature_pH_Plot.png"), combined_temp_pH,
       width =6, height = 3.5, dpi = 1300) 


```




## Summary Statistics for Mesocosm Data

```{r}
library(downloadthis)

mesocosm.data <- read_csv(here("Data", "Mesocosm_Data", "Completed_Mesocosm_Dataset_Carbonate.csv"), show_col_types = FALSE)

# Filter and reshape the data
mesocosm.data.filtered <- mesocosm.data %>%
  filter(pH_Treatment != "Sump") %>%
  filter(Date > "2022-08-21", Date < "2022-09-01") %>%
  select(Temp_Treatment, pH_Treatment, Temp_C, pH, DO_mgL,
         Salinity, TA, pCO2insitu, HCO3, CO3, DIC) %>%
  mutate(
    Temp_Treatment = factor(paste0(Temp_Treatment, " °C"),
                            levels = c("12 °C", "14 °C", "16 °C", "18 °C",
                                       "20 °C", "22 °C", "24 °C", "26 °C")),
    pH_Treatment = factor(pH_Treatment, levels = c("Ambient", "Low"))
  ) %>%
  arrange(Temp_Treatment, pH_Treatment) %>%
  pivot_longer(
    cols = c(Temp_C, pH, DO_mgL, Salinity, TA, pCO2insitu, HCO3, CO3, DIC),  # Select only numeric value columns
    names_to = "Variable",
    values_to = "Variable_Value"
  ) %>% 
  na.omit() 

# Define the variable labels
variable_labels <- list(
  Temp_C = "Temperature (°C)",
  pH = "pH<sub>T</sub>",
  DO_mgL = "DO (mg L<sup>-1</sup>)",
  Salinity = "Salinity (ppt)",
  TA = "TA (??mol kg<sup>???1</sup>)",
  pCO2insitu = "pCO<sub>2</sub> (??atm)",
  HCO3 = "HCO<sub>3</sub><sup>-</sup> (??mol kg<sup>???1</sup>)",
  CO3 = "CO<sub>3</sub><sup>2-</sup> (??mol kg<sup>???1</sup>)",
  DIC = "DIC (??mol kg<sup>???1</sup>)"
)

mesocosm.data.filtered <- mesocosm.data.filtered %>%
  mutate(Variable_Label = variable_labels[Variable]) %>%
  mutate(Variable_Label = unlist(Variable_Label))

combined.mesocosm.data.filtered <- mesocosm.data.filtered %>%
  mutate(Temp_pH_Treatment = 
  paste(Temp_Treatment, pH_Treatment, sep = " ?? "), .keep="unused") 

# Summarise the data
meso_summary_data <- combined.mesocosm.data.filtered %>%
  select(Temp_pH_Treatment, Variable_Label, Variable, Variable_Value) %>%
  group_by(Temp_pH_Treatment, Variable, Variable_Label) %>%
  summarise(
    Mean = mean(Variable_Value, na.rm = TRUE),
    Std_Deviation = sd(Variable_Value, na.rm = TRUE),
    Std_Error = sd(Variable_Value, na.rm = TRUE) / sqrt(sum(!is.na(Variable_Value))),
    CI_Lower = mean(Variable_Value, na.rm = TRUE) - 
      qt(0.975, df = sum(!is.na(Variable_Value)) - 1) * 
      (sd(Variable_Value, na.rm = TRUE) / sqrt(sum(!is.na(Variable_Value)))),
    CI_Upper = mean(Variable_Value, na.rm = TRUE) + 
      qt(0.975, df = sum(!is.na(Variable_Value)) - 1) * 
      (sd(Variable_Value, na.rm = TRUE) / sqrt(sum(!is.na(Variable_Value)))),
    N = sum(!is.na(Variable_Value)), .groups = "drop"
  ) %>%
  mutate(
    Mean = round(Mean, 2),
    Std_Deviation = round(Std_Deviation, 2),
    Std_Error = round(Std_Error, 2),
    CI_Lower = round(CI_Lower, 2),
    CI_Upper = round(CI_Upper, 2)
  )  %>%
  arrange(Temp_pH_Treatment, Variable, Variable_Label)

# Create a gt table
gt_meso_summary <- meso_summary_data %>%
  select(-Variable) %>%
  mutate(`Temp x pH Treatment` = Temp_pH_Treatment, .keep="unused") %>% 
  rename_with(~ toTitleCase(gsub("_", " ", .)), -last_col()) %>% 
  gt(rowname_col="Variable Label", groupname_col = "Temp x pH Treatment") %>%
  opt_table_font(font = "serif") %>% 
  tab_header(
    title = "Mesocosm Summary Statistics Table",
  ) %>%
  cols_align(
    align = "center"
  ) %>%
  cols_align(
    align = "left",
    columns = "Variable Label"
  )   %>%
 fmt_markdown(columns = c("Variable Label")) %>% 
  tab_source_note(cleaned_meso_summary_data_csv %>% 
      download_this(
        output_name = "mesocosm_summary_statistics",
        output_extension = ".csv", # CSV output
        button_label = "Download CSV",
        button_type = "default"
      )
  )
  

# Print the gt table
invisible(print(gt_meso_summary))

```



```{r}
continuous_summary_mesocosm <- mesocosm.data.filtered %>% 
tbl_custom_summary(
  include = "Temp_Treatment",
  by = "pH_Treatment",
  label = ~"Variable_Label",
  stat_fns = ~ continuous_summary("Variable"),
  statistic = ~list(~"{mean} ?? {se}", ~"{median} [{p25}-{p75}]", ~"{min} - {max}"),
  digits = ~2,
  overall_row = TRUE,
  overall_row_label = "All stages & grades"
  ) 

continuous_summary_mesocosm
```
```{r}

mesocosm.data <- read_csv(here("Data", "Mesocosm_Data", "Completed_Mesocosm_Dataset_Carbonate.csv"), show_col_types = FALSE)

# Filter and reshape the data
mesocosm.data.filtered <- mesocosm.data %>%
  filter(pH_Treatment != "Sump") %>%
  filter(Date > "2022-08-21", Date < "2022-09-01") %>%
  select(Temp_Treatment, pH_Treatment, Temp_C, pH, DO_mgL,
         Salinity, TA, pCO2insitu, HCO3, CO3, DIC) %>%
  mutate(
    Temp_Treatment = factor(paste0(Temp_Treatment, " °C"),
                            levels = c("12 °C", "14 °C", "16 °C", "18 °C",
                                       "20 °C", "22 °C", "24 °C", "26 °C")),
    pH_Treatment = factor(pH_Treatment, levels = c("Ambient", "Low"))
  ) %>%
  arrange(Temp_Treatment, pH_Treatment)

## No test: 
# Example 1 ----------------------------------
continuous_summary_ex1 <-
  trial %>%
  tbl_custom_summary(
    type = "marker"~"continuous",
    include = c("stage", "grade"),
    by = "trt",
    stat_fns = ~ continuous_summary("age"),
    statistic = ~"{mean} ?? {se}",
    overall_row = TRUE,
    overall_row_last = TRUE,
    overall_row_label = "marker"~"Overall"
  ) %>%
  modify_footnote(
    update = all_stat_cols() ~ 
    "Statistics are presented as mean ?? standard error (SE)."
  )
## End(No test
continuous_summary_ex1
```



```{r}
# \donttest{
# Example 1 ----------------------------------
my_stats <- function(data, ...) {
  mean_ambient <- mean(data$marker, na.rm = TRUE)
  mean_low <- mean(data$age, na.rm = TRUE)
  dplyr::tibble(
    mean_ambient = mean_ambient,
    mean_low = mean_low
  )
}

my_stats(trial)

tbl_custom_summary_ex1 <-
  trial %>%
  filter(grade %in% c("I", "II")) %>%
  mutate(grade = factor(grade, levels = c("I", "II"))) %>% 
  tbl_custom_summary(
    include = c("stage", "trt"),
    by = "grade",
    stat_fns = everything() ~ my_stats,
    statistic = everything() ~ "Ambient: {mean_ambient} - Low: {mean_low}",
    digits = everything() ~ 2,
    overall_row = TRUE,
    overall_row_last = TRUE,
    overall_row_label = "grade"~ list("**Overall**, N = {N}")
  ) %>%
  bold_labels() %>% 
  inline_text(age, level = NULL, column = NULL, pattern = NULL, ...) %>% 
  inline_text(tbl_cross, row_level = "Drug A", col_level = "1")

tbl_custom_summary_ex1 
```

```{r}

#FOR JOULES 
tbl_cross_ex2 <-
  trial %>%
  tbl_cross(row = stage, col = trt, percent = "cell") %>%
  add_p() %>%
  bold_labels()

tbl_cross_ex2
test	
List of formulas specifying statistical tests to perform for each variable. Default is two-way ANOVA when ???by=??? is not NULL, and has the same defaults as add_p.tbl_continuous() when by = NULL. See tests for details, more tests, and instruction for implementing a custom test.

```

```{r}
# Example 2 ----------------------------------
# Use `data[[variable]]` to access the current variable
mean_ci <- function(data, variable, ...) {
  test <- t.test(data[[variable]])
  dplyr::tibble(
    mean = test$estimate,
    conf.low = test$conf.int[1],
    conf.high = test$conf.int[2]
  )
}

tbl_custom_summary_ex2 <-
  trial %>%
  tbl_custom_summary(
    include = c("marker", "ttdeath"),
    by = "trt",
    stat_fns = ~mean_ci,
    statistic = ~"{mean} [{conf.low}; {conf.high}]"
  ) %>%
  add_overall(last = TRUE) %>%
  modify_footnote(
    update = all_stat_cols() ~ "mean [95% CI]"
  )

# Example 3 ----------------------------------
# Use `full_data` to access the full datasets
# Returned statistic can also be a character
diff_to_great_mean <- function(data, full_data, ...) {
  mean <- mean(data$marker, na.rm = TRUE)
  great_mean <- mean(full_data$marker, na.rm = TRUE)
  diff <- mean - great_mean
  dplyr::tibble(
    mean = mean,
    great_mean = great_mean,
    diff = diff,
    level = ifelse(diff > 0, "high", "low")
  )
}


### JOUKLESS 
tbl_custom_summary_ex3 <-
  trial %>%
  tbl_custom_summary(
    include = c("grade", "stage"),
    by = "trt",
    stat_fns = ~diff_to_great_mean,
    statistic = ~"{mean} ({level}, diff: {diff})",
    overall_row = TRUE
  ) %>%
  bold_labels()
# }
```

```{r}

gt.summary.mesocosm.data <- combined.mesocosm.data.filtered %>%
  mutate(Variable = forcats::fct_relevel(
  Variable, "Temp_C", "pH", "DO_mgL", "Salinity", 
  "TA", "pCO2insitu", "HCO3", "CO3", "DIC")) %>% 
  mutate(Temp_pH_Treatment = forcats::fct_relevel(Temp_pH_Treatment, sort)) %>% 
  select(Temp_pH_Treatment, Variable, Variable_Label, Variable_Value)


  
  
  
```


```{r}
# Create summary table
tbl_continuous_summary <- gt.summary.mesocosm.data %>%
  tbl_continuous(
    variable = Variable_Value,
    label = Variable_Label,
    statistic =  ~ "{mean} ?? {se}",
    by = Variable,
    include=  Combined_Treatment,
    digits = ~ 2
  ) 
  

alias	description	pseudo-code
'anova_2way'	Two-way ANOVA	lm(continuous_variable ~ by + variable)
't.test'	t-test	t.test(continuous_variable ~ as.factor(variable), data = data, conf.level = 0.95, ...)

# Print the summary table
tbl_continuous_summary

```


```{r Mesocosm Data Summary Statistics, warning=FALSE, message=FALSE}



```


```{r Mesocosm Data Summary Statistics, warning=FALSE, message=FALSE}
# Define the variable labels
variable_labels <- list(
  Temp_C = "Temperature (°C)",
  pH = "pH<sub>T</sub>",
  DO_mgL = "DO (mg L<sup>-1</sup>)",
  Salinity = "Salinity (ppt)",
  TA = "TA (??mol kg<sup>???1</sup>)",
  pCO2insitu = "pCO<sub>2</sub> (??atm)",
  HCO3 = "HCO<sub>3</sub><sup>-</sup> (??mol kg<sup>???1</sup>)",
  CO3 = "CO<sub>3</sub><sup>2-</sup> (??mol kg<sup>???1</sup>)",
  DIC = "DIC (??mol kg<sup>???1</sup>)"
)


# Function to generate and process HTML in summary tables
generate_summary_tables <- function(gt_meso_summary, variable_labels) {
  all_tables <- list()
  
  temp_treatments <- c("12", "14", "16", "18", "20", "22", "24", "26")
  
  
  for (temp in temp_treatments) {
    temp_table <- gt_meso_summary %>%
      filter(Temp_Treatment == paste0(temp, " °C")) %>%
      select(-Temp_Treatment) %>%
      tbl_summary(
        by = pH_Treatment,
        type = everything() ~ "continuous",
        statistic = all_continuous() ~ "{mean} ?? {se}",
        digits = all_continuous() ~ 2,
        missing = "no",
        label = variable_labels
      ) %>%
      modify_header(
        label ~ "<b>Variable (x?? ?? ??<sub>M</sub>)</b>",
        stat_1 ~ "<b>Ambient<br/>pH</b>",
        stat_2 ~ "<b>Low<br/>pH</b>"
      ) %>%
      modify_table_styling(
        align = "center",
        columns = all_stat_cols(),
        text_interpret = "html"
      ) %>%
      modify_table_styling(
        align = "left",
        columns = vars(label),
        text_interpret = "html",
        fmt_fun = function(x) {
          sapply(x, function(cell) {
            sprintf(
              "<span style='white-space: nowrap;'>%s</span>", cell)
          })
        }
      ) %>%
      modify_footnote(
        all_stat_cols() ~ NA
      ) %>%
  modify_spanning_header(all_stat_cols() ~ sprintf("Temperature %s", temp))
    
      table_name <- sprintf("tbl_summary_variable_%s", temp)
      all_tables[[table_name]] <- temp_table
      
  }
  
  # Create a list of tab spanners matching the number of tables
  tab_spanners <- sprintf("<b style='white-space: nowrap;'>Temperature %s °C</b>",
                          temp_treatments)
  
  # Merging tables and applying spanning headers
  merged_tables <- tbl_merge(
    tbls = all_tables,
    tab_spanner = tab_spanners
  ) 
  
  return(merged_tables)
}

#1. make it so that the each variable is summarized by the level of pH across temperature treatments instead of temp and ph with each variable (this will help reduce the width of the table as well which is an issue)
#2. add an overall calcualtion for the ambient and low pH treatments for just pH 
#3. add a footnote to the table that explains the overall calculation for the ambient and low pH treatments pHand that it was caluclated using the seacarb() package in R
# 5. add a spanning header for the variable name across all_stat_cols
# 4. add a spanning header for temperature treatmetn accross all_Stat_cols 

mesocosm_summary_table <- generate_summary_tables(gt_meso_summary, variable_labels)

invisible(print(mesocosm_summary_table))
```

```{r}
# Set table width to 100% of the container, apply serif font, and prevent line breaks
mesocosm_summary_table_final <- mesocosm_summary_table %>%
  as_gt() %>%
  tab_options(
    table.width = pct(100),
    data_row.padding = px(10)
  ) %>%
  opt_table_font(font = "serif") %>%
      opt_css(
        css = "
          .gt_table {
          background-color: white;
          font-size: 12px;
          }
          .gt_col_heading {
            font-size: 12px;
          }
          .gt_column {
          }
          .gt_label {
            font-size: 12px;
          }
          .gt_row {
            font-size: 10px;
          }

    }


        "
      )

mesocosm_summary_table_final 
```


# Thermal Performance Curve Analysis 

## Respiration Analysis 


```{r Clearing Environment, message=FALSE, include=FALSE} 

The Sharpe-Schoolfield model (Eq. (1) in the main text) has an exponential (Arrhenius) term in its numerator and, as a result, the model cannot predict non-positive rate values.
# Clear the environment
rm(list = ls())
```



```{r For Loop for Reading and Writing Respiration Data, eval=FALSE, include=FALSE}

# Reading in respiration metadata
respo.metadata <- read_csv(here::here("Data", "Experiment_Metadata", "Respirometry_Metadata.csv"))

# Function to read a CSV file, skip the first row, skip the last few rows, and check for issues
read_and_process_csv <- function(filename, path) {
  
  # Determine the number of lines in the file and subtract 4 to skip the last few
  total_lines <- length(readLines(file.path(path, filename)))
  data_length <- total_lines - 4  # Assuming the last 4 lines are not needed
  
  # Read the CSV file, skipping the first row and the last few rows
  data <- read_csv(file.path(path, filename), skip = 1, n_max = data_length, col_names=TRUE, 
                   show_col_types = FALSE, name_repair = "unique_quiet")
  
  # Return the data without the last rows
  return(data)
}

#Set the path to the location of the raw oxygen data files
path.p <- here::here("Data","Respirometry_Data")

#renaming the file names as file.names.full
file.names <- list.files(path = path.p, pattern = "csv$", recursive = TRUE)

#creating data frame to import respiration data into 
respiration.rates <- data.frame(matrix(NA, nrow=length(file.names), ncol=6)) #setting column names
colnames(respiration.rates) <- c("File_ID","Intercept", "umolO2.L.sec","Temp.C", "Salinity", "Pressure") 

for(i in 1:length(file.names)) {
  
#Reading and processing respirometry data
Respiration_Data <- read_and_process_csv(file.names[i], path.p) %>%
    mutate(File_ID=file.names[i]) %>% # Add the file name to the data frame
    dplyr::select(File_ID, Date, Time, Value, Temp, Salinity, Pressure) %>%  # Selecting variables
    unite(Datetime, Date, Time, sep = " ") %>%  # Combine Date and Time into Datetime
    mutate(Datetime = mdy_hms(Datetime)) %>%  # Convert Datetime to POSIXct format
    drop_na()  # Drop rows with NA values

  # Getting the active file name
  Filename <- sub(".csv", "", file.names[i])
  
  # Matching file row to file name 
  Row <- which(respo.metadata$File_ID == file.names[i])
  
  Start_Time <- ymd_hms(respo.metadata$Respirometry_Start_Time[Row])
  Stop_Time <- ymd_hms(respo.metadata$Respirometry_Stop_Time[Row])
  
  # Trimming start and stop times
  Respiration_Data <-  Respiration_Data %>%
    filter(Datetime >= Start_Time & Datetime <= Stop_Time) %>% # filter to start and stop time
    slice(120:n()) %>% # drop the first two minutes of data
    mutate(Sec = 1:n())  # create a new column for every second for the regression
  
  Respiration_Data_Unthinned <- Respiration_Data # saving original data prior to thinning
  
  Respiration_Tibble <- tibble(Time = as.numeric(), Value = as.numeric(),
                               Temp = as.numeric(), Sec = as.numeric(),
                               Salinity = as.numeric(), Pressure = as.numeric())

  Subsample <- respR::subsample(Respiration_Data, n = 15, plot=FALSE)
  Respiration_Tibble<-rbind(Respiration_Tibble, Subsample)
  
# Plotting full data     
full.plot<- ggplot(Respiration_Data_Unthinned, aes(x = Sec, y = Value)) +
    geom_point(color = "lightblue") +
    labs(x = 'Time (seconds)', y = expression(paste(' O'[2],' (',mu,'mol/L)')),
      title = "Original Data")
  
# Plotting thinned data 
thinned.plot <- ggplot(Respiration_Tibble, aes(x = Sec, y = Value))+
    geom_point(color = "lightblue")+
    labs(x = 'Time (seconds)', y = expression(paste(' O'[2],' (',mu,'mol/L)')),
      title = "Thinned Data")

# Defining the alpha value for bootstrapping
N <- nrow(Respiration_Tibble)
alpha <- 0.3  #alpha value for bootstrapping
min_n <- alpha * N

if (min_n < 15) {
  stop("Alpha is too small")
}

# Bootstrapping technique (Olito et al. 2017)
Regs <- rankLocReg(xall=Respiration_Tibble$Sec, yall=Respiration_Tibble$Value, alpha=alpha, method="pc", verbose=TRUE)  

#creates pdf of each individual respiration plot and statistics for each plot
pdf(paste0(here::here("Output","Respo_Output","Thinning_Plots"),"/", Filename ,"thinning.pdf"))
  
plot(Regs) # plot the results of the regs bootstrapping technique
plot(full.plot+thinned.plot) # use patchwork to bring the raw and thinned data together
dev.off()

# fill in all the O2 consumption and rate data
respiration.rates[i,2:3] <- Regs$allRegs[1,c(4,5)] #inserts slope and intercept in the dataframe
respiration.rates[i,1] <- paste0(Filename,".csv") #stores the file name
respiration.rates[i,4] <- mean(Respiration_Tibble$Temp, na.rm=TRUE) #stores mean temperature organisms experienced
respiration.rates[i,5] <- mean(Respiration_Tibble$Salinity, na.rm=TRUE) #stores mean salinity organisms experienced
respiration.rates[i,6] <- mean(Respiration_Tibble$Pressure, na.rm=TRUE) #stores mean pressure organisms experienced

}

#writing out data as a CSV
write_csv(respiration.rates, here("Data", "Thinned_Respirometry_Data", "Respirometry.Data.csv")) #saves to location


```

## Rate Calculations


```{r, Loading Data, eval=FALSE, message=FALSE}

# Reading in Experiment Data 

# Reading in experimental treatment metadata for the mesocosm 
experiment.metadata <- read_csv(here::here("Data", "Experiment_Metadata","Experiment_Treatment_Metadata.csv"))

# Reading in mesocosm system data and averaging pH and temperature data
mesocosm.treatment.data <- read_csv(here::here("Data", "Mesocosm_Data", "Completed_Mesocosm_Dataset_Carbonate.csv")) %>% 
  filter(Tank_ID != "Sump") %>%
  filter(Date >= "2022-08-21") %>%
  filter(Date != "2022-08-27") %>%
  dplyr::select(Temp_Treatment, pH_Treatment, Temp_C, pH) %>% 
  dplyr::group_by(Temp_Treatment, pH_Treatment) %>%
  dplyr::summarize(Mean_pH=mean(pH, na.rm = TRUE),
                   Mean_Temp=mean(Temp_C, na.rm = TRUE))

# Merging the experimental treatment data 
treatment.info <- left_join(experiment.metadata, mesocosm.treatment.data) 

# Reading in organism data 

# Reading in organism morphometric data 
snail.data <- read_csv(here::here("Data", "Snail_Data", "Tegula_funebralis_Morphometric_Data.csv")) %>% 
  dplyr::select(Snail_ID:Experiment_End_Date)

# Reading in ash free dry weight data and calculating ash free dry weight 
snail.afdw.data <- read_csv(here::here("Data", "Snail_Data","Tegula_funebralis_AFDW.csv"))%>% 
  dplyr::select(Snail_ID, Ash_Free_Dry_Weight)

# Joining experimental treatment metadata and snail data 
experiment.data <- left_join(treatment.info, snail.data) %>% 
  left_join(snail.afdw.data)

# Reading in respiration data 

# Reading in respiration metadata
respo.metadata <- read_csv(here::here("Data", "Experiment_Metadata", "Respirometry_Metadata.csv"))

# Loading in respiration data from the foor loop 
respirometry.data <- read_csv(here("Data", "Thinned_Respirometry_Data", "Respirometry.Data.csv"))

# Merging respiration data with the experimental data 
joined.respo.dataset <- left_join(experiment.data, respo.metadata) %>% 
  left_join(respirometry.data) %>% 
  dplyr::select(-File_ID, -Respirometry_Start_Time, -Respirometry_Stop_Time, -Experiment_Start_Date, -Experiment_End_Date)

#Removing snails that faced mortality during the experiment
filtered.respo.dataset <- joined.respo.dataset %>% 
  filter(
         Snail_ID != 43,
         Snail_ID != 36,
         Snail_ID != 72
         ) 
 # mortality (not responsive to forceps)

#load in image icon for plots
image <- readPNG(here::here("Images", "Tegula_funebralis_icon.png"))


#calculating chamber water volume by subtracting organism volume from chamber volume
respo.dataset <- filtered.respo.dataset %>%    
  mutate(Volume_mL = 650-Organism_Volume_mL, #Volume of water in chamber = full chamber volume (650 mL) of water minus organism volume (seawater columns set to 0)
         Volume_L = conv_multiunit(Volume_mL, "mL", "L"), #converting from mL to L (/1000)
         umolO2.sec = umolO2.L.sec*Volume_L) #standardizing to umolO2/sec

# Filtering control respiration rates
respo.control.data <- respo.dataset %>%
  filter(Identity=="Control") %>% 
  group_by(Temp_Treatment, pH_Treatment) %>% #grouping by treatments and control identity
  dplyr::summarize(blank.rate = mean(umolO2.sec, na.rm=TRUE)) #means for blank seawater respiration 

# Joining summarized control respiration data to full respiration dataset 
respo.dataset <- left_join(respo.dataset, respo.control.data)

# Conversionting respiration rates and normalizing to ash free dry weight (grams) 
respo.rates.dataset <- respo.dataset %>% 
  mutate(
    
    # calculating corrected respiration rates
    umolO2.sec = -(umolO2.sec - blank.rate), # subtract the blank rates from the raw rates

    # converting from umolO2/sec to umolO2/hour
    umolO2.hr=conv_resp_unit(umolO2.sec, from="umol_O2 / sec", to="umol_O2 / hr"),
    umolO2.gram.hr = umolO2.hr / Ash_Free_Dry_Weight,
    
    # converting from umolO2/sec to mmolO2/hour
    mmolO2.hr = conv_resp_unit(umolO2.sec, from="umol_O2 / sec", to="mmol_O2 / hr"), 
    mmolO2.gram.hr = mmolO2.hr / Ash_Free_Dry_Weight,
    
    # converting from mmol/hr to molO2/hour
    molO2.hr = mmolO2.hr / 1000,
    molO2.gram.hr = molO2.hr / Ash_Free_Dry_Weight,

    # converting from umolO2/sec to mLO2/hr
    mLO2.hr = conv_resp_unit(umolO2.sec, from="umol_O2 / sec", to="ml_O2 / hr"),
    mLO2.gram.hr = mLO2.hr / Ash_Free_Dry_Weight,
    
    # converting from mLO2/hr to uLO2/hr
    uLO2.hr = mLO2.hr*1000, # converting from mL to uL
    uLO2.gram.hr = uLO2.hr/ Ash_Free_Dry_Weight) %>% 
  
  filter(Identity == "Treatment") 

 as_tibble(respo.rates.dataset)

write_csv(respo.rates.dataset, here("Data", "Thinned_Respirometry_Data", "Respiration.Rates.Dataset.csv"))

#Paine 1971 ~206 uL O2/hr @ 13.5 C ~ 413 uL O2/hr @ 23 C (+ or - 66 uL O2/hr)
```

```{r Examining Respo Rates, warning=FALSE, message=FALSE}

# Creating a discrete color palette for the insect orders
colourCount <- length(unique(respo.rates.dataset$pH_Treatment))
pal.1 <- colorRampPalette(RColorBrewer::brewer.pal(9, "Set1"))(colourCount)

# Scaling figure with orders color-coded
ggplot(respo.rates.dataset, aes(x=log(Ash_Free_Dry_Weight), y=log(umolO2.hr), color=pH_Treatment)) + 
  geom_point(aes(col=pH_Treatment), size=1) + 
  scale_color_manual(values=pal.1) +
  stat_smooth(method="lm", se=T)  + 
  common_theme 

pal.1 <- c("cyan3", "orange", "#E41A1C", "#3A85A8", "#629363", "#C4625D", "#FFC81D", "#BF862B", "#EB7AA9", "#999999")
# Scaling figure with orders color-coded
ggplot(respo.rates.dataset, aes(x=log(Ash_Free_Dry_Weight), y=log(umolO2.hr), color=as.factor(Temp_Treatment), group=as.factor(pH_Treatment))) + 
  facet_wrap(~Temp_Treatment) +
  geom_point(aes(col=as.factor(Temp_Treatment)), size=1) + 
  scale_color_manual(values=pal.1) +
  stat_smooth(method="lm", se=F) + 
  common_theme 
```


# Joule Rate Analysis 

## Oxyjoulemetric Covnersions

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# Caloric Coefficients Data
coefficients_data <- tibble(
  Substrate = c("Carbohydrate", "Lipid", "Protein", "Average"),
  `J/mg O2` = c(14.77, 13.73, 13.61, 14.10),
  `J/ml O2` = c(21.06, 19.58, 19.41, 20.11),
  `J/mmol O2` = c(481.86, 447.93, 444.01, 460.00)
)

#Calculating Joules and Kilocalories from Respiration Rates using oxyjoulimetric conversions and calorific coefficients
#Joules = Respiration Rate (mmolO2/hr) * Caloric Coefficient (J/mmol O2)
#calorific coefficients used to convert 
#Substrate	J/mg O2	  J/ml O2	  J/mmol O2	     Ref
#Carbs      14.77	      21.06 	  481.86	     Ivlev 1935; Elliot & Davison 1975; Gnaiger 1983 
#Lipid	    13.73	      19.58	    447.93	     Ivlev 1935; Elliot & Davison 1975; Gnaiger 1983
#Protein. 	13.61	      19.41	    444.01	     Ivlev 1935; Elliot & Davison 1975; Gnaiger 1983 #Average. 	14.10	      20.11	    460.00       Ivlev 1935; Elliot & Davison 1975; Gnaiger 1983

# Create the gt table for Caloric Coefficients
coefficients_table <- gt(coefficients_data) %>%
  tab_header(
    title = "Caloric Coefficients for Oxy-Joulimetric Conversions"
  ) %>% gt_theme %>%
  cols_label(
    Substrate = "Substrate",
    `J/mg O2` = html("J mg<sup>-1</sup> O<sub>2</sub>"),
    `J/ml O2` = html("J ml<sup>-1</sup> O<sub>2</sub>"),
    `J/mmol O2` = html("J mmol<sup>-1</sup> O<sub>2</sub>"))  %>%
 tab_footnote(
    footnote = html("Calorific coefficients derived from Ivlev, 1935; Elliot & Davison,<br>       &nbsp;&nbsp;1975; Gnaiger, 1983."),
    locations = cells_column_labels(columns = c(`J/mg O2`, `J/ml O2`, `J/mmol O2`))
  ) %>% 
  tab_options(table.width = pct(60)) 

# Print the table to see the results
invisible(print(coefficients_table))
invisible(gt::gtsave(coefficients_table, here("Figures", "gt_table_caloric_coefficients.png")))
```

## Energetic Expenditure Calcualtion and Data Analysis 

```{r Energetic Analysis, message=FALSE, warning =FALSE}
library(afex)
library(performance) # for assumption checks
library(forcats)

respo.rates.dataset <- read_csv(here::here("Data", "Thinned_Respirometry_Data", "Respiration.Rates.Dataset.csv"), show_col_types = FALSE) 

# Convert mmolO2/hr to Joules/hr
joule.rates.dataset <- respo.rates.dataset %>%
  mutate(
    J.gram.hr = mmolO2.gram.hr * 460.00,  # Convert mmolO2/gram/hr to Joules/gram/hr
    kJ.gram.hr = J.gram.hr / 1000,  # Convert Joules/gram/hr to kJ/gram/hr
    Temp_Treatment = as.factor(Temp_Treatment),  # Convert to factor if not already
    pH_Treatment = as.factor(pH_Treatment)  # Convert to factor if not already
  ) %>%
  mutate(
    Temp_Treatment = fct_relevel(Temp_Treatment, "12", "14", "16", "18", "20", "22", "24", "26"),
    pH_Treatment = fct_relevel(pH_Treatment, "Ambient", "Low")
  ) %>%
  select(Temp_Treatment, pH_Treatment, J.gram.hr) %>%
  drop_na()  # Drop rows with NA values in any column
write_csv(joule.rates.dataset, here("Data", "Thinned_Respirometry_Data", "Joule.Rates.Dataset.csv"))

# Assuming joule.rates.dataset is your data frame
percent.joule.rates.dataset <- joule.rates.dataset %>%
  select(Temp_Treatment, pH_Treatment, J.gram.hr) %>%
  filter(!is.na(J.gram.hr)) %>%
  group_by(Temp_Treatment, pH_Treatment) %>%
  summarise(
    N = n(),
    Mean_J = mean(J.gram.hr, na.rm = TRUE),
    SD_J = sd(J.gram.hr, na.rm = TRUE),
    SE_J = SD_J / sqrt(N),  # Standard Error
    CI_Lower_J = Mean_J - qt(0.975, N - 1) * SD_J / sqrt(N),
    CI_Upper_J = Mean_J + qt(0.975, N - 1) * SD_J / sqrt(N),
    .groups = "drop"  # drops the automatic grouping
  ) %>%
  mutate(
    CI_J = paste0("[", round(CI_Lower_J, 2), ", ", round(CI_Upper_J, 2), "]"),
    .keep = "unused"
  ) %>%
  pivot_wider(
    names_from = pH_Treatment,
    values_from = c(Mean_J, SD_J, SE_J, CI_J, N),
    names_sep = "_"
  ) %>%
  mutate(
    Percent_Difference = (Mean_J_Ambient - Mean_J_Low) / Mean_J_Ambient * 100
  ) 


summarized_joule.rates.dataset <- joule.rates.dataset %>%
  group_by(Temp_Treatment, pH_Treatment) %>%
  summarize(
    mean = mean(J.gram.hr, na.rm = TRUE),
    sd = sd(J.gram.hr, na.rm = TRUE),
    median = median(J.gram.hr, na.rm = TRUE),
    min = min(J.gram.hr, na.rm = TRUE),
    max = max(J.gram.hr, na.rm = TRUE),
    N = n(),
    CI_Upper = mean - qt(0.975, N - 1) * sd / sqrt(N),
    CI_Lower = mean + qt(0.975, N - 1) * sd / sqrt(N)
  ) 

summary(print(joule.rates.dataset))
summary(print(summarized_joule.rates.dataset))

aov_joule <- aov_ez("Snail_ID", "J.gram.hr", joule.rates.dataset,
                    between = c("Temp_Treatment", "pH_Treatment"))

model_joule <- lm(J.gram.hr ~ Temp_Treatment*pH_Treatment, joule.rates.dataset)
print(summary(model_joule))

# Homogeneity of variance test
performance::check_homogeneity(model_joule) 
#Warning: Variances differ between groups (Levene's Test, p = 0.024)

#log transofrming the data to test for homogeneity of variance
log.joule.rates.dataset <- joule.rates.dataset %>%
  mutate(log_J.gram.hr = log(J.gram.hr)) %>% 
  dplyr::select(Snail_ID, Temp_Treatment, pH_Treatment, log_J.gram.hr)

model_log_joule <- lm(log_J.gram.hr ~ Temp_Treatment*pH_Treatment, log.joule.rates.dataset)
print(summary(model_log_joule))

performance::check_homogeneity(model_log_joule)
#Warning: Variances differ between groups (Levene's Test, p = 0.024)


# Normality of residuals tests
# Shapiro-Wilk test for each treatment combination
joule_normality_tests <- joule.rates.dataset %>% 
  select(-Snail_ID) %>%
  shapiro_test(J.gram.hr)

# Print results
print(joule_normality_tests)

log_joule_normality_tests <- log.joule.rates.dataset %>% 
  select(-Snail_ID) %>%
  shapiro_test(log_J.gram.hr)

# Print results
print(log_joule_normality_tests)

# Levene's Test for homogeneity of variances
joule_levene_test <- leveneTest(J.gram.hr ~ 
interaction(pH_Treatment, Temp_Treatment), data = joule.rates.dataset)

# Print results
print(levene_test)

log_joule_levene_test <- leveneTest(log_J.gram.hr ~
interaction(pH_Treatment, Temp_Treatment), data = log.joule.rates.dataset)

print(log_joule_levene_test)

```




# logistic regression for Q10 analysis 

```{r}
This function works with gtsummary::add_difference() to calculate adjusted differences and confidence intervals based on results from a logistic regression model. Adjustment covariates are set to the mean to estimate the adjusted difference. The function uses bootstrap methods to estimate the adjusted difference between two groups. The CI is estimate by either using the SD from the bootstrap difference estimates and calculating the CI assuming normality or using the centiles of the bootstrapped differences as the confidence limits

The function can also be used in add_p(), and if you do, be sure to set boot_n = 1 to avoid long, unused computation.

logistic_reg_adj_diff(
  data,
  variable,
  by,
  adj.vars,
  conf.level,
  type,
  ci_type = c("sd", "centile"),
  boot_n = 250,
  ...
)

# Example 1 -----------------------------------------------------------------
logistic_reg_adj_diff_ex1 <-
  tbl %>%
  add_difference(
    test = everything() ~ logistic_reg_adj_diff,
    adj.vars = "stage"
  )

# Example 2 -----------------------------------------------------------------
# Use the centile method, and
# change the number of bootstrap resamples to perform
logistic_reg_adj_diff_ex2 <-
  tbl %>%
  add_difference(
    test = everything() ~
      purrr::partial(logistic_reg_adj_diff, ci_type = "centile", boot_n = 100),
    adj.vars = "stage"
  )

CT is the temperature at which 50% of the animals failed to right. Each CT was estimated using logistic regression in JMP12 using the Fit Model menu and the inverse prediction option. In some cases where every temperature produced either 100% success or 100% failure, the exact LT50 with confidence limits could not be estimated in this way, and was therefore estimated using linear interpolation. Logistic regression with species and temperature as factors was used to determine if thermal tolerances of the species differed significantly. To find the optimal temperatures (Topt), performance curves were generated by fitting quadratic and biexponential curves to the time to right for each species using the nonlinear models implemented in JMP12. The inflection point, at the minimum time to right (i.e. optimal performance) was calculated using the first derivative of the function that provided the best fit to the data based on comparisons of values for Akaike’s information criterion corrected for small sample sizes (AICc) values. Finally, 1-way ANOVA with post hoc Tukey HSD tests was used to determine at which temperatures performance was statistically significantly reduced compared to Topt.
```


```{r}





#as_ggplot() Convert gt/gtsummary table to ggplot
devtools::install_github("MSKCC-Epi-Bio/bstfun")
library(bstfun)


#gouping variable
  data.frame(
    race_asian = sample(c(TRUE, FALSE), 20, replace = TRUE),
    race_black = sample(c(TRUE, FALSE), 20, replace = TRUE),
    race_white = sample(c(TRUE, FALSE), 20, replace = TRUE),
    age = rnorm(20, mean = 50, sd = 10)
  ) %>%
  gtsummary::tbl_summary(
    label = list(race_asian = "Asian",
                 race_black = "Black",
                 race_white = "White",
                 age = "Age")
  ) %>%
  add_variable_grouping(
    "Race (check all that apply)" = c("race_asian", "race_black", "race_white")
  ) 
#compact themee

  head(trial) %>%
  gt::gt() %>%
  style_tbl_compact()

#adding plot
  trial %>%
  select(age, marker) %>%
  tbl_summary(missing = "no") %>%
  add_sparkline(type ="boxplot") %>% 
    as_ggplot()

```

```{r}
library(equatiomatic)

m_interaction <- lm(J.gram.hr ~ Temp_Treatment*pH_Treatment, joule.rates.dataset)


eq <- extract_eq(
  m_interaction,
  var_colors = c(
    J.gram.hr = "#ffa91f"
  ),
  greek_colors = c(
    "black", "#3A21B3", "#58A1D9", "#FF7582", "black"
  ),
  subscript_colors = c(
     NA_character_, "#3A21B3", "#58A1D9", "#FF7582", NA_character_
  ),
  wrap = TRUE,
  terms_per_line = 3
)

library(equatiomatic)
library(shiny)
library(shinyWidgets)
library(gtsummary)
library(gt)

library(equatiomatic)
library(shiny)
library(shinyWidgets)
library(gtsummary)
library(gt)

ui <- fluidPage(
  titlePanel("equatiomatic w/Shiny"),
  withMathJax(),
  sidebarLayout(
    sidebarPanel(
      multiInput(
        inputId = "xvars",
        label = "Select predictor variables :",
        choices = names(penguins)[-3],
        selected = "island"
      )
    ),
    mainPanel(
      eqOutput("eq2"),
      eqOutput("equation"),
      gt_output("tbl")
    )
  )
)

server <- function(input, output) {
  model <- reactive({
    form <- paste("bill_length_mm ~ ", paste(input$xvars, collapse = " + "))
    lm(as.formula(form), penguins)
  })

  output$equation <- renderEq(
      extract_eq(
        model(),
        wrap = TRUE,
        terms_per_line = 2,
        use_coefs = TRUE,
        font_size = "Huge"
    )
  )

  output$tbl <- render_gt({
    as_gt(tbl_regression(model()))
  })

}

shinyApp(ui = ui, server = server)
```

```{r}
```

```{r Energetic Analysis, message=FALSE, warning =FALSE}


library(gtsummary)
#extract_eq() 	'LaTeX' code for R models
#remotes::install_github("datalorax/equatiomatic")






library(lme4)
um_long3 <- lmer(score ~ 1 + (1 | sid) + (1 | school) + (1 | district),
  data = sim_longitudinal
)
#> boundary (singular) fit: see help('isSingular')
extract_eq(um_long3)

lev1_hsb <- lmer(math ~ female + ses + minority + (1 | sch.id), hsb)
extract_eq(lev1_hsb)

# Assuming `joule.rates.dataset` is your final dataset
table1 <- tbl_summary(
  joule.rates.dataset,
  by = c("Temp_Treatment", "pH_Treatment"),
  statistic = list(all_continuous() ~ "{mean} ({sd})",
                   all_categorical() ~ "{n}")) %>% 
  add_difference(
    difference = list(all_continuous() ~ "mean"),
    by = c("Temp_Treatment", "pH_Treatment")
  ) 

table2 <- tbl_regression(
  model_joule,
  exponentiate = TRUE
)

final_table <- tbl_merge(
  list(table1, table2),
  tab_spanner = c("Descriptive Statistics", "Model Results")
)

library(gtsummary)
library(titanic)
library(tidyverse)
library(plotrix) #has a std.error function

packageVersion("gtsummary")
#> [1] '1.4.0'

# create smaller version of the dataset
df <- 
  titanic_train %>%
  select(Sex, Embarked, Age, Fare) %>%
  filter(Embarked != "") # deleting empty Embarked status

# first, write a little function to get the 2-way ANOVA p-values in a table
# function to get 2-way ANOVA p-values in tibble
twoway_p <- function(variable) {
  paste(variable, "~ Sex * Embarked") %>%
    as.formula() %>%
    aov(data = df) %>% 
    broom::tidy() %>%
    select(term, p.value) %>%
    filter(complete.cases(.)) %>%
    pivot_wider(names_from = term, values_from = p.value) %>%
    mutate(
      variable = .env$variable,
      row_type = "label"
    )
}

# add all results to a single table (will be merged with gtsummary table in next step)
twoway_results <-
  bind_rows(
    twoway_p("Age"),
    twoway_p("Fare")
  )
twoway_results
#> # A tibble: 2 x 5
#>            Sex Embarked `Sex:Embarked` variable row_type
#>          <dbl>    <dbl>          <dbl> <chr>    <chr>   
#> 1 0.00823      3.97e- 1         0.611  Age      label   
#> 2 0.0000000191 4.27e-16         0.0958 Fare     label


tbl <-
  # first build a stratified `tbl_summary()` table to get summary stats by two variables
  df %>%
  tbl_strata(
    strata =  Sex,
    .tbl_fun =
      ~.x %>%
      tbl_summary(
        by = Embarked,
        missing = "no",
        statistic = all_continuous() ~ "{mean} ({std.error})",
        digits = everything() ~ 1
      ) %>%
      modify_header(all_stat_cols() ~ "**{level}**")
  ) %>%
  # merge the 2way ANOVA results into tbl_summary table
  modify_table_body(
    ~.x %>%
      left_join(
        twoway_results,
        by = c("variable", "row_type")
      )
  ) %>%
  # by default the new columns are hidden, add a header to unhide them
  modify_header(list(
    Sex ~ "**Sex**", 
    Embarked ~ "**Embarked**", 
    `Sex:Embarked` ~ "**Sex * Embarked**"
  )) %>%
  # adding spanning header to analysis results
  modify_spanning_header(c(Sex, Embarked, `Sex:Embarked`) ~ "**Two-way ANOVA p-values**") %>%
  # format the p-values with a pvalue formatting function
  modify_fmt_fun(c(Sex, Embarked, `Sex:Embarked`) ~ style_pvalue) %>%
  # update the footnote to be nicer looking
  modify_footnote(all_stat_cols() ~ "Mean (SE)")

```


```{r Energetic Analysis, message=FALSE, warning =FALSE}
# Create summary table with gtsummary
tbl_summary <- joule.rates.dataset %>%
  select(-Snail_ID) %>%
  tbl_continuous(
    variable = J.gram.hr,
    by = pH_Treatment,
    include = Temp_Treatment,
    statistic = J.gram.hr ~ "{mean} ({sd}) {median} [{min}, {max}]"
  )
  add_p()




tbl_summary <- joule.rates.dataset %>%
  select(pH_Treatment, Temp_Treatment, J.gram.hr) %>%
  filter(Temp_Treatment == "12") %>%
  select(-Temp_Treatment) %>%
  tbl_summary(
    by = pH_Treatment,
    type = J.gram.hr ~ "continuous",
    statistic = list(J.gram.hr ~ "{mean} ?? {sd}"),
    digits = ~ 2,
    label = list(J.gram.hr ~ html("Temperature Treatment")),
    missing="no"
  ) %>% 
  add_n(  statistic = "{N}", col_label = "**N**") %>% 
  add_p(
    
    
tbl_summary <- joule.rates.dataset %>%
  select(pH_Treatment, Temp_Treatment, J.gram.hr) %>%
  mutate(J.gram.hr = paste("J g/hr", J.gram.hr)) %>%
  tbl_strata(
    strata = pH_Treatment,
    ~ .x %>%
      tbl_summary(missing = "no") %>%
      modify_header(all_stat_cols() ~ "**{level}**")
  )
  
    http://127.0.0.1:15721/help/library/gtsummary/help/add_n.tbl_summary
    http://127.0.0.1:15721/help/library/gtsummary/help/add_stat_label
http://127.0.0.1:15721/help/library/gtsummary/help/add_q
```


# TABLE THAT WORKS 
```{r, message=FALSE}
#Create a summary table for Joule rates using gtsummary
theme_gtsummary_compact()

tbl_summary <- joule.rates.dataset %>%
  select(pH_Treatment, Temp_Treatment, J.gram.hr) %>%
  tbl_continuous(variable = J.gram.hr, by = pH_Treatment,
                 statistic = ~ "{mean} ?? {se}",
                 label=everything() ~"Temperature (C)") %>%
  modify_spanning_header(all_stat_cols() ~ "**Joule g/hr (Mean +- SE)**")

# table comparing ambient versus low pH treatments
ANOVA_pvalues <- joule.rates.dataset %>%
  select(pH_Treatment, Temp_Treatment, J.gram.hr) %>%
  filter(pH_Treatment %in% c("Ambient", "Low")) %>%
  tbl_continuous(variable = J.gram.hr, by = pH_Treatment,
                 label=everything() ~"Temperature (C)") %>%
  add_p() %>%
  modify_header(p.value ~ md("**Ambient vs. Low**")) %>%
  # hide summary stat columns
  modify_column_hide(all_stat_cols())

#checout custom summary
tbl_ANOVA <- tbl_merge(
  list(tbl_summary, ANOVA_pvalues),
  tab_spanner = c("**Joule g/hr (Mean +- SE)**", "**ANOVA**")
)

invisible(print(tbl_ANOVA))
```





```{r Energetic Analysis, message=FALSE, warning =FALSE}
The main purpose of the package ExpDes is to analyze simple experiments under completely randomized designs (crd()), randomized block designs (cbd()) and Latin square designs (latsd()). Also enables the analysis of treatments in a factorial design with 2 and 3 factors (fat2.crd(), fat3.crd(), fat2.rbd(), fat3.rbd()) and also the analysis of split-plot designs (split2.crd(), split2.rbd()). Other functionality is analyzing experiments with one additional treatments on completely randomized design and randomized blocks design with 2 or 3 factors (fat2.ad.crd(), fat2.ad.rbd(), fat3.ad.crd() and fat3.ad.crd()).

 crd(treat, resp, quali = TRUE, mcomp = "tukey") 
 
  crd(trat, resp, quali = FALSE, sigF = 0.05) 
```


```{r}
#install.packages("multiplestressR")

##Note that to install the multiplestressR package from CRAN, the above line must be uncommented.

library(multiplestressR)


df <- multiplestressR::survival
We can view the first few rows of this dataframe using the following code:

head(df)
Installation
The first thing that needs to be done is to install the multiplestressR package. This can be installed from CRAN using the following code:

#install.packages("multiplestressR")

##Note that to install the multiplestressR package from CRAN, the above line must be uncommented.

library(multiplestressR)
Barring any issues, the package is now installed.

Data
The next step is to load data into R. For this tutorial, we will be using an example dataset which is provided by the multiplestressR package, which can be loaded using the following code:

df <- multiplestressR::survival
We can view the first few rows of this dataframe using the following code:

head(df)
##   Sample_Size_Control Standard_Deviation_Control Mean_Control
## 1                   4                 0.02886751    0.9550000
## 2                  10                 0.08743887    0.8430000
## 3                   4                 0.02645751    0.9550000
## 4                   6                 0.04195235    0.8900000
## 5                   4                 0.09055385    0.8600000
## 6                   6                 0.04676181    0.9033333
##   Sample_Size_Temperature Standard_Deviation_Temperature Mean_Temperature
## 1                       4                     0.07023769        0.7700000
## 2                      10                     0.11246728        0.8560000
## 3                       4                     0.10424331        0.7800000
## 4                       6                     0.07174027        0.8066667
## 5                       4                     0.09814955        0.7750000
## 6                       6                     0.11338724        0.7783333
##   Sample_Size_pH Standard_Deviation_pH   Mean_pH Sample_Size_Temperature_pH
## 1              4            0.04041452 0.8950000                          4
## 2             10            0.08753412 0.7880000                         10
## 3              4            0.03201562 0.7675000                          4
## 4              6            0.04578937 0.8683333                          6
## 5              4            0.04966555 0.7300000                          4
## 6              6            0.04578937 0.9016667                          6
##   Standard_Deviation_Temperature_pH Mean_Temperature_pH
## 1                        0.08770215              0.6725
## 2                        0.07397447              0.7750
## 3                        0.07762087              0.6575
## 4                        0.07563068              0.6900
## 5                        0.07365460              0.7275
## 6                        0.04037326              0.8450
This dataset has been generated (i.e., does not come from ???real-world??? experiments), but reflects survival data for 250 unique experiments. For this dataset, survival rates are measured for organisms exposed to i) control conditions, ii) altered temperature, iii) altered pH, or iv) altered temperature and pH together.

As you can see, the dataframe contains data for eight interactions (rows) and twelve variables (columns), with three columns for each treatment (X_Control, X_Temperature, X_pH, X_Temperature_pH). For each treatment, there is a column for treatment means (Mean_X), treatment standard deviation (Standard_Deviation_X), and treatment sample size (Sample_Size_X). It is important to note that any dataset must contain these three variables; means must be used (i.e., not medians or other metrics), standard deviation must be reported (i.e., not standard error or confidence intervals), and sample sizes must be reported.

Null models
The next step is to calculate the null models for each interaction.

Within the multiplestressR package, there are two functions for calculating the null models, either effect_size_additive or effect_size_multiplicative. The former is the implementing the additive null model, while the latter is for implementing the multiplicative null model. For this tutorial we will be implementing the additive null model. Though an example of code using the multiplicative null model is detailed at the very end of this tutorial.

For the effect_size_additive function, fourteen variables need to be specified. Of these twelve relate to the previous discussed means, standard deviations, and sample sizes. Specifying the variables in this manner ensures that each treatment is assigned the correct data (e.g., Control and StressorA are not mixed up) and that different variables are not incorrectly labelled (e.g., standard deviations and means are not mixed up).

The final two variables are user specified at this point. Firstly, Significance_Level corresponds to the level of alpha which should be used when calculating confidence intervals for a given interaction effect size. The default is 0.05, (i.e., calculating 95% confidence intervals) but this can be changed by specifying a number between 0 and 1. Secondly, Hedges??? d (i.e., the form of the additive null model used here) has been shown to slightly overestimate effects where small sizes are small (i.e., <20). As such, there is a small statistical correction which can be applied to the null model to overcome this bias. By default, the variable Small_Sample_Correction is set to TRUE, namely the bias is corrected, though this can be disabled by setting Small_Sample_Correction to FALSE. Note that for the mulitplicative null model (i.e., effect_size_multiplicative this bias does not exist and so the parameter is specified in the model).

The additive null model can be run using the following code:

df  <- effect_size_additive(Control_N                = df$Sample_Size_Control,           
                            Control_SD               = df$Standard_Deviation_Control,    
                            Control_Mean             = df$Mean_Control,                  
                            StressorA_N              = df$Sample_Size_Temperature,         
                            StressorA_SD             = df$Standard_Deviation_Temperature,  
                            StressorA_Mean           = df$Mean_Temperature,                
                            StressorB_N              = df$Sample_Size_pH,         
                            StressorB_SD             = df$Standard_Deviation_pH,  
                            StressorB_Mean           = df$Mean_pH,                
                            StressorsAB_N            = df$Sample_Size_Temperature_pH,       
                            StressorsAB_SD           = df$Standard_Deviation_Temperature_pH,
                            StressorsAB_Mean         = df$Mean_Temperature_pH,
                            Small_Sample_Correction  = TRUE,
                            Significance_Level       = 0.05)
head(df)
##   Control_N Control_SD Control_Mean StressorA_N StressorA_SD StressorA_Mean
## 1         4 0.02886751    0.9550000           4   0.07023769      0.7700000
## 2        10 0.08743887    0.8430000          10   0.11246728      0.8560000
## 3         4 0.02645751    0.9550000           4   0.10424331      0.7800000
## 4         6 0.04195235    0.8900000           6   0.07174027      0.8066667
## 5         4 0.09055385    0.8600000           4   0.09814955      0.7750000
## 6         6 0.04676181    0.9033333           6   0.11338724      0.7783333
##   StressorB_N StressorB_SD StressorB_Mean StressorsAB_N StressorsAB_SD
## 1           4   0.04041452      0.8950000             4     0.08770215
## 2          10   0.08753412      0.7880000            10     0.07397447
## 3           4   0.03201562      0.7675000             4     0.07762087
## 4           6   0.04578937      0.8683333             6     0.07563068
## 5           4   0.04966555      0.7300000             4     0.07365460
## 6           6   0.04578937      0.9016667             6     0.04037326
##   StressorsAB_Mean Interaction_Effect_Size Interaction_Variance
## 1           0.6725              -0.4949691            0.8831246
## 2           0.7750              -0.2641530            0.3842288
## 3           0.6575               0.7724633            0.8927570
## 4           0.6900              -1.3751384            0.6534558
## 5           0.7275               0.8340493            0.8954668
## 6           0.8450               0.8760449            0.6317925
##   Interaction_CI_Upper Interaction_CI_Lower Null_Model
## 1            1.3469013           -2.3368395   Additive
## 2            0.9507540           -1.4790601   Additive
## 3            2.6243513           -1.0794248   Additive
## 4            0.2092301           -2.9595069   Additive
## 5            2.6887457           -1.0206471   Additive
## 6            2.4339296           -0.6818399   Additive
Accordingly, this function adds five new variables (columns) and slightly alters the dataset.

Firstly, note that within the dataset, the columns for means, standard deviations, and sample sizes have been renamed.

Secondly, Interaction_Effect_Size is a value which indicates the strength of any interaction which is occuring, the greater the value (positive or negative) of the Interaction_Effect_Size the stronger the interaction. If an interaction has an Interaction_Effect_Size of exactly 0, then the effect predicted by the additive null model is exactly the same as the observed effect of the interacting stressors.

Thirdly, Interaction_Variance details the uncertainty in the Interaction_Effect_Size. Given the uncertainty (i.e., standard deviations) in the raw data it stands to reason that there will also be uncertainty in the value of Interaction_Effect_Size. This value for the variance can be be used to calculate confidence intervals based upon the chosen Significance_Level here referred to as Interaction_CI_Upper and Interaction_CI_Lower. The values of Interaction_Effect_Size and confidence intervals can then be used to classify each interaction.

The final variable in the dataset is Null_Model, this simply reflects whether the additive or multiplicative null model was implemented.

Classifying interactions
The next step is to classify each interaction.

Within the multiplestressR package, this can be done using the classify_interactions function.

To implement this function, the following three variables need to be specified.

Firstly, effect_size_dataframe is the output of the either the effect_size_additive or effect_size_multiplicative functions. Note that this must be the exact output of either of these functions.

Secondly, assign_reversals determines whether the interaction classificaton of reversal should be distinguished from antagonisms (see above). The default for this function is TRUE; though this can be altered by specifying the variable as FALSE.

Thirdly, the remove_directionality parameter can be implemented. For the purposes of this tutorial this parameter is not considered any further, its default value is likewise FALSE. However, while this parameter is unlikely to be important for the results of any single experiment, it is important for those researchers conducting a meta-analysis. This will be addressed in a subsequent tutorial. At present, those conducting a meta-analysis should consult the following published papers when considering the importance of removing directionality from those analyses (Burgess et al. (2021b), Jackson et al. (2016), Seifert et al. (2020), Siviter et al. (2021)).

For a detailed outline of this function (e.g., how interactions are assigned, and the use of the remove_directionality parameter), see the help guide for the classify_interactions function. This can be accessed by using the following code:

#    ?multiplestressR::classify_interactions
##    Note that the above `#` must be removed in order for this code to be run.
As such, interactions can be classified by using the following code:

df  <- classify_interactions(effect_size_dataframe = df,
                    assign_reversals = TRUE)
head(df)
##   Control_N Control_SD Control_Mean StressorA_N StressorA_SD StressorA_Mean
## 1         4 0.02886751    0.9550000           4   0.07023769      0.7700000
## 2        10 0.08743887    0.8430000          10   0.11246728      0.8560000
## 3         4 0.02645751    0.9550000           4   0.10424331      0.7800000
## 4         6 0.04195235    0.8900000           6   0.07174027      0.8066667
## 5         4 0.09055385    0.8600000           4   0.09814955      0.7750000
## 6         6 0.04676181    0.9033333           6   0.11338724      0.7783333
##   StressorB_N StressorB_SD StressorB_Mean StressorsAB_N StressorsAB_SD
## 1           4   0.04041452      0.8950000             4     0.08770215
## 2          10   0.08753412      0.7880000            10     0.07397447
## 3           4   0.03201562      0.7675000             4     0.07762087
## 4           6   0.04578937      0.8683333             6     0.07563068
## 5           4   0.04966555      0.7300000             4     0.07365460
## 6           6   0.04578937      0.9016667             6     0.04037326
##   StressorsAB_Mean Interaction_Effect_Size Interaction_Variance
## 1           0.6725              -0.4949691            0.8831246
## 2           0.7750              -0.2641530            0.3842288
## 3           0.6575               0.7724633            0.8927570
## 4           0.6900              -1.3751384            0.6534558
## 5           0.7275               0.8340493            0.8954668
## 6           0.8450               0.8760449            0.6317925
##   Interaction_CI_Upper Interaction_CI_Lower Null_Model
## 1            1.3469013           -2.3368395   Additive
## 2            0.9507540           -1.4790601   Additive
## 3            2.6243513           -1.0794248   Additive
## 4            0.2092301           -2.9595069   Additive
## 5            2.6887457           -1.0206471   Additive
## 6            2.4339296           -0.6818399   Additive
##   Interaction_Classification
## 1                       Null
## 2                       Null
## 3                       Null
## 4                       Null
## 5                       Null
## 6                       Null
By considering the first few rows of the dataset, it is evident that a single new column has been added to the dataset. This column Interaction_Classification describes the classification assigned to see interaction.

Summary plots
The multiplestressR package also has a function (summary_plots) which can be used to generate five plots which may help interpret any analyses. This function has three input parameters,

Firstly, effect_size_dataframe which corresponds to the output of the classify_interactions function.

Secondly, Small_Sample_Correction which corresponds to whether or not the bias correction for small sample sizes was implemented in the effect_size_additive function.

Finally, Significance_Level which again corresponds to the significance level used in the effect_size_additive (or effect_size_multiplicative) function.

The summary plots can be generated using the following code:

dfa_plots <- summary_plots(effect_size_dataframe = df,
                           Small_Sample_Correction = TRUE,
                           Significance_Level = 0.05)

dfa_plots


For this analysis, only the first two plots (going clockwise from top-left) are overly useful for any interpretation, although the final three plots are likely to be useful to those researchers conducting a meta-analysis.

The first figure reveals the frequency of the different interaction classifications for interactions within a dataset. For this analysis it is apparent that 76.4% of interactions are assigned an null (i.e., additive) interaction class, 16.8% of interactions are assigned a antagonistic class, 4% of interactions are assigned a synergistic interaction class, while 2.8% of interactions are assigned a reversal interaction class.

The second figure plots Interaction_Effect_Size against median treatment sample sizes (averaged across all four treatments). On this figure, black lines indicate the critical effect size for a given sample size (i.e., the minimum effect size required in order for an interaction to be significantly different to zero). Only those interactions which have interaction effect sizes greater than the critical effect size have been assigned a non-null interaction classification.

It is also important to note that a null interaction classification does not necessarily mean that no interaction is occurring. A null classification may instead be assigned due to an experiment having an insufficient power to detect a given interaction (i.e., insufficient sample sizes to statistically determine whether an interaction between stressors was occurring).

For more information on the role of sample sizes in determining interaction classifications and critical effect sizes, see Burgess et al. (2021a).

Finally, all these figures are generated using ggplot, as such they can be altered by the user. For instance, if a user wishes to use the second figure with the legend shown and using a logarithmic scale on the x-axis, this can be done using the following code.

library(ggplot2)

dfa_plots[[2]] + 
  scale_x_continuous(trans='log10') +
  theme(legend.position="top",
        legend.title = element_blank())


Concluding remarks
Overall, this tutorial has shown how the multiplestressR package can be easily and quickly used to implement null models and classify interactions. Indeed, it provides an easy to use framework which is applicable in a range of scenarios. Furthermore, this package allows the results of experiments, studies, or meta-analyses to be easily compared with no concerns regarding how differences in analytical methodologies may affect results.

This tutorial likewise provides a framework which researchers may wish to modify for their studies, allowing researchers unfamiliar with R or these statistical tools to easily conduct a rigorous analysis of their data.

Multiplicative null model
Below is an example of the code which can be used to conduct the same analysis as that detailed above, but for the multiplicative null model, not the additive null model.

#install.packages("multiplestressR")

##Note that to install the multiplestressR package from CRAN, the above line must be uncommented.

library(multiplestressR)

dfm <- multiplestressR::survival

head(dfm)
##   Sample_Size_Control Standard_Deviation_Control Mean_Control
## 1                   4                 0.02886751    0.9550000
## 2                  10                 0.08743887    0.8430000
## 3                   4                 0.02645751    0.9550000
## 4                   6                 0.04195235    0.8900000
## 5                   4                 0.09055385    0.8600000
## 6                   6                 0.04676181    0.9033333
##   Sample_Size_Temperature Standard_Deviation_Temperature Mean_Temperature
## 1                       4                     0.07023769        0.7700000
## 2                      10                     0.11246728        0.8560000
## 3                       4                     0.10424331        0.7800000
## 4                       6                     0.07174027        0.8066667
## 5                       4                     0.09814955        0.7750000
## 6                       6                     0.11338724        0.7783333
##   Sample_Size_pH Standard_Deviation_pH   Mean_pH Sample_Size_Temperature_pH
## 1              4            0.04041452 0.8950000                          4
## 2             10            0.08753412 0.7880000                         10
## 3              4            0.03201562 0.7675000                          4
## 4              6            0.04578937 0.8683333                          6
## 5              4            0.04966555 0.7300000                          4
## 6              6            0.04578937 0.9016667                          6
##   Standard_Deviation_Temperature_pH Mean_Temperature_pH
## 1                        0.08770215              0.6725
## 2                        0.07397447              0.7750
## 3                        0.07762087              0.6575
## 4                        0.07563068              0.6900
## 5                        0.07365460              0.7275
## 6                        0.04037326              0.8450
dfm <- effect_size_multiplicative(Control_N                = dfm$Sample_Size_Control,           
                                  Control_SD               = dfm$Standard_Deviation_Control,    
                                  Control_Mean             = dfm$Mean_Control,                  
                                  StressorA_N              = dfm$Sample_Size_Temperature,         
                                  StressorA_SD             = dfm$Standard_Deviation_Temperature,  
                                  StressorA_Mean           = dfm$Mean_Temperature,                
                                  StressorB_N              = dfm$Sample_Size_pH,         
                                  StressorB_SD             = dfm$Standard_Deviation_pH,  
                                  StressorB_Mean           = dfm$Mean_pH,                
                                  StressorsAB_N            = dfm$Sample_Size_Temperature_pH,       
                                  StressorsAB_SD           = dfm$Standard_Deviation_Temperature_pH,
                                  StressorsAB_Mean         = dfm$Mean_Temperature_pH,
                                  Significance_Level       = 0.05)

head(dfm)
##   Control_N Control_SD Control_Mean StressorA_N StressorA_SD StressorA_Mean
## 1         4 0.02886751    0.9550000           4   0.07023769      0.7700000
## 2        10 0.08743887    0.8430000          10   0.11246728      0.8560000
## 3         4 0.02645751    0.9550000           4   0.10424331      0.7800000
## 4         6 0.04195235    0.8900000           6   0.07174027      0.8066667
## 5         4 0.09055385    0.8600000           4   0.09814955      0.7750000
## 6         6 0.04676181    0.9033333           6   0.11338724      0.7783333
##   StressorB_N StressorB_SD StressorB_Mean StressorsAB_N StressorsAB_SD
## 1           4   0.04041452      0.8950000             4     0.08770215
## 2          10   0.08753412      0.7880000            10     0.07397447
## 3           4   0.03201562      0.7675000             4     0.07762087
## 4           6   0.04578937      0.8683333             6     0.07563068
## 5           4   0.04966555      0.7300000             4     0.07365460
## 6           6   0.04578937      0.9016667             6     0.04037326
##   StressorsAB_Mean Interaction_Effect_Size Interaction_Variance
## 1           0.6725             -0.07050078          0.007070194
## 2           0.7750             -0.03193848          0.004947161
## 3           0.6575              0.04772371          0.008576382
## 4           0.6900             -0.13157314          0.004154372
## 5           0.7275              0.10063882          0.010501229
## 6           0.8450              0.08402847          0.004793997
##   Interaction_CI_Upper Interaction_CI_Lower     Null_Model
## 1           0.09430170          -0.23530326 Multiplicative
## 2           0.10591767          -0.16979462 Multiplicative
## 3           0.22923354          -0.13378613 Multiplicative
## 4          -0.00524479          -0.25790148 Multiplicative
## 5           0.30148712          -0.10020947 Multiplicative
## 6           0.21973382          -0.05167688 Multiplicative
#classifying interactions
dfm <- classify_interactions(effect_size_dataframe = dfm,
                             assign_reversals = TRUE,
                             remove_directionality = TRUE)

head(dfm)
##   Control_N Control_SD Control_Mean StressorA_N StressorA_SD StressorA_Mean
## 1         4 0.02886751    0.9550000           4   0.07023769      0.7700000
## 2        10 0.08743887    0.8430000          10   0.11246728      0.8560000
## 3         4 0.02645751    0.9550000           4   0.10424331      0.7800000
## 4         6 0.04195235    0.8900000           6   0.07174027      0.8066667
## 5         4 0.09055385    0.8600000           4   0.09814955      0.7750000
## 6         6 0.04676181    0.9033333           6   0.11338724      0.7783333
##   StressorB_N StressorB_SD StressorB_Mean StressorsAB_N StressorsAB_SD
## 1           4   0.04041452      0.8950000             4     0.08770215
## 2          10   0.08753412      0.7880000            10     0.07397447
## 3           4   0.03201562      0.7675000             4     0.07762087
## 4           6   0.04578937      0.8683333             6     0.07563068
## 5           4   0.04966555      0.7300000             4     0.07365460
## 6           6   0.04578937      0.9016667             6     0.04037326
##   StressorsAB_Mean Interaction_Effect_Size Interaction_Variance
## 1           0.6725              0.07050078          0.007070194
## 2           0.7750              0.03193848          0.004947161
## 3           0.6575             -0.04772371          0.008576382
## 4           0.6900              0.13157314          0.004154372
## 5           0.7275             -0.10063882          0.010501229
## 6           0.8450             -0.08402847          0.004793997
##   Interaction_CI_Upper Interaction_CI_Lower     Null_Model
## 1           0.23530326          -0.09430170 Multiplicative
## 2           0.16979462          -0.10591767 Multiplicative
## 3           0.13378613          -0.22923354 Multiplicative
## 4           0.25790148           0.00524479 Multiplicative
## 5           0.10020947          -0.30148712 Multiplicative
## 6           0.05167688          -0.21973382 Multiplicative
##   Interaction_Classification
## 1                       Null
## 2                       Null
## 3                       Null
## 4                Synergistic
## 5                       Null
## 6                       Null
#generate summary plots
dfm_plots <- summary_plots(effect_size_dataframe = dfm,
                           Significance_Level = 0.05)

dfm_plots


References
Burgess, B. J., Jackson, M. C., & Murrell, D. J. (2021a). Multiple stressor null models frequently fail to detect most interactions due to low statistical power. bioRxiv.

Burgess, B. J., Purves, D., Mace, G., & Murrell, D. J. (2021b). Classifying ecosystem stressor interactions: Theory highlights the data limitations of the additive null model and the difficulty in revealing ecological surprises. Global Change Biology.

Jackson, M. C., Loewen, C. J., Vinebrooke, R. D., & Chimimba, C. T. (2016). Net effects of multiple stressors in freshwater ecosystems: a meta-analysis. Global Change Biology, 22(1), 180-189.

Seifert, M., Rost, B., Trimborn, S., & Hauck, J. (2020). Meta-analysis of multiple driver effects on marine phytoplannkton highlights modulating role of pCO2. Global Change Biology, 26(12), 6787-6804.

Siviter, H., Bailes, E. J., Martin, C. D., Oliver, T. R., Koricheva, J., Leadbeater, E., & Brown, M. J. (2021). Agrochemicals interact synergistically to increase bee mortality. Nature, 596(7872), 389-392.

Links
benjburgess.github.io

multiplestressR
summary_plots Generate Summary Figures Description Using the output from classify_interactions function, summary figures can be created using this function. Usage summary_plots( effect_size_dataframe, Small_Sample_Correction, Significance_Level ) Arguments effect_size_dataframe Output from the classify_interactions function. Small_Sample_Correction Whether the correction for small sample sizes should be enacted (TRUE or FALSE;defaultis TRUE)Notethatifthemultiplicative null model (see effect_size_multiplicative) was implemented, this parameter is not used and can be ignored. If the additive null model (see effect_size_additive) was implemented, then this parameter should be assigned the same value as in that analysis. Significance_Level The value of alpha for which confidence intervals are calculated (numeric, between 0 and 1; default is 0.05) Note that if the multiplicative null model (see effect_size_multiplicative) was implemented, this parameter is not used and can be ignored. If the additive null model (see effect_size_additive) was implemented, then this parameter should be assigned the same value as in that analysis. Details The figures include: a) The proportions of the different interaction classifications from the dataset b) Median sample sizes plotted against effect size (different interaction classifications are highlighted). Where the additive null model was used in the analysis, lines for critical effect sizes are plotted (see critical_effect_size_additive function). c) Density of different median sample sizes. d) Inverse of effect size variance plotted against effect size (i.e., one iteration of a funnel plot). e) Effect size standard error (i.e., the square root of the effect size variance) plotted against effect size (i.e., one iteration of a funnel plot)). Note that c- e) are most useful for researchers conducting a meta-analysis.12 survival Value The function returns a series of figures each of which is outlined above. Examples #loading up an example dataset from the multiplestressR package df <- multiplestressR::survival #calculating effect sizes df <- effect_size_additive(Control_N Control_SD Control_Mean StressorA_N StressorA_SD StressorA_Mean StressorB_N StressorB_SD StressorB_Mean StressorsAB_N StressorsAB_SD = df$Sample_Size_Control, = df$Standard_Deviation_Control, = df$Mean_Control, = df$Sample_Size_Temperature, = df$Standard_Deviation_Temperature, = df$Mean_Temperature, = df$Sample_Size_pH, = df$Standard_Deviation_pH, = df$Mean_pH, = df$Sample_Size_Temperature_pH, = df$Standard_Deviation_Temperature_pH, StressorsAB_Mean = df$Mean_Temperature_pH, Significance_Level = 0.05); #classifying interactions df <- classify_interactions(effect_size_dataframe = df, assign_reversals = TRUE, remove_directionality = TRUE); #generate summary plots df_plots <- summary_plots(effect_size_dataframe = df, Significance_Level = 0.05) survival Survival data for 250 populations exposed to the stressors of temperature and pH. Description A generated dataset on the survival rates of 250 populations (each composed of 100 individuals) exposed to the stressors of temperature and pH. The dataset uses a factorial design comprising four treatments: i) Control conditions; ii) Exposed to temperature; iii) Exposed to pH; iv) Exposed to both temperature and pH. This generated dataset is used in the examples for each function in the multiplestressR package. Please note that this is a generated dataset and does not reflect data from actual ecological experiments. Usage survivalsurvival 13 Format Adata frame with 250 rows and 12 variables: Sample_Size_Control Number of samples for the control treatment Standard_Deviation_Control standard deviation for mean survival of the control treatment Mean_Control mean survival (as a proportion) for the control treatment Sample_Size_Temperature Number of samples for the temperature treatment Standard_Deviation_Temperature standard deviation for mean survival of the temperature treatment Mean_Temperature mean survival (as a proportion) for the temperature treatment Sample_Size_pH Number of samples for the pH treatment Standard_Deviation_pH standard deviation for mean survival of the pH treatment Mean_pH meansurvival (as a proportion) for the pH treatment Sample_Size_Temperature_pH Number of samples for the combined temperature and pH treatment Standard_Deviation_Temperature_pH standard deviation for mean survival of the combined temperature and pH treatment Mean_Temperature_pH mean survival (as a proportion) for the combined temperature and pH treatmentIndex ??? datasets survival, 12 classify_interactions, 2, 11 critical_effect_size_additive, 5, 11 effect_size_additive, 2, 5, 6, 11 effect_size_multiplicative, 2, 8, 11 summary_plots, 11 survival, 12 1

```


```{r Energetic Analysis, message=FALSE, warning =FALSE}

joule_summary_tbl <- joule.rates.dataset %>%
  select(pH_Treatment, Temp_Treatment, J.gram.hr) %>%
  pivot_wider(
    names_from = pH_Treatment,
    names_glue = "J_gram_hr_{pH_Treatment}",
    values_from = J.gram.hr,
    values_fn = list
  )


summary_table <- joule_summary_tbl %>%
 tbl_summary(
    type = all_continuous() ~ "continuous",
    statistic = list(all_continuous() ~ "{mean} ?? {sd}"),
    include = Temp_Treatment
  )
```


```{r Energetic Analysis, message=FALSE, warning =FALSE}
joule_summary_tbl <- joule.rates.dataset %>%
  select(pH_Treatment, Temp_Treatment, J.gram.hr) %>%
  pivot_longer(
    names_to = "Variable",
    values_to = "Value"
  )

joule_summary_tbl <- joule.rates.dataset %>%
  select(pH_Treatment, Temp_Treatment, J.gram.hr) %>%
  pivot_longer(cols = -c(Temp_Treatment, pH_Treatment), names_to = "variable",
values_to = "value",


joule_summary_tbl  <- joule.rates.dataset %>%
  select(pH_Treatment, Temp_Treatment, J.gram.hr) %>%
  pivot_wider(names_from = c(Temp_Treatment, pH_Treatment), values_from = J.gram.hr) %>%
  tbl_summary(
  value = J.gram.hr,
  type = J.gram.hr ~ "continuous2",
  statistic =  J.gram.hr ~ "{mean} ?? {se}",
  by = pH_Treatment,
  include = Temp_Treatment
)


```


```{r Energetic Analysis, message=FALSE, warning =FALSE}
mean_joule_rates <- joule.rates.dataset %>% 
  group_by(Temp_Treatment, pH_Treatment) %>%
  summarise(Mean_J_gram_hr = mean(J.gram.hr), .groups = "drop") 
  

# Calculating the percent differences (your code seems correct here)
percent_difference <- joule.rates.dataset %>%
  group_by(Temp_Treatment, pH_Treatment) %>%
  summarise(mean_J_gram_hr = mean(J.gram.hr), .groups = "drop") %>% 
  group_by(Temp_Treatment) %>%
  summarise(
    Percent_Difference = (mean_J_gram_hr[pH_Treatment == "Low"] -
                            mean_J_gram_hr[pH_Treatment == "Ambient"]) / 
                         mean_J_gram_hr[pH_Treatment == "Ambient"] * 100,
    .groups = 'drop'  # Ensures the grouping is dropped after summarisation
  ) %>% ungroup()

joule.rates.dataset <- joule.rates.dataset %>%
  left_join(percent_difference, by = "Temp_Treatment") %>% 
  na.omit(J.gram.hr)

# Perform a two-way ANOVA
Jhr_model <- lm(log(J.gram.hr) ~ Temp_Treatment * pH_Treatment, data = joule.rates.dataset)
anova_result <- anova(Jhr_model)
#check_model(Jhr_model) # interactive effect is significant thus we dont need to check main effects
# Note: check_model function is typically part of diagnostic steps; ensure it's correctly defined or available in your context

# nested model 
#nested_Jhr_model <- lm(log(J.gram.hr) ~ Temp_Treatment / pH_Treatment, data = joule.rates.dataset)
#nested_anova_result <- anova(nested_Jhr_model)
#nested_tukey_result <- TukeyHSD(aov(nested_Jhr_model))

#nested_Jhr_model2 <- lm(log(J.gram.hr) ~ pH_Treatment / Temp_Treatment, data = joule.rates.dataset)
#nested_anova_result2 <- anova(nested_Jhr_model2)
#nested_tukey_result2 <- TukeyHSD(aov(nested_Jhr_model2))

#nested_Jhr_model3 <- lm(log(J.gram.hr) ~ pH_Treatment : Temp_Treatment, data = joule.rates.dataset)
#nested_anova_result3 <- anova(nested_Jhr_model3)
#nested_tukey_result3 <- TukeyHSD(aov(nested_Jhr_model3))


#nested_Jhr_model4 <- lm(log(J.gram.hr) ~ pH_Treatment + pH_Treatment %in% Temp_Treatment, data = joule.rates.dataset)
#nested_anova_result4 <- anova(nested_Jhr_model4)
#nested_tukey_result4 <- TukeyHSD(aov(nested_Jhr_model4))


#lmer_Jhr_model <- lmer(J.gram.hr ~ pH_Treatment*Temp_Treatment + (1|Temp_Treatment), data = joule.rates.dataset)
#anova_lmer <- anova(lmer_Jhr_model)

# Inspect the model diagnostic metrics
model.metrics <- augment(Jhr_model) %>%
  dplyr::select(-.hat, -.sigma, -.fitted) # Remove details
head(model.metrics, 3)

#Check for outliers # three outliers none extreme
#joule.rates.dataset %>%
    #group_by(Temp_Treatment, pH_Treatment) %>%
    #identify_outliers(J.gram.hr)

# Assess normality of residuals using shapiro wilk test
energy.shapiro.test <- shapiro_test(model.metrics$.resid)
# homogeneity of variance (not sinificant)
energy.levene.test <- model.metrics %>% levene_test(.resid ~ Temp_Treatment*pH_Treatment)

#significance <- joule.rates.dataset %>%
# group_by(Temp_Treatment) %>%
# emmeans_test(J.gram.hr ~ pH_Treatment, p.adjust.method = "tukey")

# Perform ANOVA
anova_results <- anova(lm(J.gram.hr ~ Temp_Treatment*pH_Treatment, data = joule.rates.dataset))

# Print ANOVA results
summary(print(anova_results))

significance <- joule.rates.dataset %>%
 group_by(Temp_Treatment) %>%
 emmeans_test(J.gram.hr ~ pH_Treatment, p.adjust.method = "bonferroni", detailed=TRUE)

# Create the bar plot
bar_plot_with_significance <- ggplot(significance, aes(x = Temp_Treatment, y = estimate, fill = factor(sign(estimate)))) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2, position = position_dodge(width = 0.7)) +
  geom_text(data = significance, aes(x = Temp_Treatment, y = conf.low, label = if_else(p.adj.signif == "**", p.adj.signif, "")), vjust = 1, color = "black", size = 4) +
  scale_fill_manual(values = c("red", "yellow"), guide = FALSE) +
  labs(x = "Temp Treatment", y = "Difference Estimate") +
  theme_minimal()

# Print the plot
print(bar_plot_with_significance)


print(anova_result)
summary(anova_result)

# Tidy up the ANOVA result into a data frame
anova_df <- tidy(anova_result)

# Convert and format the 'p.value' column to scientific notation
anova_df <- anova_df %>%
  format_p_values() 


anova_df <- anova_df %>% 
  mutate(
    term = gsub("_", " ", term),
    term = gsub(":", " ?? ", term),
    term = tools::toTitleCase(term),  # Capitalize terms
    # Round statistical metrics to two significant figures
    sumsq = round(sumsq, digits = 2),
    meansq = round(meansq, digits = 2),
    statistic = round(statistic, digits = 2)
  ) 

anova_df <- anova_df %>%
  mutate(across(everything(), ~ ifelse(is.na(.), "", .))) %>%
  dplyr::select(term, p.value, statistic, sumsq, meansq, df) 

#print(anova_df)

# Perform a Tukey's HSD test
tukey_result <- TukeyHSD(aov(Jhr_model))

tukey_df <- broom::tidy(tukey_result) 

# Filter Tukey results for "Low" vs "Ambient"
tukey_filtered <- tukey_df %>%
    mutate(
        estimate = round(estimate, digits=2),
        conf.low = round(conf.low, digits=2),
        conf.high = round(conf.high, digits=2),
        p.value = as.numeric(adj.p.value)
    ) %>%
  #sepearting terms into filterable conditions toselect for only relevant comparisons 
  filter(term == "Temp_Treatment:pH_Treatment") %>% 
  format_p_values() %>%
    separate(contrast, into = c("pH_Treatment_Left", "pH_Treatment_Right"), sep = "-") %>%
    separate(pH_Treatment_Left, into = c("Temp_Left", "pH_Left"), sep = ":") %>%
    separate(pH_Treatment_Right, into = c("Temp_Right", "pH_Right"), sep = ":") %>% 
  # filtering for comparisons between temp treatments 
  filter(Temp_Left == Temp_Right) %>% 
  mutate(
    contrast = paste0(Temp_Left, "°C ", pH_Left, " ?? ",
                      Temp_Right, "°C ", pH_Right),
    term = gsub("_", " ", term),
    term = gsub(":", " ?? ", term),
    confidence.interval = paste0("(",conf.low, ", ", conf.high, ")")) %>%
  dplyr::select(contrast, estimate, confidence.interval, adj.p.value=p.value) 



print(tukey_filtered)

# Create a table for the ANOVA results
anova_table <- anova_df %>%
  gt() %>%
  tab_header(
    title = md("**Two-Way ANOVA Results for Energetic Expenditure<br/> (log(J g<sup>-1</sup> hr<sup>-1</sup>)) in *Tegula funebralis***"),
     subtitle = md("Interactive Effects Between Temperature & pH Treatments")
  ) %>%
  cols_label(
    term = "Term",
    p.value = "P-Value",
    statistic = "F-Statistic",
    sumsq = "Sum of Squares",
    meansq = "Mean Squares",
    df = "Degrees of Freedom"
  ) %>%
  gt_theme 


# Create a table for the Tukey's HSD results
tukey_table <- tukey_filtered %>%
  gt() %>%
  tab_header(
    title = md("**Tukey's HSD Post Hoc Test Results for Energetic Expenditure<br/> (log(J g<sup>-1</sup> hr<sup>-1</sup>)) in *Tegula funebralis***"),
    subtitle = md("Selected Comparisons Between Low vs. Ambient pH Treatment")
  ) %>%
  cols_label(
    contrast = "Comparison",
    estimate = "Difference",
    confidence.interval = "Confidence Interval",
    adj.p.value = "Adjusted P-Value"
  ) %>% gt_theme

# Print the resulting data frames
print(anova_table)
print(tukey_table)

# Save the tables as PNG images
gtsave(anova_table, here("Figures", "gt_table_anova_energy_expenditure.png"))

gtsave(tukey_table, here("Figures", "gt_table_tukey_energy_expenditure.png"))
```


```{r, message= FALSE}

# joule rate summary statistics 
joule.rates.summary <- joule.rates.dataset %>%
  filter(pH_Treatment != "Sump") %>%
  group_by(Temp_Treatment, pH_Treatment) %>%
  summarize(
    Mean_joule_sec = mean(J.gram.hr),
    Median_joule_sec = median(J.gram.hr),
    Min_joule_sec = min(J.gram.hr),
    Max_joule_sec = max(J.gram.hr),
    SD_joule_sec = sd(J.gram.hr),
    SE_joule_sec = sd(J.gram.hr) / sqrt(n()),
    N_joule_sec = n()
  ) 


# Pivot the data to have Temp_Treatment as rows and pH_Treatment as columns
pivot_mean_joule_sec <- pivot_wider(mean_joule_sec, names_from = pH_Treatment, values_from = Mean_joule_sec)

# Calculate the percent difference
difference_mean_joule_hr <- pivot_mean_joule_sec %>%
  group_by(Temp_Treatment) %>%
  mutate(
    Percent_Difference = (Ambient - Low) / Ambient*100,
    Percent_Difference = round(Percent_Difference, digits=2),
    Ambient = round(Ambient, digits=2),
    Low = round(Low, digits=2)
  ) %>%
  ungroup()


# Assuming difference_mean_joule_table is already defined in your environment
tbl_joule <- gt(difference_mean_joule_hr) %>%
  tab_header(
    title = md("**Energetic Expenditure (Joule g<sup>-1</sup> hr<sup>-1</sup>) in *Tegula funebralis* Statistical Summary**"),
    subtitle = md("Comparative Analysis by Temperature Treatment")
  ) %>%
  cols_label(
    Temp_Treatment = "Temperature Treatment",
    Ambient = "Ambient Energy Expenditure (J/h)",
    Low = "Low Energy Expenditure (J/h)",
    Percent_Difference = "Percent Difference (%)"
  )  %>%
  cols_add(dir = ifelse(Percent_Difference < 0, "red3", "#ffeda0")) %>% 
   gt_theme %>% 
  text_case_match(
    "red3" ~ fontawesome::fa("arrow-up"),
    "#ffeda0" ~ fontawesome::fa("arrow-down")
  ) %>% 
  tab_style(
    style = cell_text(color = from_column("dir")),
    locations = cells_body(columns = dir)
  ) %>% 
  cols_label(
    dir = "Expenditure"
  ) %>% 
  gt::tab_style(
    style = gt::cell_fill(color = "cyan3", alpha=0.5),
    locations = cells_body(columns = Low)
  ) %>% 
  gt::tab_style(
    style = gt::cell_fill(color = "orange", alpha=0.5),
    locations = cells_body(columns = Ambient)
  )


print(tbl_joule)
gt::gtsave(tbl_joule, here("Figures", "gt_table_energy_expenditure.png"))


```


```{r, warning=FALSE, message=FALSE}
# Ensure Temp_Treatment is a factor ordered from 12 to 26
difference_mean_joule_hr$Temp_Treatment <- factor(difference_mean_joule_hr$Temp_Treatment, levels = 12:26)

difference_mean_joule_hr <- difference_mean_joule_hr %>% 
  mutate(fill=ifelse(Percent_Difference >= 0, "#ffeda0", "red3"))


# Create the circular bar plot
p <- ggplot(difference_mean_joule_hr, aes(x = Temp_Treatment, y = Percent_Difference),
            fill=fill) +
  geom_hline(yintercept = 0, color = "grey25", linetype="dashed", size = 0.5) +
  geom_hline(yintercept = 45, color = "grey25", linetype="dashed", size = 0.25) +
  geom_segment(data = data.frame(x = unique(difference_mean_joule_hr$Temp_Treatment)), 
               aes(x = x, y = 45, xend = x, yend = -75), color = "grey25", size = 0.25) +
  geom_bar(stat = "identity", aes(fill = fill), width = 0.85) +
  geom_text(aes(label = paste0(round(Percent_Difference, 1), "%"),
                y = if_else(Percent_Difference >= 0,
                            Percent_Difference + 1, Percent_Difference - 1),
                color = if_else(Percent_Difference >= 0, "grey25", "white")), 
            position = position_stack(vjust = 0.5), size = 2.5) +
  scale_fill_identity()+
  coord_polar(start = -pi/2.04) +
  scale_y_continuous(breaks = seq(-75, 45, by=15), expand=c(0,1)) +
  scale_color_identity() +
  common_theme+
  theme(
    plot.margin = margin(t = 1, r = 0, b = -9, l = 0),
    panel.background = element_rect(fill = "white", color="white"),
    panel.border = element_blank(),
    # Remove axis ticks and text
    axis.title = element_blank(),
    axis.ticks = element_blank(),
    axis.text.y = element_blank(),
    axis.text.x = element_text(margin=margin(b=-10)),
    axis.title.y=element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.major.y= element_line(color="grey90", linewidth=0.2),
    # Use gray text for the region names
    axis.text = element_text(color = "gray12", size = 12, family="serif", margin=margin(t=-10)),
    # Move the legend to the bottom
    legend.position = "bottom") +
  labs(title="Temperature Treatment")+
  # Annotate custom scale inside plot
  annotate(
    x = 17, 
    y = 10, 
    label = NA, 
    geom = "text", 
    color = "gray12", 
    family = "serif", size=2, hjust = 0.6, vjust = -0.4
  ) + annotate("text", x = rep(0, 9), y = seq(-75, 45, by=15), 
           label = paste0(seq(-75, 45, by=15), "%"),  # Append percentage sign
             color = "grey25", size = 3, angle = 0,
           family="serif", hjust = 1.2) 

# Print the plot
#print(p)

ggsave(here("Figures", "circular_bar_plot_energy_expenditure.png"), p, width = 8,height =4.5, dpi = 1300)
```


```{r, warning=FALSE, message=FALSE}
energetic_bar_plot <- ggplot(difference_mean_joule_hr, aes(x = Temp_Treatment, y = Percent_Difference)) +
ggchicklet::geom_chicklet(aes(fill = ifelse(Percent_Difference >= 0, "#ffeda0", "red3")),
                color = NA,
                    radius = grid::unit(1, 'mm'), linewidth=0.15) +
  labs(
    x = NULL,
    y = "Energetic Expenditure Difference (%)"
  ) +
  scale_fill_identity() +  # Use the colors specified in the data
  common_theme + 
  scale_y_continuous(breaks = seq(-60, 40, by = 10)) +
  ylim(limits = c(-60, 40)) +  # Set the y-axis limits and breaks
  theme(
  panel.grid.major.x = element_blank(),  # Removes major x gridlines
  panel.grid.minor.x = element_blank(),   # Removes minor x gridlines, if any
  plot.margin = margin(t = 30, r = 10, b = 30, l = 7.5, unit = "pt"),
  x.axis.label = element_text(size=16),
  y.axis.label = element_text(size=16),
  x.axis.text = element_text(size=14),
  y.axis.text = element_text(size=14)
)

#print(energetic_bar_plot)

# Define color palette for pH levels
palette <- c("orange", "cyan3")
palette2 <- c("darkorange3", "cyan4")

mean_joule_sec <- joule.rates.summary %>%
  group_by(Temp_Treatment,pH_Treatment) %>%
  dplyr::select(Temp_Treatment, pH_Treatment, Mean_joule_sec) 

boxplot <- ggplot(joule.rates.dataset, aes(x = Temp_Treatment, y = J.gram.hr,
                                           color=pH_Treatment, fill = pH_Treatment
                                           )) +
 # geom_point(aes(color=pH_Treatment, group=pH_Treatment), position = position_dodge(0.8)) +
  geom_line(aes(group=pH_Treatment), stat = "summary", alpha=0.7,
            fun = mean, linetype = "dashed", position = position_dodge(0.8), linewidth=0.3) +
  stat_boxplot(geom = "errorbar", width = 0.4,
                position = position_dodge(0.8), linewidth=0.3)+
  geom_boxplot(outlier.shape = NA, position = position_dodge(0.8), linewidth=0.2,
               width=0.7) + 
  geom_point(data=mean_joule_sec, aes(x = Temp_Treatment, y=Mean_joule_sec,
                                      color=pH_Treatment, group=pH_Treatment),
             position = position_dodge(0.8), size=1) +
  scale_color_manual(values = palette2) +
  scale_fill_manual(values = palette) +
  coord_trans(y = "log") +
  #scale_y_continuous(breaks = seq(1, 4, by = 1), expand = c(0, 0)) +
  labs(x = NULL, 
       y = TeX("$Energetic \\ Expenditure\\ J\\ g^{-1}\\ hr^{-1}$"),
       title = NULL) +
  common_theme +
  theme(
  plot.margin = margin(t = 30, r = 0, b = 30, l = 7.5, unit = "pt"),  # Adjust margin
  x.axis.label = element_text(size=16),
  y.axis.label = element_text(size=16),
  x.axis.text = element_text(size=14),
  y.axis.text = element_text(size=14)
)

#print(boxplot)

combined_energetics <- cowplot::plot_grid(boxplot, energetic_bar_plot, labels = c("(A)", "(B)"),
  label_size = 14,  # Adjust label size
  label_fontfamily = "serif",  # Set label font family to serif
  align = 'v',  # Ensure vertical alignment if needed
  axis = 'lr',  # Reduce the gap by aligning top and bottom
  hjust = -0.5, 
  nrow = 1, rel_widths = c(4.5, 2.5))

# Add a common x-axis label using ggdraw() and draw_label()
final_plot <- ggdraw() +
  draw_plot(combined_energetics) +
  draw_label("Temperature Treatment", x = 0.5, y = 0.04, hjust = 0.5, fontfamily = "serif", size=12) 

# Print the final plot
print(final_plot)

ggsave(final_plot, filename = here("Figures", "combined_energetics_plot.png"), height = 4, width = 7, dpi = 1300)


```

# Thermal Performance Curve Analysis 


```{r, Data Preparation}


respo.rates.dataset <- read_csv(here::here("Data", "Thinned_Respirometry_Data", "Respiration.Rates.Dataset.csv"), show_col_types = FALSE) 


ylab = expression(mu*mol~O[2]~g^-1~hr^-1)
xlab = expression(Temperature~plain("(°C)"))

#load in the data set and select for temperature, rate, and pH treatment
respo.rates.tpc <- respo.rates.dataset %>% 
  #mutate(rate=umolO2.hr/Final_Blotted_Wet_Mass_g, temp=Temp.C) %>%
  mutate(rate=umolO2.gram.hr, temp=Temp.C) %>%
  dplyr::select(Temp_Treatment, pH_Treatment, temp, rate) %>% 
  na.omit(umolO2.gram.hr)

# Create a single ggplot graph with both curves
respo.rates.plot <- ggplot(respo.rates.tpc, aes(Temp_Treatment, rate, color=pH_Treatment)) +
  geom_point() +
  theme_bw() +
  common_theme +
  labs(
    x = xlab,
    y = ylab,
    title = 'Respiration Rates Across Temperatures'
  ) +
  scale_x_continuous(breaks = seq(12, 26, by = 2)) +
  scale_color_manual(values = c("Low" = "cyan3", "Ambient" = "orange"))

print(respo.rates.plot)

```




##  Model Selection and Comparison

```{r Model Selection and Comaparison, warning =FALSE, message=FALSE}

multi.respo.rates.tpc <- respo.rates.tpc %>% 
  dplyr::select(pH_Treatment, temp, rate) %>% 
  mutate(pH_Treatment = factor(pH_Treatment, levels = c("Low", "Ambient"))) %>% 
  group_by(pH_Treatment) %>% 
  na.omit(rate)

# Load in data and filter to keep just a single curve (low pH)
low.pH <- filter(respo.rates.tpc, pH_Treatment == 'Low')

# Load in data and filter to keep just a single curve (high pH)
high.pH <- filter(respo.rates.tpc, pH_Treatment == 'Ambient')

# fit every model formulation in rTPC
model_selection_fits <- nest(multi.respo.rates.tpc, data = c(temp, rate)) %>% 
   mutate(beta = purrr::map(data, ~nls_multstart(rate~beta_2012(temp = temp, a, b, c, d, e),
                        data = .x,
                        iter = c(5,5,5,5,5),
                        start_lower = rTPC::get_start_vals(.x$temp, .x$rate, model_name = 'beta_2012') - 10,
                        start_upper = rTPC::get_start_vals(.x$temp, .x$rate, model_name = 'beta_2012') + 10,
                        lower = rTPC::get_lower_lims(.x$temp, .x$rate, model_name = 'beta_2012'),
                        upper = rTPC::get_upper_lims(.x$temp, .x$rate, model_name = 'beta_2012'),
                        supp_errors = 'Y',
                        convergence_count = FALSE)),
          boatman = purrr::map(data, ~nls_multstart(rate~boatman_2017(temp = temp, rmax, tmin, tmax, a,b),
                        data = .x,
                        iter = c(4,4,4,4,4),
                        start_lower = rTPC::get_start_vals(.x$temp, .x$rate, model_name = 'boatman_2017') - 10,
                        start_upper = rTPC::get_start_vals(.x$temp, .x$rate, model_name = 'boatman_2017') + 10,
                        lower = rTPC::get_lower_lims(.x$temp, .x$rate, model_name = 'boatman_2017'),
                        upper = rTPC::get_upper_lims(.x$temp, .x$rate, model_name = 'boatman_2017'),
                        supp_errors = 'Y',
                        convergence_count = FALSE)),
         gaussian = purrr::map(data, ~nls_multstart(rate~gaussian_1987(temp = temp, rmax, topt, a),
                        data = .x,
                        iter = c(4,4,4),
                        start_lower = rTPC::get_start_vals(.x$temp, .x$rate, model_name = 'gaussian_1987') - 10,
                        start_upper = rTPC::get_start_vals(.x$temp, .x$rate, model_name = 'gaussian_1987') + 10,
                        lower = rTPC::get_lower_lims(.x$temp, .x$rate, model_name = 'gaussian_1987'),
                        upper = rTPC::get_upper_lims(.x$temp, .x$rate, model_name = 'gaussian_1987'),
                        supp_errors = 'Y',
                        convergence_count = FALSE)),
         sharpeschoolfull = purrr::map(data, ~nls_multstart(rate~sharpeschoolfull_1981(temp = temp, r_tref,e,el,tl,eh,th, tref = 15),
                        data = .x,
                        iter = c(4,4,4,4,4,4),
                        start_lower = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolfull_1981') - 10,
                        start_upper = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolfull_1981') + 10,
                        lower = get_lower_lims(.x$temp, .x$rate, model_name = 'sharpeschoolfull_1981'),
                        upper = get_upper_lims(.x$temp, .x$rate, model_name = 'sharpeschoolfull_1981'),
                        supp_errors = 'Y',
                        convergence_count = FALSE)),
         sharpeschoolhigh = purrr::map(data, ~nls_multstart(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 15),
                        data = .x,
                        iter = c(4,4,4,4),
                        start_lower = rTPC::get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') - 10,
                        start_upper = rTPC::get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') + 10,
                        lower = rTPC::get_lower_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),
                        upper = rTPC::get_upper_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),
                        supp_errors = 'Y',
                        convergence_count = FALSE)),
         weibull = purrr::map(data, ~nls_multstart(rate~weibull_1995(temp = temp, a,topt,b,c),
                        data = .x,
                        iter = c(4,4,4,4),
                        start_lower = rTPC::get_start_vals(.x$temp, .x$rate, model_name = 'weibull_1995') - 10,
                        start_upper = rTPC::get_start_vals(.x$temp, .x$rate, model_name = 'weibull_1995') + 10,
                        lower = rTPC::get_lower_lims(.x$temp, .x$rate, model_name = 'weibull_1995'),
                        upper = rTPC::get_upper_lims(.x$temp, .x$rate, model_name = 'weibull_1995'),
                        supp_errors = 'Y',
                        convergence_count = FALSE)))

summary(glimpse(dplyr::select(model_selection_fits, 1:8)))

# stack models
model_stack <- dplyr::select(model_selection_fits, -data) %>%
  pivot_longer(., names_to = 'model_name', values_to = 'fit', boatman:weibull)

# get parameters using tidy
params <- model_stack %>%
  mutate(., est = purrr::map(fit, tidy)) %>%
  dplyr::select(-fit) %>%
  unnest(est)

# get predictions using augment
newdata <- tibble(temp = seq(min(respo.rates.tpc$temp), max(respo.rates.tpc$temp), length.out = 100))
model_preds <- model_stack %>%
  mutate(., preds = purrr::map(fit, augment, newdata = newdata)) %>%
  dplyr::select(-fit) %>%
  unnest(preds)

# seperating the curves for graphing seperate
high_pH <- filter(respo.rates.tpc, pH_Treatment == "Ambient") 
high_pH_model_preds <- filter(model_preds, pH_Treatment == "Ambient")
low_pH <- filter(respo.rates.tpc, pH_Treatment == "Low") 
low_pH_model_preds <- filter(model_preds, pH_Treatment == "Low")

# take a random point from each model for labelling
high_pH_model_labs <- filter(high_pH_model_preds, temp < 24) %>%
  group_by(., model_name) %>%
  sample_n(., 1) %>%
  ungroup()

# take a random point from each model for labelling
low_pH_model_labs <- filter(low_pH_model_preds, temp < 24) %>%
  group_by(., model_name) %>%
  sample_n(., 1) %>%
  ungroup()


# plot
wrapped.model.fits <- ggplot(model_preds, aes(temp, rate, color = pH_Treatment)) +
  geom_point(aes(Temp_Treatment, rate), respo.rates.tpc) +
  geom_line(aes(temp, .fitted)) +
  facet_wrap(~model_name, scales = 'free', ncol = 5) +
  common_theme +
  scale_x_continuous(breaks = seq(12, 26, by = 2)) +
  scale_color_manual(values = c("Low" = "cyan3", "Ambient" = "orange"), 
                     name = "Treatment") +
  labs(x = xlab,
       y = ylab,
       title = 'Thermal Performance Curve Model Fits') +
  geom_hline(aes(yintercept = 0), linetype = 2)

# multiple models low pH plot
low_pH_model_preds_plot <- ggplot(low_pH_model_preds, aes(temp, .fitted)) +
  geom_point(aes(temp, rate), low_pH) +
  geom_line(aes(col = model_name)) +
  geom_label_repel(aes(temp, .fitted, label = model_name, col = model_name), fill = 'white', nudge_x=-1, nudge_y = 15, segment.size = 0.5, size=2.5, family = "serif",
                   segment.colour = 'grey25', low_pH_model_labs) +
  common_theme +
  theme(plot.subtitle = element_text(hjust = 0.5),
        plot.margin = margin(t = 10, r = 10, b = 10, l = 10, unit = "pt")) +
  scale_x_continuous(breaks = seq(12, 26, by = 2)) +
  labs(x = xlab,
       y = ylab,
       title = 'Thermal Performance Curve Model Fits',
       subtitle='Low pH') +
  geom_hline(aes(yintercept = 0), linetype = 2) +
  scale_color_brewer(type = 'qual', palette = "Set1")

# multiple models high pH plot
high_pH_model_preds_plot <- ggplot(high_pH_model_preds, aes(temp, .fitted)) +
  geom_point(aes(temp, rate), high_pH) +
  geom_line(aes(col = model_name)) +
  geom_label_repel(aes(temp, .fitted, label = model_name, col = model_name), fill = 'white', nudge_x=-1, nudge_y = 15, segment.size = 0.5, size=2.5, family = "serif",
                   segment.colour = 'grey25', high_pH_model_labs) +
  common_theme +
  theme(plot.subtitle = element_text(hjust = 0.5),
        plot.margin = margin(t = 0, r = 10, b = 10, l = 10, unit = "pt")
        ) +
  scale_x_continuous(breaks = seq(12, 26, by = 2)) +
  labs(x = xlab,
       y = ylab,
       title =  NULL,
       subtitle='Ambient pH') +
  geom_hline(aes(yintercept = 0), linetype = 2) +
  scale_color_brewer(type = 'qual', palette = "Set1")

# Combine plots vertically
combined_model_fits <- low_pH_model_preds_plot / high_pH_model_preds_plot
combined_model_fits
wrapped.model.fits

ggsave(here::here("Figures", "multi_combined_model_fits.png"), combined_model_fits, width = 5, height = 6)
ggsave(here::here("Figures", "wrapped_model_fits.png"), wrapped.model.fits, width = 10, height = 4)

# Calculate AICc and create model_aic
model_aic <- model_stack %>%
  mutate(info = purrr::map(fit, glance),
         AICc =  purrr::map_dbl(fit, MuMIn::AICc)) %>%
  unnest(info) %>%
  dplyr::select(Treatment = pH_Treatment, model_name, sigma, AIC, AICc, BIC, df.residual)

# Calculate Delta AIC for each treatment
# Calculate Delta AIC for each treatment
model_aic <- model_aic %>%
  group_by(Treatment) %>%
  mutate(Delta_AICc = AICc - min(AICc),
  DF = df.residual) %>%
  ungroup()

# Title case the model names
model_aic$model_name <- tools::toTitleCase(model_aic$model_name)
model_aic$model_name <- gsub("Sharpeschoolfull", "Sharpe-Schoolfield (Full)", model_aic$model_name)
model_aic$model_name <- gsub("Sharpeschoolhigh", "Sharpe-Schoolfield (High)", model_aic$model_name)

# Write AIC values to CSV file
write.csv(model_aic, "model_aics.csv", row.names = FALSE)

# Find the best model across both pH_Treatment levels
best_model <- model_aic %>%
  group_by(Treatment) %>% 
  filter(AICc == min(AICc)) %>%
  pull(model_name)

# Print the best model
print(best_model)

# Find the best model for low pH
low_pH_best_model <- model_aic %>%
  filter(Treatment == "Low") %>% 
  filter(AICc == min(AICc)) %>%
  pull(model_name) 

# Find the best model for high pH
high_pH_best_model <- model_aic %>%
  filter(Treatment == "Ambient") %>% 
  filter(AICc == min(AICc)) %>%
  pull(model_name) 

# Print the best models
print(paste("Best Low pH Model:", low_pH_best_model))
print(paste("Best High pH Model:", high_pH_best_model))



model_aic <- model_aic %>%
  mutate(across(where(is.numeric), round, 2)) %>% 
  mutate(pH_Treatment=Treatment) %>% 
  mutate(pH_Treatment=ifelse(pH_Treatment=="Low", "Low pH Treatment", "Ambient pH Treatment"))

tbl_aic <- model_aic %>% 
  dplyr::select(-Treatment)



# Display model_aic as a table with highlighted rows for best models
tbl.aic <- tbl_aic %>%
  gt(rowname_col = "model_name", groupname_col = "pH_Treatment") %>%
  tab_header(title = md("**Thermal Performance Model Fit Statistical Summaries**")) %>%
  gt_highlight_rows(rows = which(model_aic$model_name %in%
                                 low_pH_best_model & model_aic$pH_Treatment == "Low pH Treatment"),
                                 font_weight = "normal", fill="cyan3", alpha=0.5) %>%
  gt_highlight_rows(rows = which(model_aic$model_name %in%
                                 high_pH_best_model & model_aic$pH_Treatment == "Ambient pH Treatment"),
                                 font_weight = "normal", fill="orange", alpha=0.5) %>%
  cols_label(
    model_name = "Model",
    AIC = "AIC",
    BIC = "BIC",
    AICc = "AICc",
    Delta_AICc = "ΔAICc",
    BIC = "BICc",
    sigma = "Sigma",
    df.residual = "DF"
  ) %>% gt_theme %>% 
  gt::tab_options(table.width = px(300),
                    table.align = "center",
                  table.border.top.width = px(1), 
                  table.border.bottom.width = px(1),
                  column_labels.border.top.color = "grey25",
                   heading.border.bottom.color =  "grey25",
                  row_group.font.weight = "bold",
                  row_group.border.bottom.color = "grey25",
                  row_group.border.top.color = "grey25",
                  row_group.border.top.width = px(1.5),
                  row_group.border.bottom.width = px(1.5),
                  table_body.vlines.color ="grey25",
                  stub.border.color = "grey25",
                  stub.border.width = px(1.5),
                  stub_row_group.border.color ="grey25",
                  stub_row_group.border.width = px(5),
                  stub.indent_length = px(5)
                  )
  


print(tbl.aic)

# Save the table as a PNG image
gtsave(tbl.aic, here::here("Figures", "model_fits_aic.png"))

# Title case the model names for plotting
low_pH_model_preds$model_name <- tools::toTitleCase(low_pH_model_preds$model_name)
low_pH_model_preds$model_name <- gsub("Sharpeschoolfull", "Sharpe-Schoolfield (Full)", low_pH_model_preds$model_name)
low_pH_model_preds$model_name <- gsub("Sharpeschoolhigh", "Sharpe-Schoolfield (High)", low_pH_model_preds$model_name)
high_pH_model_preds$model_name <- tools::toTitleCase(high_pH_model_preds$model_name)
high_pH_model_preds$model_name <- gsub("Sharpeschoolfull", "Sharpe-Schoolfield (Full)", high_pH_model_preds$model_name)
high_pH_model_preds$model_name <- gsub("Sharpeschoolhigh", "Sharpe-Schoolfield (High)", high_pH_model_preds$model_name)

# Plot for low pH
low_pH_model_preds_plot <- ggplot(low_pH_model_preds, aes(temp, .fitted)) +
  geom_point(aes(temp, rate), data = low_pH, size=0.75) +  # Use data = low_pH
  geom_line(aes(group = model_name), col = 'grey50', alpha = 0.5) +
  geom_line(data = filter(low_pH_model_preds, model_name == low_pH_best_model), col = "cyan3") +
  geom_label_repel(aes(temp, .fitted, label = model_name), fill = 'white', segment.size = 0.5,
                   segment.colour = 'grey25', nudge_x=-1, nudge_y = 15, size=2.5,
                   family = "serif", 
                   data = filter(low_pH_model_labs, model_name == low_pH_best_model),
                   col = "cyan3") +
  common_theme +
    theme(plot.subtitle = element_text(hjust = 0.5),
        plot.margin = margin(t = 10, r = 20, b = 0, l = 10, unit = "pt")
        ) +
  scale_x_continuous(breaks = seq(12, 26, by = 2)) +
  labs(x = xlab,
       y = ylab,
       title = NULL, 
       subtitle='Low pH Treatment') +
  geom_hline(aes(yintercept = 0), linetype = 2)

# Plot for high pH
high_pH_model_preds_plot <- ggplot(high_pH_model_preds, aes(temp, .fitted)) +
  geom_point(aes(temp, rate), data = high_pH, size=0.75) +  # Use data = high_pH
  geom_line(aes(group = model_name), col = 'grey50', alpha = 1) +
  geom_line(data = filter(high_pH_model_preds, model_name == high_pH_best_model), col = "orange") +
  geom_label_repel(aes(temp, .fitted, label = model_name), fill = 'white', segment.size = 0.5,
                   segment.colour = 'grey25', nudge_x=-1, nudge_y = 15, size=2.5,
                   family = "serif",
                   data = filter(high_pH_model_labs, model_name == high_pH_best_model),
                   col = "orange") +
  common_theme +
    theme(plot.subtitle = element_text(hjust = 0.5),
        plot.margin = margin(t = 10, r = 20, b = 10, l = 10, unit = "pt")
        ) +
  theme(plot.subtitle = element_text(hjust = 0.5)) +
  scale_x_continuous(breaks = seq(12, 26, by = 2)) +
  labs(x = xlab,
       y = ylab,
       title =  NULL,
       subtitle='Ambient pH Treatment') +
  geom_hline(aes(yintercept = 0), linetype = 2) 

# Combine plots vertically
combined_model_fits <- low_pH_model_preds_plot / high_pH_model_preds_plot
combined_model_fits

# Save combined plots to "Figures" folder using here
ggsave(here::here("Figures", "combined_model_fits.png"), combined_model_fits, width = 4, height = 5)

utils::citation("rTPC")
utils::citation("MuMIn")
utils::citation("minpack.lm")
```

Choose the Sharpe Schoolfield model(high) because it had the lowest AIC values for 7.7 and 8.0 pH, collectively and was a biological model used for ectotherms that fit the curve correctly. 

# Fitting the Sharpe-Schoolfield Model to Respiration Rates Across Temperature

```{r, message=FALSE}
# Sharpe-Schoolfield Thermal Performance Curve


#load in the data set and select for temperature, rate, and pH treatment
respo.rates.tpc <- respo.rates.dataset %>% 
  mutate(rate=umolO2.gram.hr, temp=Temp.C) %>%
  dplyr::select(Temp_Treatment, pH_Treatment, temp, rate) 

# keeping just a single curve
low_pH <- filter(respo.rates.tpc, pH_Treatment == 'Low') %>% 
  dplyr::select(-Temp_Treatment)
high_pH <- filter(respo.rates.tpc, pH_Treatment == 'Ambient') %>% 
  dplyr::select(-Temp_Treatment)

# Fit the Sharpe-Schoolfield model for low pH
sharpe_schoolfield_model_low <- nest(low_pH, data = c(temp, rate)) %>%
  mutate(sharpeschoolhigh = purrr::map(data, ~nls_multstart(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 15),
                        data = .x,
                        iter = c(4,4,4,4),
                        start_lower = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') - 10,
                        start_upper = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') + 10,
                        lower = get_lower_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),
                        upper = get_upper_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),
                        supp_errors = 'Y',
                        convergence_count = FALSE)),
         # create new temperature data
         low_pH_new_data = purrr::map(data, ~tibble(temp = seq(min(.x$temp), max(.x$temp), length.out = 100))),
         # predict over that data,
         low_pH_preds =  purrr::map2(sharpeschoolhigh, low_pH_new_data, ~augment(.x, newdata = .y)))

# unnest predictions
low_pH_preds <- dplyr::select(sharpe_schoolfield_model_low, low_pH_preds)  %>% unnest(low_pH_preds) %>% 
  mutate(pH_Treatment="Low")

# Fit the Sharpe-Schoolfield model for high pH
sharpe_schoolfield_model_high <- nest(high_pH, data = c(temp, rate)) %>%
  mutate(sharpeschoolhigh = purrr::map(data, ~nls_multstart(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 15),
                        data = .x,
                        iter = c(4,4,4,4),
                        start_lower = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') - 10,
                        start_upper = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') + 10,
                        lower = get_lower_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),
                        upper = get_upper_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),
                        supp_errors = 'Y',
                        convergence_count = FALSE)),
         # create new temperature data
         high_pH_new_data = purrr::map(data, ~tibble(temp = seq(min(.x$temp), max(.x$temp), length.out = 100))),
         # predict over that data,
         high_pH_preds =  purrr::map2(sharpeschoolhigh, high_pH_new_data, ~augment(.x, newdata = .y)))

# unnest predictions
high_pH_preds <- dplyr::select(sharpe_schoolfield_model_high, high_pH_preds) %>% unnest(high_pH_preds) %>% 
  mutate(pH_Treatment="Ambient")

#Joining the data together for plotting 
TPC_preds <- full_join(low_pH_preds, high_pH_preds)


# plotting TPC for Sharpe-Schoolfield (high activation) model 
ggplot() +
  geom_line(aes(temp, .fitted, group=pH_Treatment, color=pH_Treatment), TPC_preds) +
  geom_point(aes(temp, rate, group=pH_Treatment, color=pH_Treatment, fill=pH_Treatment), respo.rates.tpc, size = 1.5, shape = 21, alpha=0.75) +
  facet_wrap(~pH_Treatment) +
  scale_color_manual(values = c("Low" = "cyan3", "Ambient" = "orange"), guide = "none") +
  scale_fill_manual(values = c("Low" = "cyan3", "Ambient" = "orange"), guide = "none") +
  common_theme +
  labs(x = xlab,
       y = ylab) + 
  scale_y_continuous(breaks = seq(0, 125, 25), limits=c(0,110)) +
  scale_x_continuous(breaks = seq(12, 26, 2), limits=c(12,26.5), position = "bottom") +
  geom_hline(yintercept = 0, linetype = 2)

```


# Bootstrapping the Sharpe-Schoolfield Model to Respiration Rates Across Temperature

```{r, message=FALSE, warning=FALSE}
# Fit the Sharpe-Schoolfield model for high pH using nmls LM prior to bootstrap
low_pH_fit_nlsLM <- minpack.lm::nlsLM(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 20),
                        data = low_pH,
                        start = coef(sharpe_schoolfield_model_low$sharpeschoolhigh[[1]]),
                        lower = get_lower_lims(low_pH$temp, low_pH$rate, model_name = 'sharpeschoolhigh_1981'),
                        upper = get_upper_lims(low_pH$temp, low_pH$rate, model_name = 'sharpeschoolhigh_1981'),
                        #weights = rep(1, times = nrow(low_pH)), 
                        na.action=na.exclude,
                        control = nls.control(maxiter = 1000, tol = 1e-6, minFactor = 1e-8, warnOnly = TRUE))

# Fit the Sharpe-Schoolfield model for high pH using nmls LM prior to bootstrap
high_pH_fit_nlsLM <- minpack.lm::nlsLM(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 20),
                        data = high_pH,
                        start = coef(sharpe_schoolfield_model_high$sharpeschoolhigh[[1]]),
                        lower = get_lower_lims(high_pH$temp, high_pH$rate, model_name = 'sharpeschoolhigh_1981'),
                        upper = get_upper_lims(high_pH$temp, high_pH$rate, model_name = 'sharpeschoolhigh_1981'),
                        #weights = rep(1, times = nrow(high_pH)),
                        na.action=na.exclude,
                        control = nls.control(maxiter = 1000, tol = 1e-6, minFactor = 1e-8, warnOnly = TRUE))
summary(low_pH_fit_nlsLM)
summary(high_pH_fit_nlsLM)
# bootstrap technique case versus residual bootstrapping 
# using residual resampling due to decreased standard error 

##  bootstrap using case resampling
low_pH_boot_case <- Boot(low_pH_fit_nlsLM, method = 'case', R=999)
hist.boot{low_pH_boot_case)
high_pH_boot_case <- Boot(high_pH_fit_nlsLM, method = 'case', R=999)
summary(low_pH_boot_case)
summary(high_pH_boot_case)

## bootstrapping using residual resampling 
low_pH_boot_residual <- Boot(low_pH_fit_nlsLM, method = 'residual', R=999) 
high_pH_boot_residual <- Boot(high_pH_fit_nlsLM, method = 'residual', R=999) 
summary(low_pH_boot_residual)
summary(high_pH_boot_residual)
 # choose bootstrapping through residual due to lower standard error
low_pH_boot_residual_preds <- low_pH_boot_residual$t %>%
  as.data.frame() %>%
  drop_na() %>%
  mutate(iter = 1:n()) %>%
  group_by_all() %>%
  do(data.frame(temp = seq(min(low_pH$temp), max(low_pH$temp), length.out = 100))) %>%
  ungroup() %>%
  mutate(pred = sharpeschoolhigh_1981(temp, r_tref, e, eh, th, tref = 15)) %>% 
  mutate(pH_Treatment="Low")

# predict over new data
high_pH_boot_residual_preds <- high_pH_boot_residual$t %>%
  as.data.frame() %>%
  drop_na() %>%
  mutate(iter = 1:n()) %>%
  group_by_all() %>%
  do(data.frame(temp = seq(min(high_pH$temp), max(high_pH$temp), length.out = 100))) %>%
  ungroup() %>%
  mutate(pred = sharpeschoolhigh_1981(temp, r_tref, e, eh, th, tref = 15)) %>% 
  mutate(pH_Treatment="Ambient")

# calculate bootstrapped confidence intervals
low_pH_boot_residual_CI_preds <- group_by(low_pH_boot_residual_preds, temp) %>%
  summarise(conf_lower = quantile(pred, 0.025),
            conf_upper = quantile(pred, 0.975),
            .groups = 'drop') %>% 
  mutate(pH_Treatment = "Low")

# calculate bootstrapped confidence intervals
high_pH_boot_residual_CI_preds <- group_by(high_pH_boot_residual_preds, temp) %>%
  summarise(conf_lower = quantile(pred, 0.025),
            conf_upper = quantile(pred, 0.975),
            .groups = 'drop') %>% 
  mutate(pH_Treatment = "Ambient")


TPC_boot_residual_preds <- full_join(low_pH_boot_residual_preds, high_pH_boot_residual_preds)
TPC_boot_residual_CI_preds <- full_join(low_pH_boot_residual_CI_preds, high_pH_boot_residual_CI_preds)

# Plotting the Sharpe-Schoolfield Model (Confidence Intervals and Model Predictions)
prediction_plots <- ggplot() +
  geom_point(aes(temp, rate, group=pH_Treatment, color=pH_Treatment), respo.rates.tpc, size = 1.1) +
  geom_line(aes(temp, pred, group = iter, color=pH_Treatment), TPC_boot_residual_preds, linewidth=0.5, alpha = 0.0095) +
  geom_line(aes(temp, .fitted, group=pH_Treatment, color=pH_Treatment), TPC_preds, linewidth=1) +
  facet_wrap(~pH_Treatment) +
  scale_color_manual(values = c("Low" = "cyan3", "Ambient" = "orange"), name = "pH Treatment") +
  common_theme +
  theme(
    strip.text.x = element_blank(),  # This will remove the facet labels
    legend.position = "top",
    legend.direction = "horizontal",
    legend.text = element_text(size = 12)
  ) +
  labs(x = xlab, y = ylab) + 
  scale_x_continuous(breaks = seq(12, 26, 2), expand = c(0, 0)) +
  scale_y_continuous(breaks = seq(0, 120, 30), limits = c(0, 120), expand = c(0, 0)) +
  geom_hline(yintercept = 0, linetype = 2) 

#print(prediction_plots)

# Assuming 'image' is your initial image path or object
original_image <- image_read(here::here("Images", "Tegula_funebralis_icon.png"))  # Adjust path as needed

# Colorize images with magick
image_ambient <- image_colorize(original_image,  color = "orange", opacity = 0.25)
image_low <- image_colorize(original_image, color = "cyan3", opacity = 0.25)
image_ambient <- image_modulate(image_ambient, brightness = 140)
image_low <- image_modulate(image_low, brightness = 140)


# Write images to files
image_write(image_ambient, "ambient_snail.png", quality=20, format = "png")
image_write(image_low, "low_snail.png", quality=20, format = "png")

# Add a column to your data frame for the image file path based on pH Treatment
respo.rates.tpc$image_path <- ifelse(respo.rates.tpc$pH_Treatment == "Low", "low_snail.png", "ambient_snail.png")
# Example of preparing data for geom_image
respo.rates.tpc$image_path <- ifelse(respo.rates.tpc$pH_Treatment == "Low", "low_snail.png", "ambient_snail.png")
respo.rates.tpc$x <- 13  # Example fixed position, adjust based on your data/needs
respo.rates.tpc$y <- 93  # Example fixed position, adjust based on your data/needs

respo.rates.tpc <- respo.rates.tpc %>% 
  mutate(pH_Treatment = case_when(pH_Treatment == "Low" ~ "pH 7.7", pH_Treatment == "Ambient" ~ "pH 7.9")) %>%
  mutate(pH_Treatment = factor(pH_Treatment, levels = c("pH 7.9", "pH 7.7")))

TPC_boot_residual_CI_preds <- TPC_boot_residual_CI_preds %>% 
  mutate(pH_Treatment = ifelse(pH_Treatment == "Low", "pH 7.7", "pH 7.9"))%>%
  mutate(pH_Treatment = factor(pH_Treatment, levels = c("pH 7.9", "pH 7.7")))
TPC_preds <- TPC_preds %>%
  mutate(pH_Treatment = ifelse(pH_Treatment == "Low", "pH 7.7", "pH 7.9"))%>%
  mutate(pH_Treatment = factor(pH_Treatment, levels = c("pH 7.9", "pH 7.7")))


CI_plots <- ggplot(data = respo.rates.tpc) +
  geom_point(aes(x= temp, y=rate, color = pH_Treatment), size = 1.5) +
  geom_ribbon(aes(x= temp, ymin = conf_lower, ymax = conf_upper, fill = pH_Treatment), data = TPC_boot_residual_CI_preds, alpha = 0.2) +
  geom_line(aes(x=temp, y = .fitted, color = pH_Treatment), data = TPC_preds, size=1) +
  geom_line(aes(x=temp, y = conf_lower, color = pH_Treatment), data = TPC_boot_residual_CI_preds, linetype = "dashed", size = 0.5) +
  geom_line(aes(x=temp, y = conf_upper, color = pH_Treatment), data = TPC_boot_residual_CI_preds, linetype = "dashed", size = 0.5) +
  facet_wrap(~pH_Treatment) +
  scale_fill_manual(values = c("pH 7.7" = "cyan3", "pH 7.9" = "orange"), name = "pH Treatment") +
  scale_color_manual(values = c("pH 7.7" = "cyan3", "pH 7.9" = "orange"), name = "pH Treatment") +
  common_theme +
  theme(strip.text.x = element_blank(), legend.position = "none") +
  labs(x = xlab, y = ylab) +  # Ensure these labels are correctly defined
  scale_x_continuous(breaks = seq(12, 26, 2), expand = c(0.05, 0)) +
  scale_y_continuous(breaks = seq(0, 100, 20), expand = c(0, 2)) +
  #geom_hline(yintercept = 0, linetype = 2) +
geom_label(aes(x = 13, y = 85, label = pH_Treatment,
               family="serif", color=pH_Treatment),  size = 4,
                     label.r=unit(0, "cm"), fill = NA, label.size = NA)+  # Positioning text 
  geom_image(aes(x=x, y=y, image = image_path, color=pH_Treatment, alpha=0.2), size = 0.15) +
  theme(
    plot.margin = margin(10, 10, 10, 10),
    axis.text.x = element_text(size = 14),
    axis.text.y = element_text(size = 14),
    axis.title = element_text(size = 16),
    axis.title.y= element_text(size=16), margin = margin(t = 0, r = 5, b = 0, l = 0),
    axis.title.x= element_text(size=16), margin = margin(t = 0, r = 10, b = 0, l = 0),
    panel.grid.major = element_line(color = "gray90", size = 0.5),
    panel.grid.minor = element_line(color = "gray90", size = 0.25),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.background = element_rect(fill = "white", color = "white")  # Adjust if needed
  ) 

#print(CI_plots)

# Save the plots as a PNG file
ggsave(here::here("Figures", "TPC_CI_plot.png"), CI_plots, width = 8, height = 5, units = "in")


process_coefficients <- function(data, pH_label) {
  tidy(data) %>%
    mutate(
      pH_condition = pH_label,
      term = case_when(
        term == "r_tref" ~ "R_Tref",
        term == "e" ~ "E_a",  # Using html() to ensure rendering
        term == "eh" ~ "E_h",
        term == "th" ~ "T_h",
        TRUE ~ term
      ),
      estimate = round(estimate, digits = 2)  # Ensure estimates are rounded
    )
}

# Adjust bootstrapping results, including bias and std.error directly
process_bootstrap <- function(data, pH_label) {
  tidy(data) %>%
    mutate(
      pH_condition = pH_label,
      term = case_when(
        term == "r_tref" ~ "R_Tref",
        term == "e" ~ "E_a",  # Using html() to ensure rendering
        term == "eh" ~ "E_h",
        term == "th" ~ "T_h",
        TRUE ~ term
      ),
      boot.std.error = std.error,
      bias = bias
    )
}

format_p_values <- function(data) {
  data %>%
    mutate(
      p.value = case_when(
        p.value < 0.001 ~ "<0.001 ***",
        p.value < 0.01  ~ "<0.01 **",
        p.value < 0.05  ~ "<0.05 *",
        is.na(p.value)  ~ "",
        TRUE            ~ sprintf("%.2f", p.value)  # Use sprintf for precise formatting
      )
    )
}


# Process data
low_pH_coefficients <- process_coefficients(low_pH_fit_nlsLM, "Low pH Treatment")
high_pH_coefficients <- process_coefficients(high_pH_fit_nlsLM, "Ambient pH Treatment")

low_pH_boot_resid <- process_bootstrap(low_pH_boot_residual, "Low pH Treatment")
high_pH_boot_resid <- process_bootstrap(high_pH_boot_residual, "Ambient pH Treatment")

# Correctly bind and join datasets
pH_coefficients <- bind_rows(low_pH_coefficients, high_pH_coefficients) %>% 
  dplyr::select(term, statistic, p.value, pH_condition)
boot_resid <- bind_rows(low_pH_boot_resid, high_pH_boot_resid) %>% 
  dplyr::select(term, estimate=statistic, boot.std.error, bias, pH_condition)

# Perform join using only term and pH_condition
summary_coefficients <- full_join(pH_coefficients, boot_resid, by = c("term", "pH_condition")) %>%
  format_p_values() %>% 
  group_by(pH_condition)

summary_coefficients <- summary_coefficients %>% 
mutate(
    estimate = round(estimate, digits = 2),
    statistic = round(statistic, digits = 2),
    bias = round(bias, digits = 2),
    bootstrapped_std_error = round(boot.std.error, digits = 2)
  ) %>%
  dplyr::select(term, pH_condition, estimate, bootstrapped_std_error, p.value, statistic, bias)

  
# Create the summary table
summary_table <- summary_coefficients %>% 
  gt(rowname_col = "term", groupname_col = "pH_condition") %>%
  tab_header(
    title = md("**Comparative Summary of Bootstrapped Coefficient Estimates For <br>The Sharpe-Schoolfield Model (High-Activation Energy)**")
  ) %>%
  cols_label(
    term = "Parameter",
    estimate = "Estimate",
    bootstrapped_std_error = "Std. Error",
    statistic = "Statistic",
    p.value = "P-Value",
    bias = "Bias"
  ) %>%
  text_transform(
    locations = cells_stub(
    ),
    fn = function(x) {
      sub <- str_extract(x, "(?<=_)[A-Za-z]+$")  
      text <- str_extract(x, "^[A-Za-z]+") 
      glue::glue("{text}<sub>{sub}</sub>")
    }
  ) %>% 
  gt_theme %>% 
  gt::tab_options(table.width = px(650),
                  table.align = "center",
                  table.border.top.width = px(1), 
                  table.border.bottom.width = px(1),
                  column_labels.border.top.color = "grey25",
                   heading.border.bottom.color =  "grey25",
                  row_group.font.weight = "bold",
                  row_group.border.bottom.color = "grey25",
                  row_group.border.top.color = "grey25",
                  row_group.border.top.width = px(1.5),
                  row_group.border.bottom.width = px(1.5),
                  table_body.vlines.color ="grey25",
                  stub.border.color = "grey25",
                  stub.border.width = px(1.5),
                  stub_row_group.border.color ="grey25",
                  stub_row_group.border.width = px(5),
                  stub.indent_length = px(5)
                  )

# Print the table
print(summary_table)

# Save the table as an image
gtsave(summary_table, file = here::here("Figures", "sharpe-schoolfield-summary_table.png"))

```


```{r}
tidy_summary_low <- broom::tidy(low_pH_fit_nlsLM)
glance(low_pH_fit_nlsLM)
augment(low_pH_fit_nlsLM)
print(tidy_summary_low)


tidy_summary_high <- broom::tidy(high_pH_fit_nlsLM)
glance(high_pH_fit_nlsLM)
print(tidy_summary_high)
augment(high_pH_fit_nlsLM)

```




# "Metabolic Scaling Analysis"



# Calculating Parameters and Confidence Intervals for the Sharpe-Schoolfield Model
 only calculates the activation energy,
deactivation energy, and q10 when using <code>method = case</code> not
<code>method = residual</code>

```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)

# Assuming you have the necessary data loaded
# load your joule rates dataset
joule_rate_data <- read_csv(here("Data", "Thinned_Respirometry_Data", "Respiration.Rates.Dataset.csv"), show_col_types = FALSE) %>% 
  mutate(rate = umolO2.gram.hr, temp = Temp.C) %>% 
  dplyr::select(temp, rate)
# Filter data for the required temperature range (12°C to 26°C)
filtered_data <- joule_rate_data %>% filter(temp >= 12 & temp <= 26)

# Fit the Sharpe-Schoolfield model
fit_sharpe_schoolfield <- nls(rate ~ sharpeschoolhigh_1981(temp, r_tref, e, eh, th, tref = 15), 
                              data = filtered_data,
                              start = list(r_tref = 1, e = 0.65, eh = 2, th = 30))


# Function to calculate average performance at fluctuating temperatures
calc_average_performance <- function(temps, fit) {
  predict(fit, newdata = data.frame(temp = temps))
}

# Calculate average performance for given temperature ranges within 12°C to 26°C
avg_perf_12_26 <- mean(predict(fit_sharpe_schoolfield, newdata = data.frame(temp = seq(12, 26, by = 0.1))))
avg_perf_14_24 <- mean(predict(fit_sharpe_schoolfield, newdata = data.frame(temp = c(14, 24))))
avg_perf_16_20 <- mean(predict(fit_sharpe_schoolfield, newdata = data.frame(temp = c(16, 20))))


```



```{r}
# Install TrenchR package if not already installed
if (!requireNamespace("TrenchR", quietly = TRUE)) {
  install.packages("TrenchR")
}

# Load necessary libraries
library(TrenchR)

# Define parameters
T_r <- 30
l <- 0.0176
h <- 0.0122
S <- 800
u <- 1
psi <- 30
c <- 1
position <- "anterior"


# Define additional parameters for Tb_snail
l_snail <- 0.012
CC <- 0.5
WL <- 0
WSH <- 10

# Calculate body temperature for Tb_snail
t.seq.snail <- sapply(20:40, 
                      FUN = function(T_a) Tb_snail(T_a, l_snail, S, u, CC, WL, WSH))

# Add Tb_snail to the plot
points(x = 20:40, 
       y = t.seq.snail, 
       type = "l", 
       lty = "dotted")

# Add abline for reference
abline(a = 0, b = 1, col = "gray")

# Add legend
legend(x = "bottomright",
       title = expression("function"), 
       legend = c("Tb_limpet", "Tb_limpetBH", "Tb_Snail"), 
       lty = c("solid", "dashed", "dotted"))

```

```{r}
MO2 <- c(0.05, 0.3, 0.6, 1.2, 3, 6) # Example metabolic rates

# Calculate scaling coefficients
result_nls <- calc_b(mass, MO2, method = "nls", plot = "linear")
result_lm <- calc_b(mass, MO2, method = "lm", plot = "log")

# Print results
print(result_nls)
print(result_lm)
```

# JEnson in equality 

```{r}



# Load necessary libraries
library(ggplot2)
library(dplyr)

# Generate synthetic data for the TPC
temp <- seq(10, 40, by = 0.1)
rate <- sharpeschoolhigh_1981(temp = temp, r_tref = 1, e = 0.65, eh = 2, th = 30, tref = 15)

# Create a data frame
tpc_data <- data.frame(temp = temp, rate = rate)

# Define empirical measurements (open dots)
empirical_temps <- c(15, 20, 25, 30, 35)
empirical_rates <- sharpeschoolhigh_1981(temp = empirical_temps, r_tref = 1, e = 0.65, eh = 2, th = 30, tref = 15)
empirical_data <- data.frame(temp = empirical_temps, rate = empirical_rates)

# Function to calculate average performance at fluctuating temperatures
calc_average_performance <- function(temps, r_tref, e, eh, th, tref) {
  rates <- sharpeschoolhigh_1981(temp = temps, r_tref = r_tref, e = e, eh = eh, th = th, tref = tref)
  mean(rates)
}

# Calculate average performance for given temperature ranges
avg_perf_21_31 <- calc_average_performance(c(21, 31), 1, 0.65, 2, 30, 15)
avg_perf_16_36 <- calc_average_performance(c(16, 36), 1, 0.65, 2, 30, 15)
avg_perf_26 <- sharpeschoolhigh_1981(temp = 26, r_tref = 1, e = 0.65, eh = 2, th = 30, tref = 15)

# Plot the results
ggplot(tpc_data, aes(x = temp, y = rate)) +
  geom_line(color = "red") +
  geom_point(data = empirical_data, aes(x = temp, y = rate), shape = 1, size = 3) +
  geom_hline(yintercept = avg_perf_26, linetype = "dashed") +
  geom_point(aes(x = 26, y = avg_perf_26), color = "red", size = 3) +
  geom_point(aes(x = 26, y = avg_perf_21_31), color = "blue", size = 3) +
  geom_point(aes(x = 26, y = avg_perf_16_36), color = "green", size = 3) +
  annotate("text", x = 27, y = avg_perf_26 + 1, label = "Avg performance\nat constant temp", color = "red") +
  annotate("text", x = 27, y = avg_perf_21_31 + 1, label = "Avg performance\n(21°C & 31°C)", color = "blue") +
  annotate("text", x = 27, y = avg_perf_16_36 + 1, label = "Avg performance\n(16°C & 36°C)", color = "green") +
  labs(x = "Temperature (°C)", y = "Performance Rate", title = "Jensen's Inequality and Thermal Performance Curves") +
  theme_minimal()
```



```{r}
library(MASS)

# Calculating Parameters for fitted low pH model 
param_low_pH <- broom::tidy(low_pH_fit_nlsLM) %>%
  dplyr::select(param = term, estimate) 



# CIs from residual  resampling
ci_low_pH_residual <- confint(low_pH_boot_residual, method = 'bca', level = 0.95) %>%
  as.data.frame() %>%
  rename(conf_lower = 1, conf_upper = 2) %>%
  tibble::rownames_to_column(., var = 'param') %>%
  mutate(method = 'residual bootstrap')

# CIs from residual resampling
ci_param_low_pH <- bind_rows(ci_low_pH_residual) %>% 
  left_join(param_low_pH) %>% 
  mutate(pH_Treatment = "Low")

# Calculating Parameters for fitted high pH model
param_high_pH <- broom::tidy(high_pH_fit_nlsLM) %>%
  dplyr::select(param = term, estimate)



# CIs from residual resampling
ci_high_pH_residual <- confint(high_pH_boot_residual, method = 'bca', level = 0.95) %>%
  as.data.frame() %>%
  rename(conf_lower = 1, conf_upper = 2) %>%
  tibble::rownames_to_column(., var = 'param') %>%
  mutate(method = 'residual bootstrap')

# CIs from residual resampling high pH
ci_param_high_pH <- bind_rows(ci_high_pH_residual) %>% 
  left_join(param_high_pH) %>% 
  mutate(pH_Treatment = "Ambient")

# Combine the confidence intervals for low and high pH
ci_param <- bind_rows(ci_param_low_pH, ci_param_high_pH)

ggplot(ci_param, aes(param, estimate, color = pH_Treatment, group = pH_Treatment)) +
  geom_point(size = 4, position = position_dodge(width = 0.5)) +
  geom_linerange(aes(ymin = conf_lower, ymax = conf_upper), position = position_dodge(width = 0.5)) +
  geom_hline(aes(yintercept = conf_lower, color = pH_Treatment), linetype = 2) +
  geom_hline(aes(yintercept = conf_upper, color = pH_Treatment), linetype = 2) +
  common_theme +
  theme_minimal() +
  facet_wrap(~ param, scales = 'free') +
  scale_color_manual(values = c("Low" = "cyan3", "Ambient" = "orange")) +
  scale_x_discrete('') +
  labs(title = 'Calculation of confidence intervals for TPC parameters')
```


```{r}

MASS::rms.curv		Relative Curvature Measures for Non-Linear Regression
minpack.lm::wfct		Weighting function that can be supplied to the 'weights' argument of 'nlsLM' or 'nls'
nlstools::confint2		Confidence intervals in nonlinear regression
nlstools::nlsBoot		Bootstrap resampling
nlstools::nlsJack		Jackknife resampling
nlstools::nlsResiduals		NLS residuals
stats::vcov
```




```{r, warning =FALSE, messsage=FALSE}
# Calculate parameters for high pH fit
high_pH_fit_extra_params <- calc_params(high_pH_fit_nlsLM) %>%
  pivot_longer(everything(), names_to = 'param', values_to = 'estimate') %>%
  mutate(pH_Treatment = "Ambient") 
print(high_pH_fit_extra_params)

high_pH_ci_extra_params <- Boot(high_pH_fit_nlsLM, f = function(x){unlist(calc_params(x))}, 
  labels = names(calc_params(high_pH_fit_nlsLM)), R = 1000, method = 'residual') %>%
  confint(.,  method = 'bca', level = 0.95) %>%
  as.data.frame() %>%
  rename(conf_lower = 1, conf_upper = 2) %>%
  tibble::rownames_to_column(., var = 'param')  %>%
  mutate(method = 'residual bootstrap',
       pH_Treatment = "Ambient")
print(high_pH_ci_extra_params)

high_pH_ci_extra_params <- left_join(high_pH_ci_extra_params, high_pH_fit_extra_params)
  
# Calculate parameters for low pH fit
low_pH_fit_extra_params <- calc_params(low_pH_fit_nlsLM) %>%
  pivot_longer(everything(), names_to = 'param', values_to = 'estimate') %>%
  mutate(pH_Treatment = "Low")
print(low_pH_fit_extra_params)

low_pH_ci_extra_params <- Boot(low_pH_fit_nlsLM, f = function(x){unlist(calc_params(x))},
  labels = names(calc_params(low_pH_fit_nlsLM)), R = 1000, method = 'residual') %>%
  confint(.,  method = 'bca', level = 0.95) %>%
  as.data.frame() %>%
  rename(conf_lower = 1, conf_upper = 2) %>%
  tibble::rownames_to_column(., var = 'param')  %>%
  mutate(method = 'residual bootstrap',
       pH_Treatment = "Low")
print(low_pH_ci_extra_params)

low_pH_ci_extra_params <- left_join(low_pH_ci_extra_params, low_pH_fit_extra_params)

# Combine the confidence intervals for low and high pH
ci_extra_param <- bind_rows(low_pH_ci_extra_params, high_pH_ci_extra_params)

# Merge and filter
ci_parameters <- full_join(ci_param, ci_extra_param) %>% 
  filter(param %in% c("e", "eh", "breadth", "rmax", "topt", "ctmax"))

# Define scientific notations
sci_notations <- list(
  e = expression(Activation~Energy~(paste(E))),
  eh = expression(Deactivation~Energy~(paste(E[h]))),
  breadth = expression(Thermal~Breadth~(paste(T[Br]))),
  rmax = expression(Maximum~Rate~(paste(R[max]))),
  topt = expression(Thermal~Optimum~(paste(T[opt]))),
  ctmax =  expression(Thermal~Maximum~(paste(CT[max])))
)

# Apply labels and notations to ci_parameters
ci_parameters$param_sci <- sci_notations[ci_parameters$param]

# Convert expressions to strings suitable for ggplot2 without the "expression" wrapper
ci_parameters$param_sci <- sapply(ci_parameters$param_sci, function(item) {
  if (is.expression(item)) {
    # Deparse the inner expression to avoid "expression()" wrapper
    return(deparse(item[[1]]))
  } else {
    # Already a plain string; return as is
    return(item)
  }
})

# Ordering based on scientific notations
ordered_sci_notations <- names(sci_notations)
ci_parameters <- ci_parameters[order(match(ci_parameters$param, ordered_sci_notations)),]

# Reset row numbers
ci_parameters <- ci_parameters %>% 
  mutate(row_number = row_number())

# Step 1: Data Preprocessing
# Calculate the min and max confidence interval values for each param_sci to limit the figure upwards and downwards
ci_parameters %>%
  group_by(param_sci) %>%
  summarise(
    range_ci = max(conf_upper, na.rm = TRUE) - min(conf_lower, na.rm = TRUE),  # Determine the range of CI for each parameter
    min_conf_lower = floor(min(conf_lower, na.rm = TRUE)) - (range_ci * 0.05),  # Expand lower limit based on the CI range
    max_conf_upper = ceiling(max(conf_upper, na.rm = TRUE)) + (range_ci * 0.17)  # Expand upper limit based on the CI range
  ) -> conf_limits
# Join the limits back to the original data for plotting
ci_parameters <- left_join(ci_parameters, conf_limits, by = "param_sci") 

significant <- subset(ci_parameters, param == "rmax") %>% dplyr::select(param, param_sci, pH_Treatment, conf_upper) %>% mutate(conf_upper=conf_upper+1.5) %>%  filter(pH_Treatment == "Low")


ci_params <- ggplot(ci_parameters, aes(x = pH_Treatment, y = estimate, color = pH_Treatment, group = param_sci)) +
  geom_hline(data = conf_limits, aes(yintercept = min_conf_lower, group = param_sci), alpha = 0) +
  geom_hline(data = conf_limits, aes(yintercept = max_conf_upper, group = param_sci), alpha = 0) +
  geom_errorbar(aes(ymin = conf_lower, ymax = conf_upper), width = 0.2, size = 0.5) +
  geom_point(size = 1.8, stroke = 1) +
  geom_text(data = significant,
            aes(label = "*", hjust = 5, y = conf_upper), 
            position = position_dodge(width = -0.5),  # Adjusted position
            color = "grey25", size = 5) +
  facet_wrap(~factor(param_sci, levels = unique(ci_parameters$param_sci)), scales = 'free_y', labeller = label_parsed, 
             strip.position = "top", nrow = 3, ncol = 3) +
  labs(
    y = expression("Parameter Estimate (?? 95% CI Range)"),
    x = NULL,  # Removing x-axis label
    legend.title = expression("pH Treatment")
  ) +
  scale_color_manual(values = c("Low" = "cyan3", "Ambient" = "orange"),  name="pH Treatment") +
  common_theme +
  theme(
    strip.text.x = element_text(size = 12.5),
    axis.text.x = element_text(size = 14),
    axis.text.y = element_text(size = 14),
    axis.title = element_text(size = 16),
    axis.title.y= element_text(size=16),
    legend.position = "none",
    panel.grid.major = element_line(color = "gray90", size = 0.5),
    panel.grid.minor = element_line(color = "gray90", size = 0.25),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.border = element_rect(color = "grey25", fill = NA, size = 0.5),
    panel.background = element_rect(fill = "white", color = "white")  # Adjust if needed
  ) +
  scale_y_continuous(breaks = scales::pretty_breaks(min.n = 4), oob = scales::oob_squish)

print(ci_params)
ggsave(here::here("Figures", "TPC_parameters.png"), ci_params, width = 8, height = 6, units = "in", dpi=1200)

```

```{r}
# Summarize the bootstrapped results
low_pH_summary_case <- broom::tidy(low_pH_boot_case)
low_pH_summary_residual <- broom::tidy(low_pH_boot_residual)
print(low_pH_summary_case)
print(low_pH_summary_residual)

high_pH_summary_case <- broom::tidy(high_pH_boot_case)
high_pH_summary_residual <- broom::tidy(high_pH_boot_residual)
print(high_pH_summary_case)
print(high_pH_summary_residual)


# Define the Sharpe-Schoolfield model function
sharpeschoolhigh_1981 <- function(temp, r_tref, e, eh, th, tref = 20) {
  r_tref * exp(e * (1 / (273.15 + tref) - 1 / (273.15 + temp))) / 
    (1 + exp(eh * (1 / (273.15 + th) - 1 / (273.15 + temp))))
}

# Manually construct the equations based on the parameters
low_pH_equation <- paste0(
  "Rate = ", round(low_pH_summary_residual$statistic[1], 2), " * exp(",
  round(low_pH_summary_residual$statistic[2], 2), " * ((1 / (8.617e-5 * (Temperature + 273.15))) - (1 / (8.617e-5 * 298.15)))) / (1 + exp(",
  round(low_pH_summary_residual$statistic[3], 2), " * ((1 / (8.617e-5 * ", round(low_pH_summary_residual$statistic[4], 2), ")) - (1 / (8.617e-5 * (Temperature + 273.15)))))"
)

high_pH_equation <- paste0(
  "Rate = ", round(high_pH_summary_case$statistic[1], 2), " * exp(",
  round(high_pH_summary_case$statistic[2], 2), " * ((1 / (8.617e-5 * (Temperature + 273.15))) - (1 / (8.617e-5 * 298.15)))) / (1 + exp(",
  round(high_pH_summary_case$statistic[3], 2), " * ((1 / (8.617e-5 * ", round(high_pH_summary_case$statistic[4], 2), ")) - (1 / (8.617e-5 * (Temperature + 273.15)))))"
)

print(low_pH_equation)
print(high_pH_equation)
```

# Thermal optimum analysis for temeprature distributions 

```{r}
merged.respo.rates <- read_csv(here("Data", "Thinned_Respirometry_Data", "Respiration.Rates.Dataset.csv"), show_col_types = FALSE)

merged.respo.rates <- merged.respo.rates %>% 
  select(rate=umolO2.gram.hr, temp=Temp.C, Temp_Treatment, pH_Treatment)

low_pH <- merged.respo.rates %>% 
  filter(pH_Treatment == "Low") %>% 
  select(rate, temp)

ambient_pH <- merged.respo.rates %>% 
  filter(pH_Treatment == "Ambient") %>% 
  select(rate, temp)


# Calculate the variance of the rate for each temperature group
var_low_pH <- tapply(low_pH$rate, low_pH$temp, var, na.rm = TRUE)
var_high_pH <- tapply(high_pH$rate, high_pH$temp, var, na.rm = TRUE)

# Handle NA values: Replace NA variances with the mean of non-NA variances
mean_var_low_pH <- mean(var_low_pH, na.rm = TRUE)
mean_var_high_pH <- mean(var_high_pH, na.rm = TRUE)

var_low_pH[is.na(var_low_pH)] <- mean_var_low_pH
var_high_pH[is.na(var_high_pH)] <- mean_var_high_pH

# Repeat the variances for each observation within the same temperature group
weights_low_pH <- sapply(low_pH$temp, function(t) 1 / (var_low_pH[as.character(t)]^2))
weights_high_pH <- sapply(high_pH$temp, function(t) 1 / (var_high_pH[as.character(t)]^2))

# Ensure the lengths are correct
length(weights_low_pH) == nrow(low_pH)
length(weights_high_pH) == nrow(high_pH)

# Print weights vectors to ensure correctness
print(weights_low_pH)
print(weights_high_pH)

# Assuming 'low_pH' and 'high_pH' data frames are available and properly formatted

# Fit the model using nlsLM for low pH with calculated weights
low_pH_fit_nlsLM <- minpack.lm::nlsLM(
  rate ~ sharpeschoolhigh_1981(temp = temp, r_tref, e, eh, th, tref = 15),
  data = low_pH,
  start = coef(total_fit$sharpeschoolhigh[[1]]),  # Use the extracted coefficients as starting values
  lower = get_lower_lims(low_pH$temp, low_pH$rate, model_name = 'sharpeschoolhigh_1981'),
  upper = get_upper_lims(low_pH$temp, low_pH$rate, model_name = 'sharpeschoolhigh_1981'),
  weights = weights_low_pH,
  na.action = na.exclude,
  control = nls.control(maxiter = 1000, tol = 1e-6, minFactor = 1e-8, warnOnly = TRUE)
)

# Fit the model using nlsLM for high pH with calculated weights
high_pH_fit_nlsLM <- minpack.lm::nlsLM(
  rate ~ sharpeschoolhigh_1981(temp = temp, r_tref, e, eh, th, tref = 15),
  data = high_pH,
  start = coef(total_fit$sharpeschoolhigh[[1]]),  # Use the extracted coefficients as starting values
  lower = get_lower_lims(high_pH$temp, high_pH$rate, model_name = 'sharpeschoolhigh_1981'),
  upper = get_upper_lims(high_pH$temp, high_pH$rate, model_name = 'sharpeschoolhigh_1981'),
  weights = weights_high_pH,
  na.action = na.exclude,
  control = nls.control(maxiter = 1000, tol = 1e-6, minFactor = 1e-8, warnOnly = TRUE)
)

# Print the summaries of the models
summary(low_pH_fit_nlsLM)
summary(high_pH_fit_nlsLM)
```


```{r}



merged.respo.rates <- read_csv(here("Data", "Thinned_Respirometry_Data", "Respiration.Rates.Dataset.csv"), show_col_types = FALSE)

merged.respo.rates <- merged.respo.rates %>% 
  select(rate=umolO2.gram.hr, temp=Temp.C) 


# fit Sharpe-Schoolfield model
total_fit <- nest(merged.respo.rates, data = c(temp, rate)) %>%
  mutate(sharpeschoolhigh = map(data, ~nls_multstart(
    rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 20),
                        data = .x,
                        iter = c(3,3,3,3),
                        start_lower = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') - 10,
                        start_upper = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') + 10,
                        lower = get_lower_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),
                        upper = get_upper_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),
                        supp_errors = 'Y',
                        convergence_count = FALSE)),
         # create new temperature data
         new_data = map(data, ~tibble(temp = seq(min(.x$temp), max(.x$temp), length.out = 100))),
         # predict over that data,
         preds =  map2(sharpeschoolhigh, new_data, ~augment(.x, newdata = .y)))

# unnest predictions
total_preds <- select(total_fit, preds) %>%
  unnest(preds)
total_fit
summary(total_fit)

# Extract coefficients from the initial fit
start_vals <- coef(total_fit$sharpeschoolhigh[[1]])

# refit model using nlsLM
total_fit_nlsLM <- minpack.lm::nlsLM(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 20),
                        data = merged.respo.rates,
                        start = start_vals,
                        lower = get_lower_lims(merged.respo.rates$temp, merged.respo.rates$rate,
                                               model_name = 'sharpeschoolhigh_1981'),
                        upper = get_upper_lims(merged.respo.rates$temp, merged.respo.rates$rate,
                                               model_name = 'sharpeschoolhigh_1981'),
                        weights = rep(1, times = nrow(merged.respo.rates)))


total_fit_nlsLM # Residual Sum of Squares: residual sum-of-squares: 7864
summary(total_fit_nlsLM)
residuals(total_fit_nlsLM)
logLik(total_fit_nlsLM)

# bootstrap using case resampling
boot_total_case <- car::Boot(total_fit_nlsLM, method = 'residual')
hist(boot_total_case, layout = c(2,2))

# create predictions of each bootstrapped model
boot_total_case_preds <- boot_total_case$t %>%
  as.data.frame() %>%
  drop_na() %>%
  mutate(iter = 1:n()) %>%
  group_by_all() %>%
  do(data.frame(temp = seq(min(merged.respo.rates$temp), max(merged.respo.rates$temp), length.out = 100))) %>%
  ungroup() %>%
  mutate(pred = sharpeschoolhigh_1981(temp, r_tref, e, eh, th, tref = 15))

# calculate bootstrapped confidence intervals
boot_total_case_preds_conf_preds <- group_by(boot_total_case_preds, temp) %>%
  summarise(conf_lower = quantile(pred, 0.025),
            conf_upper = quantile(pred, 0.975)) %>%
  ungroup()



case_total_CI + case_total_preds
```




```{r}

# Define the Sharpe-Schoolfield model function
sharpeschoolhigh_1981 <- function(temp, r_tref, e, eh, th, tref = 20) {
  r_tref * exp(e * (1 / (273.15 + tref) - 1 / (273.15 + temp))) / 
    (1 + exp(eh * (1 / (273.15 + th) - 1 / (273.15 + temp))))
}

# Load your data
merged.respo.rates <- read_csv(here("Data", "Thinned_Respirometry_Data", "Respiration.Rates.Dataset.csv"),
                               show_col_types = FALSE)

# Select necessary columns including treatment variables
merged.respo.rates <- merged.respo.rates %>% 
  select(temp = Temp.C, Temp_Treatment, pH = pH_Treatment, rate = umolO2.gram.hr)

# Fit the model using nls_multstart for each combination of Temp_Treatment and pH
total_fit <- merged.respo.rates %>%
  group_by(pH) %>%
  nest() %>%
  mutate(sharpeschoolhigh = map(data, ~nls_multstart(
    rate ~ sharpeschoolhigh_1981(temp = temp, r_tref, e, eh, th, tref = 20),
    data = .x,
    iter = c(3, 3, 3, 3),
    start_lower = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') - 10,
    start_upper = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') + 10,
    lower = get_lower_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),
    upper = get_upper_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),
    supp_errors = 'Y',
    convergence_count = FALSE)),
    # Create new temperature data
    new_data = map(data, ~tibble(temp = seq(min(.x$temp), max(.x$temp), length.out = 100))),
    # Predict over that data
    preds = map2(sharpeschoolhigh, new_data, ~augment(.x, newdata = .y))) %>%
  unnest(preds)

# Refit the model using nlsLM with weights and get equations
refit_nlsLM <- function(data) {
  start_vals <- coef(nls_multstart(
    rate ~ sharpeschoolhigh_1981(temp = temp, r_tref, e, eh, th, tref = 20),
    data = data,
    iter = c(3, 3, 3, 3),
    start_lower = get_start_vals(data$temp, data$rate, model_name = 'sharpeschoolhigh_1981') - 10,
    start_upper = get_start_vals(data$temp, data$rate, model_name = 'sharpeschoolhigh_1981') + 10,
    lower = get_lower_lims(data$temp, data$rate, model_name = 'sharpeschoolhigh_1981'),
    upper = get_upper_lims(data$temp, data$rate, model_name = 'sharpeschoolhigh_1981'),
    supp_errors = 'Y',
    convergence_count = FALSE))
  
  model <- nlsLM(
    rate ~ sharpeschoolhigh_1981(temp = temp, r_tref, e, eh, th, tref = 20),
    data = data,
    start = start_vals,
    lower = get_lower_lims(data$temp, data$rate, model_name = 'sharpeschoolhigh_1981'),
    upper = get_upper_lims(data$temp, data$rate, model_name = 'sharpeschoolhigh_1981'),
    weights = rep(1, times = nrow(data))
  )
  
  model
}

# Refit the model using nlsLM for each combination
total_fit_nlsLM <- total_fit %>%
  mutate(fit_nlsLM = map(data, ~tryCatch(refit_nlsLM(.x), error = function(e) NULL)))

# Generate LaTeX equations for each model
total_fit_nlsLM <- total_fit_nlsLM %>%
  mutate(equation = map(fit_nlsLM, ~tryCatch(extract_eq(.x, wrap = TRUE), error = function(e) NULL)))

# Print the equations
total_fit_nlsLM %>% select(pH, equation) %>% print()


```



```{r}
model_coefficients <- coef(total_fit$sharpeschoolhigh[[1]])

r_tref <- model_coefficients[1]
e <- model_coefficients[2]
eh <- model_coefficients[3]
th <- model_coefficients[4]

sharpeschoolhigh_1981(temp, r_tref=r_tref, e=e, eh=eh, th=th, tref=20) 
  
  
  r_tref * exp(e * (1/(tref + 273.15) - 1/(temp + 273.15)) - eh * (1/(th + 273.15) - 1/(temp + 273.15)))
#broom::augment.nls(total_fit_nlsLM)

# bootstrap using case resampling
boot1 <- Boot(total_fit_nlsLM, method = 'case')

# look at the data
head(boot1$t)

# plot data and predictions

ggplot() +
  geom_line(aes(temp, .fitted), total_preds, col = "#F7D54FFF") +
  geom_point(aes(temp, rate), col = "black", merged.respo.rates, size = 2, alpha = 0.5) +
  theme_bw(base_size = 12) +
  labs(x = 'Temperature (ºC)',
       y = 'Growth rate',
       title = 'Growth rate across temperatures')
```


```{r}
# Fit the Sharpe-Schoolfield model for high pH using nmls LM prior to bootstrap
total_fit_nlsLM <- minpack.lm::nlsLM(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 20),
                        data = merged.respo.rates,
                        start = 
                        lower = 
                        upper = 
                        weights = rep(1, times = nrow(high_pH)),
                        na.action=na.exclude,
                        control = nls.control(maxiter = 1000, tol = 1e-6, minFactor = 1e-8, warnOnly = TRUE))

merged_fit <- nest(merged.respo.rates, data = c(temp, rate)) %>%
  mutate(sharpeschoolhigh = map(data, ~nls_multstart(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 20),
                        data = .x,
                        iter = c(3,3,3,3),
                        start_lower = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') - 10,
                        start_upper = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') + 10,
                        lower = get_lower_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),
                        upper = get_upper_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),
                        supp_errors = 'Y',
                        convergence_count = FALSE)),
         # create new temperature data
         new_data = map(data, ~tibble(temp = seq(min(.x$temp), max(.x$temp), length.out = 100))),
         # predict over that data,
         preds =  map2(sharpeschoolhigh, new_data, ~augment(.x, newdata = .y)))


nlsLM(low_pH_boot_case)
```

```{r}
# extracting Topt 
topt <- ci_parameters %>% filter(param == "topt") %>% summarise(topt=mean(estimate, na.rm = TRUE))
# Define 'topt' as a single numeric value, not as a column, for ease of use in ggplot
topt_value <- topt$topt

# Read the CSV file
newport_temp <- read_csv(here("Data", "Site_Data", "NewportBeach_TEMP_1924-2023.csv"), show_col_types = FALSE)

# Data Cleaning and Transformation
# Filter data based on TEMP_FLAG and SURF_TEMP_C values
filtered_temp_data <- newport_temp %>%
  filter(is.na(TEMP_FLAG) | TEMP_FLAG %in% c(0, 5, 6)) %>%
  filter(between(SURF_TEMP_C, 5, 35)) %>%
  mutate(Date = make_date(YEAR, MONTH, DAY)) %>%
  filter(between(Date, as.Date("2022-01-01"), as.Date("2023-01-01"))) %>% 
  dplyr::select(Date, Temperature = SURF_TEMP_C)

# Calculate Daily Mean Temperature
daily_temp_stats <- filtered_temp_data %>%
  group_by(Date) %>% 
  summarise(Mean_Temp = mean(Temperature, na.rm = TRUE)) %>% 
  mutate(topt = as.numeric(topt_value)) %>% 
  na.omit()

# Extend daily_temp_stats to include future scenarios within the same dataset
extended_temp_stats <- daily_temp_stats %>%
  mutate(Scenario = "Sea Surface Temperature Distribution (2022)",
         Adjusted_Temp = Mean_Temp, 
         color=ifelse(Adjusted_Temp > topt, "orange2","gold1")) %>%
  bind_rows(
    daily_temp_stats %>% 
      mutate(Scenario = "Sea Surface Temperature Distribution (+2°C)",
             Adjusted_Temp = Mean_Temp + 2,
             color=ifelse(Adjusted_Temp > topt, "orange2", "gold1")),
    daily_temp_stats %>%
      mutate(Scenario = "Sea Surface Temperature Distribution (+4°C)",
             Adjusted_Temp = Mean_Temp + 4,
             color=ifelse(Adjusted_Temp > topt, "orange2", "gold1"))
  ) %>%
  mutate(Exceeds_Topt = Adjusted_Temp > topt_value, 
         Topt=ifelse(Adjusted_Temp > topt_value, "Exceeds T_opt", "Below T_opt")) %>% 
  na.omit(Adjusted_Temp)

# To calculate the percentage exceeding 'topt' for each 'Scenario'
exceeds_topt_percentages <- extended_temp_stats %>%
  group_by(Scenario) %>%
  summarise(
    Percentage = mean(Exceeds_Topt) * 100 # This gives you the percentage
  )

# View the percentages
print(exceeds_topt_percentages)



extended_temp_stats <- left_join(extended_temp_stats, exceeds_topt_percentages, by = "Scenario")

# Adjust the Scenario ordering
extended_temp_stats$Scenario <- factor(extended_temp_stats$Scenario, 
                                       levels = c("Sea Surface Temperature Distribution (2022)",
                                                  "Sea Surface Temperature Distribution (+2°C)",
                                                  "Sea Surface Temperature Distribution (+4°C)"))

common_theme_2 <- common_theme + theme(legend.position = "bottom")

topt_distribution <- ggplot(extended_temp_stats, aes(x = Adjusted_Temp, fill = as.factor(Topt))) +
  geom_histogram(color = "grey25", binwidth = 0.2, linewidth = 0.1) +
  geom_vline(xintercept = topt_value, color = "darkred", linetype = "dashed", size = 0.8) +  # Use dynamic topt_value
  facet_wrap(~Scenario, scales = "free_y", ncol = 1) +
  labs(
    x = "Sea Surface Temperature (°C)",
    y = "Frequency",
    fill = NULL  # Set to NULL since we handle legend title below
  ) +
  scale_fill_manual(
    values = c("Exceeds T_opt" = "red3", "Below T_opt" = "gold1"),
    labels = c(expression(paste("Below T"[opt])), expression(paste("Exceeds T"[opt])))  # TeX-style subscripts
  ) +
  scale_x_continuous(expand = c(0, 0), breaks = seq(10, 30, 2)) +
  scale_y_continuous(expand = c(0, 0), breaks = seq(0, 20, 5), limits = c(0, 20)) + 
  common_theme_2 +
  theme(
    text = element_text(family = "serif"),
    legend.margin = margin(t = 0, r = 15, b = 0, l = 15, unit = "pt")  # Adjusted legend margins
  ) +
  geom_text(aes(label = paste0(round(Percentage, 1), "%"), y = Inf, x = 29), vjust = 1.75, hjust = 1.5, size = 4, color = "red3", family="serif") +
  common_theme +
  theme(
    strip.text.x = element_text(size = 16),
    axis.text.x = element_text(size = 14),
    axis.text.y = element_text(size = 14),
    axis.title = element_text(size = 16),
    axis.title.y= element_text(size=16),
    axis.title.x= element_text(size=16),
    legend.position = "bottom",
    legend.text = element_text(size = 12.5),
    panel.grid.major = element_line(color = "gray90", size = 0.5),
    panel.grid.minor = element_line(color = "gray90", size = 0.25),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.background = element_rect(fill = "white", color = "white")  # Adjust if needed
  ) 

# Print the plot to view
print(topt_distribution)

# Save the plot
ggsave(here::here("Figures", "Temperature_Distribution.png"), plot = topt_distribution, width = 8, height = 6, units = "in")

```


```{r}

```

```{r}
# Define the Q10 calculation function
Q10 <- function(rate1, rate2, temp1, temp2) {
  (rate2 / rate1) ^ (10 / (temp2 - temp1))
}

# Bootstrap the Q10 values
bootstrap_Q10 <- function(model_fit, temp1, temp2, R = 999) {
  boot_Q10 <- numeric(R)
  for (i in 1:R) {
    boot_sample <- sample(1:nrow(model_fit$model), replace = TRUE)
    boot_fit <- nlsLM(
      rate ~ sharpeschoolhigh_1981(temp, r_tref, e, eh, th, tref = 15),
      data = model_fit$model[boot_sample, ],
      start = coef(model_fit)
    )
    rate1 <- predict(boot_fit, newdata = data.frame(temp = temp1))
    rate2 <- predict(boot_fit, newdata = data.frame(temp = temp2))
    boot_Q10[i] <- Q10(rate1, rate2, temp1, temp2)
  }
  boot_Q10
}

# Calculate Q10 values for low pH
low_pH_Q10 <- bootstrap_Q10(low_pH_fit_nlsLM, 10, 20)
low_pH_Q10_CI <- quantile(low_pH_Q10, c(0.025, 0.975))

# Calculate Q10 values for high pH
high_pH_Q10 <- bootstrap_Q10(high_pH_fit_nlsLM, 10, 20)
high_pH_Q10_CI <- quantile(high_pH_Q10, c(0.025, 0.975))

# Print the results
low_pH_Q10_CI
high_pH_Q10_CI

# Function to calculate Q10
calculate_Q10 <- function(R1, R2, T1, T2) {
  Q10 <- (R2 / R1) ^ (10 / (T2 - T1))
  return(Q10)
}

# Example calculation
R1 <- 0.5
R2 <- 1.0
T1 <- 10
T2 <- 20
Q10 <- calculate_Q10(R1, R2, T1, T2)
Q10

# Load necessary libraries
library(dplyr)
library(ggplot2)

# Load your Joule dataset (replace 'your_dataset.csv' with your actual file)
joule_data <- read.csv('your_dataset.csv')

# Assume values for metabolic rate calculation
a <- 1  # rate at 0°C
Q10 <- 2  # factor by which the metabolic rate increases over 10°C
T_ref <- 15  # reference temperature in °C

# Function to calculate metabolic rate
calculate_metabolic_rate <- function(T, a, Q10, T_ref) {
  a * Q10^((T - T_ref) / 10)
}

# Add a column for metabolic rate based on temperature
joule_data <- joule_data %>%
  mutate(metabolic_rate = calculate_metabolic_rate(temperature, a, Q10, T_ref))

# Calculate mean and variance in temperature
mean_temp <- mean(joule_data$temperature, na.rm = TRUE)
var_temp <- var(joule_data$temperature, na.rm = TRUE)

# Calculate mean metabolic rate for variable and constant temperatures
mean_metabolic_rate_variable <- mean(joule_data$metabolic_rate, na.rm = TRUE)
mean_metabolic_rate_constant <- calculate_metabolic_rate(mean_temp, a, Q10, T_ref)

# Jensen's Inequality effect
jensens_effect <- mean_metabolic_rate_variable - mean_metabolic_rate_constant

# Print results
cat("Mean Temperature:", mean_temp, "\n")
cat("Temperature Variance:", var_temp, "\n")
cat("Mean Metabolic Rate (Variable Temp):", mean_metabolic_rate_variable, "\n")
cat("Mean Metabolic Rate (Constant Temp):", mean_metabolic_rate_constant, "\n")
cat("Jensen's Inequality Effect:", jensens_effect, "\n")

# Visualize the results
ggplot(joule_data, aes(x = temperature, y = metabolic_rate)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  labs(title = "Metabolic Rate vs Temperature",
       x = "Temperature (°C)",
       y = "Metabolic Rate") +
  theme_minimal()
```




Benjamin P. Burford, Nicholas Carey, William F. Gilly, Jeremy A. Goldbogen. 2019. Grouping reduces the metabolic demand of a social squid. Marine Ecology Progress Series. 612: 141–150 https://doi.org/10.3354/meps12880
```{r}
install.packages("devtools")
devtools::install_github("nicholascarey/respfun")


respo.rates.tpc <- read_csv(here("Data", "Thinned_Respirometry_Data", "Respiration.Rates.Dataset.csv"), show_col_types = FALSE)
q_ten


respo.rates.Q10 = respirometry::Q10(R_vec=respo.rates.tpc$rate, T_vec=respo.rates.tpc$temp)
```



```{r}
respo.rates <- read_csv(here("Data", "Thinned_Respirometry_Data", "Respiration.Rates.Dataset.csv"), show_col_types = FALSE)
# Estimating Q10 temperature coefficient (thermal sensitivty) for respirometry data 
 
respo.rates.tpc <- respo.rates %>% 
  mutate(rate=umolO2.gram.hr, temp=Temp.C) %>%
  dplyr::select(pH_Treatment, temp, rate) %>% 
  dplyr::select(pH_Treatment, temp, rate)
# Estimating Q10 values for low and high pH data 
respo.rates.Q10 = respirometry::Q10(R_vec=respo.rates.tpc$rate, T_vec=respo.rates.tpc$temp)
# do it ransdomly by snail snail id across the 8 temp 
respo.rates.low = respo.rates.tpc %>% filter(pH_Treatment == "Low")
respo.rates.low.pH.Q10 = respirometry::Q10(R_vec=respo.rates.low$rate, T_vec=respo.rates.low$temp)
lm_model = respirometry::Q10(R_vec=respo.rates.low$rate, T_vec=respo.rates.low$temp, model=TRUE)
intercept <- coef(lm_model)[1]
slope <- coef(lm_model)[2]
lm_model
Author(s) Matthew A. Birk, <matthewabirk@gmail.com> See Also Q10, calc_b Examples # I know a species has an SMR of 800 umol O2/h at 200 g. # What would be a likely SMR for a 300 g individual? scale_MO2(mass_1 = 200, MO2_1 = 800, mass_2 = 300) # Some squids have a much higher scaling coefficient: scale_MO2(mass_1 = 200, MO2_1 = 800, mass_2 = 300, b = 0.92) # A 100 g individual at 10 *C has an MO2 of 1270 umol/h. How much # would a 250 g individual likely consume at 14 *C? Q10(Q10 = 2, R1 = scale_MO2(mass_1 = 100, MO2_1 = 1270, mass_2 = 250), T1 = 10, T2 = 14) # Now I have data from real animals and I want to mass-correct them all to a 10 g animal. mass = 2:20 # obviously not real but you get the point mo2 = c(44.8, 41, 36, 35, 35, 33.5, 34.5, 40, 30, 23, 27, 30, 25.6, 27.8, 28, 24, 27, 28, 20) desired_mass = 10 b = calc_b(mass = mass, MO2 = mo2) scale_MO2(mass_1 = mass, MO2_1 = mo2, mass_2 = desired_mass, b = b$b) plot(mass, mo2, ylab = 'Raw MO2') # before plot(mass, scale_MO2(mass_1 = mass, MO2_1 = mo2, mass_2 = 10, b = b$b), ylab = 'Mass-corrected MO2') # after # Visualize MO2 scaling by mass and temperature: mass <- seq(10, 200, 10) temp <- 10:25 base_mass <- 50 base_temp <- 20 base_MO2 <- 750 mo2 <- outer(mass, temp, function(mass, temp){ scale_MO2(mass_1 = base_mass, mass_2 = mass, MO2_1 = Q10(Q10 = 2, R1 = base_MO2, T1 = base_temp, T2 = temp)) }) persp(mass, temp, mo2, xlab = 'Mass (g)', ylab = 'Temperature (*C)', zlab = 'MO2 (umol / hr)', theta = 35, phi = 15, expand = 0.5, ticktype = 'detailed', nticks = 10)

respo.rates.high = respo.rates.tpc %>% filter(pH_Treatment == "Ambient")
respo.rates.high.pH.Q10 = respirometry::Q10(R_vec=respo.rates.high$rate, T_vec=respo.rates.high$temp)

print(paste("Tegula funebralis (Q10):", respo.rates.Q10))
print(paste("Tegula funebralis (Low pH Q10):", respo.rates.low.pH.Q10))
print(paste("Tegula funebralis (High pH Q10):", respo.rates.high.pH.Q10))

# Create a data frame to hold Q10 values
q10_values <- data.frame(
  Treatment = c("Tegula funebralis (Overall)", "Tegula funebralis (Low pH)", "Tegula funebralis (High pH)"),
  Q10 = c(respo.rates.Q10, respo.rates.low.pH.Q10, respo.rates.high.pH.Q10)
)
```


```{r}
# Save the data frame to a CSV file in the Figures directory
write.csv(q10_values, file = here("Figures", "Tegula_funebralis_Q10_values.csv"), row.names = FALSE)
```

```{r}

```



# Fitting the Sharpe-Schoolfield Model to Weighted Average Respiration Rates Across Temperature

```{r}
#Averaging Rates by temperature and pH treatment 
avg.respo.rates.tpc <- respo.rates.dataset %>% 
  mutate(rate=umolO2.gram.hr, temp=Temp.C) %>%
  group_by(Temp_Treatment, pH_Treatment) %>% 
  summarise(., sd = sd(rate),
            rate = mean(rate),
            temp = mean(temp),
            .groups = 'drop') %>% ungroup() %>% 
  dplyr::select(pH_Treatment, temp, rate, sd)

# keeping just a single curve
low_pH_mean <- filter(avg.respo.rates.tpc, pH_Treatment == 'Low') %>% 
  dplyr::select(-pH_Treatment)

high_pH_mean <- filter(avg.respo.rates.tpc, pH_Treatment == 'Ambient') %>% 
  dplyr::select(-pH_Treatment)

# Fit the Sharpe-Schoolfield model for low pH
sharpeschoolhigh_fit_low_pH_mean <- nest(low_pH_mean, data = c(temp, rate, sd)) %>%
  mutate(sharpeschoolhigh = map(data, ~nls_multstart(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 20),
                        data = .x,
                        iter = c(4,4,4,4),
                        start_lower = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') - 10,
                        start_upper = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') + 10,
                        lower = get_lower_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),
                        upper = get_upper_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),
                        supp_errors = 'Y',
                        convergence_count = FALSE,
                        #including weights for each replicate here
                        modelweights = 1/sd)))

# get predictions using augment for low pH
low_pH_mean_newdata <- tibble(temp = seq(min(low_pH_mean$temp), max(low_pH_mean$temp), length.out = 100))
sharpeschoolhigh_fit_low_pH_mean_preds <- sharpeschoolhigh_fit_low_pH_mean %>%
  mutate(., preds = map(sharpeschoolhigh, augment, newdata = low_pH_mean_newdata)) %>%
  mutate(pH_Treatment="Low") %>% 
  dplyr::select(-sharpeschoolhigh) %>%
  unnest(preds)

# Fit the Sharpe-Schoolfield model for high pH
sharpeschoolhigh_fit_high_pH_mean <- nest(high_pH_mean, data = c(temp, rate, sd)) %>%
  mutate(sharpeschoolhigh = map(data, ~nls_multstart(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 20),
                        data = .x,
                        iter = c(4,4,4,4),
                        start_lower = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') - 10,
                        start_upper = get_start_vals(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981') + 10,
                        lower = get_lower_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),
                        upper = get_upper_lims(.x$temp, .x$rate, model_name = 'sharpeschoolhigh_1981'),
                        supp_errors = 'Y',
                        convergence_count = FALSE,
                        #including weights for each replicate here
                        modelweights = 1/sd)))

# get predictions using augment for high pH 
high_pH_mean_newdata <- tibble(temp = seq(min(high_pH_mean$temp), max(high_pH_mean$temp), length.out = 100))
sharpeschoolhigh_fit_high_pH_mean_preds <- sharpeschoolhigh_fit_high_pH_mean %>%
  mutate(., preds = map(sharpeschoolhigh, augment, newdata = high_pH_mean_newdata)) %>%
  mutate(pH_Treatment="Ambient") %>% 
  dplyr::select(-sharpeschoolhigh) %>%
  unnest(preds)

high_pH_mean <- high_pH_mean %>% mutate(pH_Treatment = "Ambient")
low_pH_mean <- low_pH_mean %>% mutate(pH_Treatment = "Low")

# Joining and low and high pH prediction for plotting 
avg_TPC_rates <- full_join(high_pH_mean, low_pH_mean)
avg_TPC_preds <- full_join(sharpeschoolhigh_fit_high_pH_mean_preds, sharpeschoolhigh_fit_low_pH_mean_preds)

# plotting TPC for Sharpe-Schoolfield (high activation) model 
ggplot() +
  geom_line(aes(temp, .fitted, group=pH_Treatment, color=pH_Treatment), avg_TPC_preds) +
  geom_linerange(aes(x = temp, ymin = rate - sd, ymax = rate + sd, group=pH_Treatment), avg_TPC_rates) +
  geom_point(aes(temp, rate, group=pH_Treatment, fill=pH_Treatment), avg_TPC_rates, size = 2, shape = 21) +
  facet_wrap(~pH_Treatment) +
  scale_color_manual(values = c("Low" = "cyan3", "Ambient" = "orange"), guide = "none") +
  scale_fill_manual(values = c("Low" = "cyan3", "Ambient" = "orange"), guide = "none") +
  theme_bw(base_size = 12) +
  common_theme +
  labs(x = xlab,
       y = ylab) + 
  scale_y_continuous(breaks = seq(0, 125, 25), limits=c(0,110)) +
  scale_x_continuous(breaks = seq(12, 26, 2), limits=c(12,26.5), position = "bottom") +
  geom_hline(yintercept = 0, linetype = 2)

```

# Bootstrapping the Sharpe-Schoolfield Model to Weighted Average Respiration Rates Across Temperature

```{r}

# Fit the Sharpe-Schoolfield model for low pH using nmls model prior to bootstrap
low_pH_mean_fit_nlsLM <- minpack.lm::nlsLM(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 15),
                        data = low_pH_mean,
                        start = coef(sharpeschoolhigh_fit_low_pH_mean$sharpeschoolhigh[[1]]),
                        lower = get_lower_lims(low_pH_mean$temp, low_pH_mean$rate, model_name = 'sharpeschoolhigh_1981'),
                        upper = get_upper_lims(low_pH_mean$temp, low_pH_mean$rate, model_name = 'sharpeschoolhigh_1981'),
                        weights = 1/sd)

# perform bootstrapping (case versus residual) for low pH
low_pH_mean_case_bootstrap <- Boot(low_pH_mean_fit_nlsLM, method = 'case', R=999)
low_pH_mean_residual_bootstrap <- Boot(low_pH_mean_fit_nlsLM, method = 'residual', R=999)  # using residual bootstrapping due to lower standard error 

# predict over new data
low_pH_mean_residual_bootstrap_preds <- low_pH_mean_residual_bootstrap$t %>%
  as.data.frame() %>%
  drop_na() %>%
  mutate(iter = 1:n()) %>%
  group_by_all() %>%
  do(data.frame(temp = seq(min(low_pH_mean$temp), max(low_pH_mean$temp), length.out = 100))) %>%
  ungroup() %>%
  mutate(pred = sharpeschoolhigh_1981(temp, r_tref, e, eh, th, tref=15)) %>% 
  mutate(pH_Treatment = "Low")

# calculate bootstrapped confidence intervals
low_pH_mean_residual_bootstrap_CI_preds <- group_by(low_pH_mean_residual_bootstrap_preds, temp) %>%
  summarise(conf_lower = quantile(pred, 0.025),
            conf_upper = quantile(pred, 0.975),
            .groups = 'drop') %>% 
  mutate(pH_Treatment = "Low")

# Fit the Sharpe-Schoolfield model for high pH using nmls model prior to bootstrap
high_pH_mean_fit_nlsLM <- minpack.lm::nlsLM(rate~sharpeschoolhigh_1981(temp = temp, r_tref,e,eh,th, tref = 15),
                        data = high_pH_mean,
                        start = coef(sharpeschoolhigh_fit_high_pH_mean$sharpeschoolhigh[[1]]),
                        lower = get_lower_lims(high_pH_mean$temp, high_pH_mean$rate, model_name = 'sharpeschoolhigh_1981'),
                        upper = get_upper_lims(high_pH_mean$temp, high_pH_mean$rate, model_name = 'sharpeschoolhigh_1981'),
                        weights = 1/sd)

# perform bootstrapping (case versus residual) for high pH
high_pH_mean_case_bootstrap <- Boot(high_pH_mean_fit_nlsLM, method = 'case', R=999)
high_pH_mean_residual_bootstrap <- Boot(high_pH_mean_fit_nlsLM, method = 'residual', R=999)

# predict over new data
high_pH_mean_residual_bootstrap_preds <- high_pH_mean_residual_bootstrap$t %>%
  as.data.frame() %>%
  drop_na() %>%
  mutate(iter = 1:n()) %>%
  group_by_all() %>%
  do(data.frame(temp = seq(min(high_pH_mean$temp), max(high_pH_mean$temp), length.out = 100))) %>%
  ungroup() %>%
  mutate(pred = sharpeschoolhigh_1981(temp, r_tref, e, eh, th, tref=15)) %>% 
  mutate(pH_Treatment = "Ambient")

# calculate bootstrapped confidence intervals
high_pH_mean_residual_bootstrap_CI_preds <- group_by(high_pH_mean_residual_bootstrap_preds, temp) %>%
  summarise(conf_lower = quantile(pred, 0.025),
            conf_upper = quantile(pred, 0.975),
            .groups = 'drop') %>% 
  mutate(pH_Treatment = "Ambient")

# joining residual bootstrap preds and confidence intervals (95%)
residual_bootstrap_preds <- full_join(high_pH_mean_residual_bootstrap_preds, low_pH_mean_residual_bootstrap_preds)
residual_bootstrap_CI_preds <- full_join(high_pH_mean_residual_bootstrap_CI_preds, low_pH_mean_residual_bootstrap_CI_preds)

# plotting TPC for Sharpe-Schoolfield (high activation) model 
avg_TPC_CI_plot <- ggplot() +
  geom_line(aes(temp, .fitted, group=pH_Treatment, color=pH_Treatment), avg_TPC_preds) +
  geom_ribbon(aes(temp, ymin = conf_lower, ymax = conf_upper, fill=pH_Treatment), residual_bootstrap_CI_preds, alpha = 0.2) +
  geom_linerange(aes(x = temp, ymin = rate - sd, ymax = rate + sd, group=pH_Treatment), avg_TPC_rates) +
  geom_point(aes(temp, rate, group=pH_Treatment, fill=pH_Treatment), avg_TPC_rates, size = 2, shape = 21) +
  facet_wrap(~pH_Treatment) +
  scale_color_manual(values = c("Low" = "cyan3", "Ambient" = "orange"), guide = "none") +
  scale_fill_manual(values = c("Low" = "cyan3", "Ambient" = "orange"), guide = "none") +
  theme_bw(base_size = 12) +
  common_theme +
  labs(x = xlab,
       y = ylab) + 
  scale_y_continuous(breaks = seq(0, 125, 25), limits=c(0,100)) +
  scale_x_continuous(breaks = seq(12, 26, 2), limits=c(12,26.5), position = "bottom") +
  geom_hline(yintercept = 0, linetype = 2)

# plotting TPC for Sharpe-Schoolfield (high activation) model 
avg_TPC_predictions_plot <-ggplot() +
  geom_line(aes(temp, .fitted, group=pH_Treatment, color=pH_Treatment), avg_TPC_preds) +
  geom_line(aes(temp, pred, group = iter, col=pH_Treatment), residual_bootstrap_preds, alpha = 0.008) +
  geom_linerange(aes(x = temp, ymin = rate - sd, ymax = rate + sd, group=pH_Treatment), avg_TPC_rates) +
  geom_point(aes(temp, rate, group=pH_Treatment, fill=pH_Treatment), avg_TPC_rates, size = 2, shape = 21) +
  facet_wrap(~pH_Treatment) +
  scale_color_manual(values = c("Low" = "cyan3", "Ambient" = "orange"), guide = "none") +
  scale_fill_manual(values = c("Low" = "cyan3", "Ambient" = "orange"), guide = "none") +
  theme_bw(base_size = 12) +
  common_theme +
  labs(x = xlab,
       y = ylab) + 
  scale_y_continuous(breaks = seq(0, 125, 25), limits=c(0,100)) +
  scale_x_continuous(breaks = seq(12, 26, 2), limits=c(12,26.5), position = "bottom") +
  geom_hline(yintercept = 0, linetype = 2)

combined_avg_plots <- ggplot() +
  geom_line(aes(temp, .fitted, group=pH_Treatment, color=pH_Treatment), avg_TPC_preds) +
  geom_ribbon(aes(temp, ymin = conf_lower, ymax = conf_upper, fill=pH_Treatment), residual_bootstrap_CI_preds, alpha = 0.2) +
  geom_line(aes(temp, pred, group = iter, col=pH_Treatment), residual_bootstrap_preds, alpha = 0.008) +
  geom_linerange(aes(x = temp, ymin = rate - sd, ymax = rate + sd, group=pH_Treatment), avg_TPC_rates) +
  geom_point(aes(temp, rate, group=pH_Treatment, fill=pH_Treatment), avg_TPC_rates, size = 2, shape = 21) +
  facet_wrap(~pH_Treatment) +
  scale_color_manual(values = c("Low" = "cyan3", "Ambient" = "orange"), guide = "none") +
  scale_fill_manual(values = c("Low" = "cyan3", "Ambient" = "orange"), guide = "none") +
  theme_bw(base_size = 12) +
  common_theme +
  labs(x = xlab,
       y = ylab) + 
  scale_y_continuous(breaks = seq(0, 125, 25), limits=c(0,100)) +
  scale_x_continuous(breaks = seq(12, 26, 2), limits=c(12,26.5), position = "bottom") +
  geom_hline(yintercept = 0, linetype = 2)

print(avg_TPC_CI_plot)
print(avg_TPC_predictions_plot)
print(combined_avg_plots)

# Save the plots as a PNG file
ggsave(here::here("Figures", "TPC_CI_combined_weighted_avg_plot.png"), combined_avg_plots, width = 10, height = 10, units = "in")

```



```{r}
library(MASS)
# get parameters of fitted model
low_pH_params <- broom::tidy(low_pH_mean_fit_nlsLM)  %>%
  dplyr::select(param = term, estimate) 
  
# CIs from residual resampling
ci_low_pH_residual <- low_pH_mean_residual_bootstrap %>%
  confint(., method = 'bca', level = 0.95) %>%
  as.data.frame() %>%
  rename(conf_lower = 1, conf_upper = 2) %>%
  rownames_to_column(., var = 'param') %>%
  mutate(method = 'residual bootstrap')

low_pH_params <-left_join(low_pH_params, ci_low_pH_residual) %>%
  mutate(pH_Treatment = "Low")

# residul Bootstrapping Extracted Parameters
low_pH_extra_params <- calc_params(low_pH_mean_fit_nlsLM) %>%
  pivot_longer(everything(), names_to =  'param', values_to = 'estimate') %>% 
  filter(param != 'th', param != 'tref', param != 'e', param != "eh", param != "q10") %>%
  na.omit()

# residual Bootstrapping Extracted Parameters
ci_low_extra_params_residual <- Boot(low_pH_mean_fit_nlsLM , f = function(x){unlist(calc_params(x))},
  labels = names(calc_params(low_pH_mean_fit_nlsLM )), R = 999, method = 'residual') %>%
  confint(., method = 'bca', level = 0.95) %>%
  as.data.frame() %>%
  rename(conf_lower = 1, conf_upper = 2) %>%
  rownames_to_column(., var = 'param') %>%
  mutate(method = 'residual bootstrap', 
         pH_Treatment = 'Low')

low_pH_extra_params <-left_join(low_pH_extra_params, ci_low_extra_params_residual) %>%
  mutate(pH_Treatment = "Low")

full_low_pH_params <- bind_rows(low_pH_params, low_pH_extra_params)

high_pH_params <- broom::tidy(high_pH_mean_fit_nlsLM)  %>%
  dplyr::select(param = term, estimate)

# CIs from residual resampling
ci_high_pH_residual <- high_pH_mean_residual_bootstrap %>%
  confint(., method = 'bca', level = 0.95) %>%
  as.data.frame() %>%
  rename(conf_lower = 1, conf_upper = 2) %>%
  rownames_to_column(., var = 'param') %>%
  mutate(method = 'residual bootstrap')

high_pH_params <-left_join(high_pH_params, ci_high_pH_residual) %>%
  
  mutate(pH_Treatment = "Ambient")

# Case Bootstrapping Extracted Parameters
high_pH_extra_params <- calc_params(high_pH_mean_fit_nlsLM) %>%
  pivot_longer(everything(), names_to =  'param', values_to = 'estimate') %>% 
  filter(param != 'th', param != 'tref', param != 'e', param != "eh", param != "q10") %>%
  na.omit()

# Case Bootstrapping Extracted Parameters
ci_high_extra_params_residual <- Boot(high_pH_mean_fit_nlsLM , f = function(x){unlist(calc_params(x))},
  labels = names(calc_params(high_pH_mean_fit_nlsLM)), R = 999, method = 'residual') %>%
  confint(., method = 'bca', level = 0.95) %>%
  as.data.frame() %>%
  rename(conf_lower = 1, conf_upper = 2) %>%
  rownames_to_column(., var = 'param') %>%
  mutate(method = 'residual bootstrap', 
         pH_Treatment = 'Ambient')

high_pH_extra_params <-left_join(high_pH_extra_params, ci_high_extra_params_residual) %>%
  
  mutate(pH_Treatment = "Ambient")

full_high_pH_params <- bind_rows(high_pH_params, high_pH_extra_params)

# Combine the confidence intervals for low and high pH

ci_param <- bind_rows(full_low_pH_params, full_high_pH_params)

ggplot(ci_param, aes(param, estimate, color = pH_Treatment, group = pH_Treatment)) +
  geom_point(size = 4, position = position_dodge(width = 0.5)) +
  geom_linerange(aes(ymin = conf_lower, ymax = conf_upper), position = position_dodge(width = 0.5)) +
  geom_hline(aes(yintercept = conf_lower, color = pH_Treatment), linetype = 2) +
  geom_hline(aes(yintercept = conf_upper, color = pH_Treatment), linetype = 2) +
  common_theme +
  theme_minimal() +
  facet_wrap(~param, scales = 'free') +
  scale_color_manual(values = c("Low" = "cyan3", "Ambient" = "orange")) +
  scale_x_discrete('') +
  labs(title = 'Calculation of confidence intervals for model parameters')

```

# Calculating Metablic Information from Respiration Rates
```{r, Calulcating Metabolic Data}


# Estimating Q10 temperature coefficient (thermal sensitivty) for respirometry data 
 
# Estimating Q10 values for low and high pH data 
respo.rates.Q10 = respirometry::Q10(R_vec=respo.rates.tpc$rate, T_vec=respo.rates.tpc$temp)

respo.rates.low = respo.rates.tpc %>% filter(pH_Treatment == "Low")
respo.rates.low.pH.Q10 = respirometry::Q10(R_vec=respo.rates.low$rate, T_vec=respo.rates.low$temp)

respo.rates.high = respo.rates.tpc %>% filter(pH_Treatment == "Ambient")
respo.rates.high.pH.Q10 = respirometry::Q10(R_vec=respo.rates.high$rate, T_vec=respo.rates.high$temp)

print(paste("Tegula funebralis (Q10):", respo.rates.Q10))
print(paste("Tegula funebralis (Low pH Q10):", respo.rates.low.pH.Q10))
print(paste("Tegula funebralis (High pH Q10):", respo.rates.high.pH.Q10))

# Calculating mean respiration rate at each temperature
mean_rates_low <- aggregate(rate ~ temp, data = respo.rates.low, FUN = mean)
mean_rates_high <- aggregate(rate ~ temp, data = respo.rates.high, FUN = mean)

# Identifying temperature at which respiration rate is maximum
max_temp_low <- mean_rates_low$temp[which.max(mean_rates_low$rate)]
max_temp_high <- mean_rates_high$temp[which.max(mean_rates_high$rate)]

# Topt maximum respiration rate
print(paste("Tegula funebralis (Low pH Topt):", max_temp_low))
print(paste("Tegula funebralis (High pH Topt):", max_temp_high))

```









